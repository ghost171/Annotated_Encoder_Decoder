{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# we will use CUDA if it is available\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE=torch.device('cuda') # or set to 'cpu'\n",
    "print(\"CUDA:\", USE_CUDA)\n",
    "print(DEVICE)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.5\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, src_lengths, max_len=100, sos_index=1, eos_index=None):\n",
    "    \"\"\"Greedily decode a sentence.\"\"\"\n",
    "    src_lengths = src_lengths.cpu()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden, encoder_final = model.encode(src, src_mask, src_lengths)\n",
    "        prev_y = torch.ones(1, 1).fill_(sos_index).type_as(src)\n",
    "        trg_mask = torch.ones_like(prev_y)\n",
    "\n",
    "    output = []\n",
    "    attention_scores = []\n",
    "    hidden = None\n",
    "\n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            out, hidden, pre_output = model.decode(\n",
    "              encoder_hidden, encoder_final, src_mask,\n",
    "              prev_y, trg_mask, hidden)\n",
    "            \n",
    "            prob = model.generator(pre_output[:, -1])\n",
    "\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data.item()\n",
    "        output.append(next_word)\n",
    "        prev_y = torch.ones(1, 1).type_as(src).fill_(next_word)\n",
    "        attention_scores.append(model.decoder.attention.alphas.cpu().numpy())\n",
    "    \n",
    "    output = np.array(output)\n",
    "        \n",
    "    \n",
    "    if eos_index is not None:\n",
    "        first_eos = np.where(output==eos_index)[0]\n",
    "        if len(first_eos) > 0:\n",
    "            output = output[:first_eos[0]]      \n",
    "    \n",
    "    return output, np.concatenate(attention_scores, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_words(x, vocab=None):\n",
    "    if vocab is not None:\n",
    "        x = [vocab.itos[i] for i in x]\n",
    "\n",
    "    return [str(t) for t in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_examples(example_iter, model, n=2, max_len=100, \n",
    "                   sos_index=1, \n",
    "                   src_eos_index=None, \n",
    "                   trg_eos_index=None, \n",
    "                   src_vocab=None, trg_vocab=None):\n",
    "    \"\"\"Prints N examples. Assumes batch size of 1.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    print()\n",
    "    \n",
    "    if src_vocab is not None and trg_vocab is not None:\n",
    "        src_eos_index = src_vocab.stoi[EOS_TOKEN]\n",
    "        trg_sos_index = trg_vocab.stoi[SOS_TOKEN]\n",
    "        trg_eos_index = trg_vocab.stoi[EOS_TOKEN]\n",
    "    else:\n",
    "        src_eos_index = None\n",
    "        trg_sos_index = 1\n",
    "        trg_eos_index = None\n",
    "        \n",
    "    for i, batch in enumerate(example_iter):\n",
    "      \n",
    "        src = batch.src.cpu().numpy()[0, :]\n",
    "        trg = batch.trg_y.cpu().numpy()[0, :]\n",
    "\n",
    "        # remove </s> (if it is there)\n",
    "        src = src[:-1] if src[-1] == src_eos_index else src\n",
    "        trg = trg[:-1] if trg[-1] == trg_eos_index else trg      \n",
    "      \n",
    "        result, _ = greedy_decode(\n",
    "          model, batch.src, batch.src_mask, batch.src_lengths,\n",
    "          max_len=max_len, sos_index=trg_sos_index, eos_index=trg_eos_index)\n",
    "        print(\"Example #%d\" % (i+1))\n",
    "        print(\"Src : \", \" \".join(lookup_words(src, vocab=src_vocab)))\n",
    "        print(\"Trg : \", \" \".join(lookup_words(trg, vocab=trg_vocab)))\n",
    "        print(\"Pred: \", \" \".join(lookup_words(result, vocab=trg_vocab)))\n",
    "        print()\n",
    "        \n",
    "        count += 1\n",
    "        if count == n:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.trg_embed = trg_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask, src_lengths, trg_lengths):\n",
    "        \"\"\"Take in and process masked src and target sequences.\"\"\"\n",
    "        src_lengths = src_lengths.cpu()\n",
    "        encoder_hidden, encoder_final = self.encode(src, src_mask, src_lengths)\n",
    "        return self.decode(encoder_hidden, encoder_final, src_mask, trg, trg_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask, src_lengths):\n",
    "        return self.encoder(self.src_embed(src), src_mask, src_lengths)\n",
    "    \n",
    "    def decode(self, encoder_hidden, encoder_final, src_mask, trg, trg_mask,\n",
    "               decoder_hidden=None):\n",
    "        return self.decoder(self.trg_embed(trg), encoder_hidden, encoder_final,\n",
    "                            src_mask, trg_mask, hidden=decoder_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encodes a sequence of word embeddings\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, \n",
    "                          batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x, mask, lengths):\n",
    "        \"\"\"\n",
    "        Applies a bidirectional GRU to sequence of embeddings x.\n",
    "        The input mini-batch x needs to be sorted by length.\n",
    "        x should have dimensions [batch, time, dim].\n",
    "        \"\"\"\n",
    "        packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        output, final = self.rnn(packed)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        # we need to manually concatenate the final states for both directions\n",
    "        fwd_final = final[0:final.size(0):2]\n",
    "        bwd_final = final[1:final.size(0):2]\n",
    "        final = torch.cat([fwd_final, bwd_final], dim=2)  \n",
    "\n",
    "        return output, final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"A conditional RNN decoder with attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, emb_size, hidden_size, attention, num_layers=1, dropout=0.5,\n",
    "                 bridge=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attention = attention\n",
    "        self.dropout = dropout\n",
    "                 \n",
    "        self.rnn = nn.GRU(emb_size + 2*hidden_size, hidden_size, num_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "                 \n",
    "        # to initialize from the final encoder state\n",
    "        self.bridge = nn.Linear(2*hidden_size, hidden_size, bias=True) if bridge else None\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        self.pre_output_layer = nn.Linear(hidden_size + 2*hidden_size + emb_size,\n",
    "                                          hidden_size, bias=False)\n",
    "        \n",
    "    def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n",
    "        \"\"\"Perform a single decoder step (1 word)\"\"\"\n",
    "\n",
    "        # compute context vector using attention mechanism\n",
    "        query = hidden[-1].unsqueeze(1)  \n",
    "        context, attn_probs = self.attention(query=query, proj_key=proj_key,value=encoder_hidden, mask=src_mask)\n",
    "\n",
    "        # update rnn hidden state\n",
    "        rnn_input = torch.cat([prev_embed, context], dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "\n",
    "        pre_output = torch.cat([prev_embed, output, context], dim=2)\n",
    "        pre_output = self.dropout_layer(pre_output)\n",
    "        pre_output = self.pre_output_layer(pre_output)\n",
    "\n",
    "        return output, hidden, pre_output\n",
    "\n",
    "    def forward(self, trg_embed, encoder_hidden, encoder_final, \n",
    "            src_mask, trg_mask, hidden=None, max_len=None):\n",
    "        \"\"\"Unroll the decoder one step at a time.\"\"\"\n",
    "\n",
    "        # the maximum number of steps to unroll the RNN\n",
    "        if max_len is None:\n",
    "            max_len = trg_mask.size(-1)\n",
    "\n",
    "        # initialize decoder hidden state\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(encoder_final)\n",
    "\n",
    "        # pre-compute projected encoder hidden states\n",
    "        # (the \"keys\" for the attention mechanism)\n",
    "        # this is only done for efficiency\n",
    "        proj_key = self.attention.key_layer(encoder_hidden)\n",
    "\n",
    "        # here we store all intermediate hidden states and pre-output vectors\n",
    "        decoder_states = []\n",
    "        pre_output_vectors = []\n",
    "\n",
    "        # unroll the decoder RNN for max_len steps\n",
    "        for i in range(max_len):\n",
    "            prev_embed = trg_embed[:, i].unsqueeze(1)\n",
    "            output, hidden, pre_output = self.forward_step(\n",
    "              prev_embed, encoder_hidden, src_mask, proj_key, hidden)\n",
    "            decoder_states.append(output)\n",
    "            pre_output_vectors.append(pre_output)\n",
    "\n",
    "        decoder_states = torch.cat(decoder_states, dim=1)\n",
    "        pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n",
    "        return decoder_states, hidden, pre_output_vectors  # [B, N, D]\n",
    "\n",
    "\n",
    "\n",
    "    def init_hidden(self, encoder_final):\n",
    "        \"\"\"Returns the initial decoder state,\n",
    "        conditioned on the final encoder state.\"\"\"\n",
    "\n",
    "        if encoder_final is None:\n",
    "            return None  # start with zeros\n",
    "\n",
    "        return torch.tanh(self.bridge(encoder_final))            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, key_size=None, query_size=None):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        \n",
    "        # We assume a bi-directional encoder so key_size is 2*hidden_size\n",
    "        key_size = 2 * hidden_size if key_size is None else key_size\n",
    "        query_size = hidden_size if query_size is None else query_size\n",
    "\n",
    "        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n",
    "        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n",
    "        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "        # to store attention scores\n",
    "        self.alphas = None\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, query=None, proj_key=None, value=None, mask=None):\n",
    "        assert mask is not None, \"mask is required\"\n",
    "\n",
    "        # We first project the query (the decoder state).\n",
    "        # The projected keys (the encoder states) were already pre-computated.\n",
    "        query = self.query_layer(query)\n",
    "        \n",
    "        # Calculate scores.\n",
    "        scores = self.energy_layer(torch.tanh(query + proj_key))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "        \n",
    "        # Mask out invalid positions.\n",
    "        # The mask marks valid positions so we invert it using `mask & 0`.\n",
    "        scores.data.masked_fill_(mask == 0, -float('inf'))\n",
    "        \n",
    "        # Turn scores to probabilities.\n",
    "        alphas = F.softmax(scores, dim=-1)\n",
    "        self.alphas = alphas        \n",
    "        \n",
    "        # The context vector is the weighted sum of the values.\n",
    "        context = torch.bmm(alphas, value)\n",
    "        \n",
    "        # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n",
    "        return context, alphas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, emb_size=256, hidden_size=512, num_layers=1, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "\n",
    "    attention = BahdanauAttention(hidden_size)\n",
    "\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(emb_size, hidden_size, num_layers=num_layers, dropout=dropout),\n",
    "        Decoder(emb_size, hidden_size, attention, num_layers=num_layers, dropout=dropout),\n",
    "        nn.Embedding(src_vocab, emb_size),\n",
    "        nn.Embedding(tgt_vocab, emb_size),\n",
    "        Generator(hidden_size, tgt_vocab))\n",
    "\n",
    "    return model.cuda() if USE_CUDA else model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"Object for holding a batch of data with mask during training.\n",
    "    Input is a batch from a torch text iterator.\n",
    "    \"\"\"\n",
    "    def __init__(self, src, trg, pad_index=0):\n",
    "        \n",
    "        src, src_lengths = src\n",
    "        \n",
    "        self.src = src\n",
    "        self.src_lengths = src_lengths\n",
    "        self.src_mask = (src != pad_index).unsqueeze(-2)\n",
    "        self.nseqs = src.size(0)\n",
    "        \n",
    "        self.trg = None\n",
    "        self.trg_y = None\n",
    "        self.trg_mask = None\n",
    "        self.trg_lengths = None\n",
    "        self.ntokens = None\n",
    "\n",
    "        if trg is not None:\n",
    "            trg, trg_lengths = trg\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_lengths = trg_lengths\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = (self.trg_y != pad_index)\n",
    "            self.ntokens = (self.trg_y != pad_index).data.sum().item()\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            self.src = self.src.cuda()\n",
    "            self.src_mask = self.src_mask.cuda()\n",
    "\n",
    "            if trg is not None:\n",
    "                self.trg = self.trg.cuda()\n",
    "                self.trg_y = self.trg_y.cuda()\n",
    "                self.trg_mask = self.trg_mask.cuda()\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute, print_every=50):\n",
    "    \"\"\"Standard Training and Logging Function\"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    print_tokens = 0\n",
    "\n",
    "    for i, batch in enumerate(data_iter, 1):\n",
    "        \n",
    "        out, _, pre_output = model.forward(batch.src, batch.trg,\n",
    "                                           batch.src_mask, batch.trg_mask,\n",
    "                                           batch.src_lengths, batch.trg_lengths)\n",
    "        loss = loss_compute(pre_output, batch.trg_y, batch.nseqs)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        print_tokens += batch.ntokens\n",
    "        \n",
    "        if model.training and i % print_every == 0:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.nseqs, print_tokens / elapsed))\n",
    "            start = time.time()\n",
    "            print_tokens = 0\n",
    "\n",
    "    return math.exp(total_loss / float(total_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"\"\"A simple loss compute and train function.\"\"\"\n",
    "\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
    "                              y.contiguous().view(-1))\n",
    "        loss = loss / norm\n",
    "\n",
    "        if self.opt is not None:\n",
    "            loss.backward()          \n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "        return loss.data.item() * norm\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LC_ALL=\"en_US.UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(filepaths):\n",
    "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "  data = []\n",
    "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "    de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)],\n",
    "                            dtype=torch.long)\n",
    "    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)],\n",
    "                            dtype=torch.long)\n",
    "    data.append((de_tensor_, en_tensor_))\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_model(model_url, max_seq_length):\n",
    "    with tf.device('/cpu:0'):\n",
    "        labse_layer = hub.KerasLayer(model_url, trainable=False)\n",
    "\n",
    "      # Define input.\n",
    "        input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                             name=\"input_word_ids\")\n",
    "        input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                     name=\"input_mask\")\n",
    "        segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                          name=\"segment_ids\")\n",
    "\n",
    "      # LaBSE layer.\n",
    "        pooled_output,  _ = labse_layer([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "      # The embedding is l2 normalized.\n",
    "        pooled_output = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.nn.l2_normalize(x, axis=1))(pooled_output)\n",
    "\n",
    "      # Define model.\n",
    "        labse_model = tf.keras.Model(\n",
    "        inputs=[input_word_ids, input_mask, segment_ids],\n",
    "        outputs=pooled_output)\n",
    "    return labse_model, labse_layer\n",
    "max_seq_length = 10\n",
    "labse_model, labse_layer = get_model(model_url=\"https://tfhub.dev/google/LaBSE/1\", max_seq_length=max_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import tokenization\n",
    "\n",
    "vocab_file = labse_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "with tf.device('/cpu:0'):\n",
    "    do_lower_case = labse_layer.resolved_object.do_lower_case.numpy()\n",
    "    tokenizer = bert.tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(filepath, tokenizer):\n",
    "  counter = Counter()\n",
    "  with io.open(filepath, encoding=\"utf8\") as f:\n",
    "    for string_ in f:\n",
    "      counter.update(tokenizer(string_))\n",
    "  return build_vocab_from_iterator(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "ininormer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_kz_unnormalized(text):\n",
    "    out = [tok for tok in ininormer.tokenize(text)]\n",
    "    return out\n",
    "\n",
    "def tokenize_kz_normalized(text):\n",
    "    out = [tok for tok in ininormer.tokenize(text)]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_TOKEN = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\"    \n",
    "SOS_TOKEN = \"<s>\"\n",
    "EOS_TOKEN = \"</s>\"\n",
    "LOWER = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = data.Field(tokenize=tokenize_kz_unnormalized, batch_first=True, lower=LOWER, include_lengths=True, unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=None, eos_token=EOS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG = data.Field(tokenize=tokenize_kz_normalized, batch_first=True, lower=LOWER, include_lengths=True, unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import TranslationDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "\n",
    "class MyIterableDatasetForOneField(torch.utils.data.IterableDataset):\n",
    "    \n",
    "    def line_mapper(self, line):\n",
    "        text = line.strip()\n",
    "        return text\n",
    "        \n",
    "        \n",
    "    \n",
    "    def __init__(self, path, exts, fields, **kwargs):\n",
    "        self.fields = fields\n",
    "        #if not isinstance(fields[0], (tuple, list)):\n",
    "        #    fields = [('src', fields[0]), ('trg', fields[1])]\n",
    "\n",
    "        src_path = os.path.expanduser(path + exts)\n",
    "\n",
    "        self.src_file = io.open(src_path, mode='r', encoding='utf-8')\n",
    "        #self.trg_file = io.open(trg_path, mode='r', encoding='utf-8')\n",
    "        \n",
    "        self.examples = []\n",
    "        super(torch.utils.data.IterableDataset, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        #self.mapped_ut_iter = map(self.line_mapper, self.src_file)\n",
    "        #self.mapped_nt_iter = map(self.line_mapper, self.trg_file)\n",
    "        \n",
    "        examples = []\n",
    "        for example in self.src_file:\n",
    "            itertools.repeat(float(price),len(bids))\n",
    "            examples.append(data.Example.fromlist(itertools.repeat(example, self.fields)))\n",
    "        \n",
    "        #zipped_itr = self.mapped_ut_iter\n",
    "        \n",
    "        #examples = []\n",
    "        \n",
    "        #for itr, value in enumerate(mapped_ut_iter):\n",
    "        #    examples.append()\n",
    "        \n",
    "        #example = zip(data.Example.fromlist(zipped_itr, self.fields))\n",
    "        \n",
    "        \n",
    "        return examples\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterableDatasetForTwoFields(torch.utils.data.IterableDataset):\n",
    "    \n",
    "    def line_mapper(self, line):\n",
    "        text = line.strip()\n",
    "        return text\n",
    "        \n",
    "        \n",
    "    \n",
    "    def __init__(self, path, exts, fields, **kwargs):\n",
    "        self.fields = fields\n",
    "        if not isinstance(fields[0], (tuple, list)):\n",
    "            fields = [('src', fields[0]), ('trg', fields[1])]\n",
    "\n",
    "        src_path, trg_path = tuple(os.path.expanduser(path + x) for x in exts)\n",
    "\n",
    "        self.src_file = open(src_path)\n",
    "        self.trg_file = open(trg_path)\n",
    "        \n",
    "        self.examples = []\n",
    "        super(torch.utils.data.IterableDataset, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        examples_src = []\n",
    "        for example in self.src_file:\n",
    "            \n",
    "            examples_src.append(example)\n",
    "            \n",
    "        print(\"EXAMPLES_SRC\", examples_src)\n",
    "        examples_trg = []\n",
    "        for example in self.trg_file:\n",
    "            examples_trg.append(example)\n",
    "            \n",
    "        print(\"EXAMPLES_TRG\", examples_trg)\n",
    "        \n",
    "        #examples = zip(examples_src, examples_trg)\n",
    "        #print(\"EXAMPLES\", examples)\n",
    "        #del examples_src\n",
    "        #del examples_trg\n",
    "        \n",
    "        answer = data.Example.fromlist([example_src, example_trg], self.fields)\n",
    "            \n",
    "            \n",
    "        #self.mapped_ut_iter = map(self.line_mapper, self.src_file)\n",
    "        #self.mapped_nt_iter = map(self.line_mapper, self.trg_file)\n",
    "        \n",
    "        #zipped_itr = zip(self.mapped_ut_iter, self.mapped_nt_iter)\n",
    "        \n",
    "        #examples = []\n",
    "        \n",
    "        #for itr, value in enumerate(mapped_ut_iter):\n",
    "        #    examples.append()\n",
    "        \n",
    "        #example = zip(data.Example.fromlist(zipped_itr, self.fields))\n",
    "        print(\"ANSWER\", answer)\n",
    "        \n",
    "        return answer\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset_src = MyIterableDatasetForOneField(path='./', exts='data.ut', fields=SRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_trg = MyIterableDatasetForOneField(path='./', exts='data.nt', fields=TRG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_all = MyIterableDatasetForTwoFields(path='./', exts=('data.ut', 'data.nt'), fields=(SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  torch.utils.data import DataLoader\n",
    "\n",
    "src_dataloader = DataLoader(dataset_src, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_dataloader = DataLoader(dataset_trg, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataloader = DataLoader(dataset_all, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchtext import data, datasets\n",
    "\n",
    "if True:\n",
    "\n",
    "    UNK_TOKEN = \"<unk>\"\n",
    "    PAD_TOKEN = \"<pad>\"    \n",
    "    SOS_TOKEN = \"<s>\"\n",
    "    EOS_TOKEN = \"</s>\"\n",
    "    LOWER = True\n",
    "    \n",
    "    # we include lengths to provide to the RNNs\n",
    "    SRC = data.Field(tokenize=tokenize_kz_unnormalized,\n",
    "                         batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                         unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=None, eos_token=EOS_TOKEN)\n",
    "                     \n",
    "    TRG = data.Field(tokenize=tokenize_kz_normalized,\n",
    "                         batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                         unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)                     \n",
    "\n",
    "    MAX_LEN = 25  # NOTE: we filter out a lot of sentences for speed\n",
    "    #train_data, valid_data, test_data = text_dataset.splits(path='./', exts=('.ut', '.nt'), fields=(SRC, TRG), filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN)\n",
    "    MIN_FREQ = 5  # NOTE: we limit the vocabulary to frequent words for speed\n",
    "    SRC.build_vocab(all_dataloader, min_freq=MIN_FREQ)\n",
    "    #TRG.build_vocab(trg_dataloader, min_freq=MIN_FREQ)\n",
    "    \n",
    "    PAD_INDEX = TRG.vocab.stoi[PAD_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_info(train_data, src_field, trg_field):\n",
    "    \"\"\" This prints some useful stuff about our data sets. \"\"\"\n",
    "\n",
    "    #print(\"Data set sizes (number of sentence pairs):\")\n",
    "    #print('train', len(train_data))\n",
    "    #print('valid', len(valid_data))\n",
    "    #print('test', len(test_data), \"\\n\")\n",
    "\n",
    "    '''print(\"First training example:\")\n",
    "    print(\"src:\", \" \".join(vars(train_data[0])['src']))\n",
    "    print(\"trg:\", \" \".join(vars(train_data[0])['trg']), \"\\n\")'''\n",
    "\n",
    "    print(\"Most common words (src):\")\n",
    "    print(\"\\n\".join([\"%10s %10d\" % x for x in src_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
    "    print(\"Most common words (trg):\")\n",
    "    print(\"\\n\".join([\"%10s %10d\" % x for x in trg_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
    "\n",
    "    '''print(\"First 10 words (src):\")\n",
    "    print(\"\\n\".join(\n",
    "        '%02d %s' % (i, t) for i, t in enumerate(src_field.vocab.itos[:10])), \"\\n\")\n",
    "    print(\"First 10 words (trg):\")\n",
    "    print(\"\\n\".join(\n",
    "        '%02d %s' % (i, t) for i, t in enumerate(trg_field.vocab.itos[:10])), \"\\n\")\n",
    "\n",
    "    print(\"Number of Unnormalized words (types):\", len(src_field.vocab))\n",
    "    print(\"Number of normalizeds (types):\", len(trg_field.vocab), \"\\n\")'''\n",
    "    \n",
    "    \n",
    "print_data_info(dataset_all, SRC, TRG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = data.BucketIterator(src_dataloader, batch_size=1, train=True, sort_within_batch=True, \n",
    "                                 sort_key=lambda x: (len(x.src), len(x.trg)), repeat=False,\n",
    "                                 device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-4ab82a5243c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m valid_iter = data.Iterator(dataset_trg, batch_size=1, train=False, sort=False, repeat=False, \n\u001b[0;32m----> 2\u001b[0;31m                            device=DEVICE)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py36pyt14/lib/python3.6/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, sort_key, device, batch_size_fn, train, repeat, shuffle, sort, sort_within_batch)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_within_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_within_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msort_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36pyt14/lib/python3.6/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attribute_name)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "valid_iter = data.Iterator(, batch_size=1, train=False, sort=False, repeat=False, \n",
    "                           device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rebatch(pad_idx, batch):\n",
    "    \"\"\"Wrap torchtext batch into our own Batch class for pre-processing\"\"\"\n",
    "    return Batch(batch.src, batch.trg, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs=10, lr=0.0003, print_every=100):\n",
    "    \"\"\"Train a model on IWSLT\"\"\"\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        model.cuda()\n",
    "\n",
    "    # optionally add label smoothing; see the Annotated Transformer\n",
    "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    dev_perplexities = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "      \n",
    "        print(\"Epoch\", epoch)\n",
    "        model.train()\n",
    "        train_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in train_iter), \n",
    "                                     model,\n",
    "                                     SimpleLossCompute(model.generator, criterion, optim),\n",
    "                                     print_every=print_every)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            print_examples((rebatch(PAD_INDEX, x) for x in valid_iter), \n",
    "                           model, n=3, src_vocab=SRC.vocab, trg_vocab=TRG.vocab)  \n",
    "            \n",
    "            dev_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in valid_iter), \n",
    "                                       model, \n",
    "                                       SimpleLossCompute(model.generator, criterion, None))\n",
    "            print(\"Validation perplexity: %f\" % dev_perplexity)\n",
    "            dev_perplexities.append(dev_perplexity)\n",
    "        \n",
    "    return dev_perplexities\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghost/anaconda3/envs/py36pyt14/lib/python3.6/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = make_model(len(SRC.vocab), len(TRG.vocab),\n",
    "                   emb_size=256, hidden_size=256,\n",
    "                   num_layers=1, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch Step: 10 Loss: 56.003826 Tokens per Sec: 156.976698\n",
      "Epoch Step: 20 Loss: 19.423466 Tokens per Sec: 686.278869\n",
      "Epoch Step: 30 Loss: 27.757423 Tokens per Sec: 898.606999\n",
      "Epoch Step: 40 Loss: 43.151146 Tokens per Sec: 913.349945\n",
      "Epoch Step: 50 Loss: 37.045105 Tokens per Sec: 766.394448\n",
      "Epoch Step: 60 Loss: 37.304745 Tokens per Sec: 895.244726\n",
      "Epoch Step: 70 Loss: 51.614880 Tokens per Sec: 924.389709\n",
      "Epoch Step: 80 Loss: 47.537582 Tokens per Sec: 894.542739\n",
      "Epoch Step: 90 Loss: 9.911974 Tokens per Sec: 766.405432\n",
      "Epoch Step: 100 Loss: 33.564816 Tokens per Sec: 886.418342\n",
      "Epoch Step: 110 Loss: 41.917564 Tokens per Sec: 802.102742\n",
      "Epoch Step: 120 Loss: 16.602837 Tokens per Sec: 877.667393\n",
      "Epoch Step: 130 Loss: 60.921822 Tokens per Sec: 883.191493\n",
      "Epoch Step: 140 Loss: 21.491993 Tokens per Sec: 858.763670\n",
      "Epoch Step: 150 Loss: 32.945202 Tokens per Sec: 910.952909\n",
      "Epoch Step: 160 Loss: 38.439415 Tokens per Sec: 896.218803\n",
      "Epoch Step: 170 Loss: 36.431477 Tokens per Sec: 843.838795\n",
      "Epoch Step: 180 Loss: 35.503696 Tokens per Sec: 891.513347\n",
      "Epoch Step: 190 Loss: 37.305779 Tokens per Sec: 812.884620\n",
      "Epoch Step: 200 Loss: 57.915714 Tokens per Sec: 872.096058\n",
      "Epoch Step: 210 Loss: 50.367237 Tokens per Sec: 880.955605\n",
      "Epoch Step: 220 Loss: 31.100368 Tokens per Sec: 852.861484\n",
      "Epoch Step: 230 Loss: 12.140144 Tokens per Sec: 888.773433\n",
      "Epoch Step: 240 Loss: 35.360989 Tokens per Sec: 879.700256\n",
      "Epoch Step: 250 Loss: 23.458317 Tokens per Sec: 855.074765\n",
      "Epoch Step: 260 Loss: 18.425371 Tokens per Sec: 844.269755\n",
      "Epoch Step: 270 Loss: 46.928024 Tokens per Sec: 871.180448\n",
      "Epoch Step: 280 Loss: 21.906191 Tokens per Sec: 871.432880\n",
      "Epoch Step: 290 Loss: 74.006279 Tokens per Sec: 835.088409\n",
      "Epoch Step: 300 Loss: 23.715973 Tokens per Sec: 871.036637\n",
      "Epoch Step: 310 Loss: 31.968357 Tokens per Sec: 868.890115\n",
      "Epoch Step: 320 Loss: 64.308861 Tokens per Sec: 880.471767\n",
      "Epoch Step: 330 Loss: 57.602406 Tokens per Sec: 846.977669\n",
      "Epoch Step: 340 Loss: 16.505146 Tokens per Sec: 887.062406\n",
      "Epoch Step: 350 Loss: 45.822037 Tokens per Sec: 861.700724\n",
      "Epoch Step: 360 Loss: 51.866524 Tokens per Sec: 910.386027\n",
      "Epoch Step: 370 Loss: 66.842674 Tokens per Sec: 888.552240\n",
      "Epoch Step: 380 Loss: 34.063934 Tokens per Sec: 903.600868\n",
      "Epoch Step: 390 Loss: 59.853642 Tokens per Sec: 893.483941\n",
      "Epoch Step: 400 Loss: 24.068108 Tokens per Sec: 806.077623\n",
      "Epoch Step: 410 Loss: 73.615532 Tokens per Sec: 900.754948\n",
      "Epoch Step: 420 Loss: 23.271317 Tokens per Sec: 906.143689\n",
      "Epoch Step: 430 Loss: 32.964195 Tokens per Sec: 837.814801\n",
      "Epoch Step: 440 Loss: 36.936451 Tokens per Sec: 701.524128\n",
      "Epoch Step: 450 Loss: 61.877510 Tokens per Sec: 868.278061\n",
      "Epoch Step: 460 Loss: 55.283310 Tokens per Sec: 901.727383\n",
      "Epoch Step: 470 Loss: 18.077938 Tokens per Sec: 780.150096\n",
      "Epoch Step: 480 Loss: 4.581265 Tokens per Sec: 699.748534\n",
      "Epoch Step: 490 Loss: 19.105961 Tokens per Sec: 857.627841\n",
      "Epoch Step: 500 Loss: 32.645279 Tokens per Sec: 865.969524\n",
      "Epoch Step: 510 Loss: 1.931338 Tokens per Sec: 799.006632\n",
      "Epoch Step: 520 Loss: 102.069290 Tokens per Sec: 838.003751\n",
      "Epoch Step: 530 Loss: 23.972145 Tokens per Sec: 823.319435\n",
      "Epoch Step: 540 Loss: 21.157940 Tokens per Sec: 753.344711\n",
      "Epoch Step: 550 Loss: 83.824638 Tokens per Sec: 787.872096\n",
      "Epoch Step: 560 Loss: 59.064117 Tokens per Sec: 896.242278\n",
      "Epoch Step: 570 Loss: 16.821039 Tokens per Sec: 896.416150\n",
      "Epoch Step: 580 Loss: 32.905964 Tokens per Sec: 857.166666\n",
      "Epoch Step: 590 Loss: 38.690056 Tokens per Sec: 868.618579\n",
      "Epoch Step: 600 Loss: 31.170889 Tokens per Sec: 912.847935\n",
      "Epoch Step: 610 Loss: 22.800489 Tokens per Sec: 831.688934\n",
      "Epoch Step: 620 Loss: 44.941341 Tokens per Sec: 822.109379\n",
      "Epoch Step: 630 Loss: 16.366571 Tokens per Sec: 862.596268\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghost/anaconda3/envs/py36pyt14/lib/python3.6/site-packages/torchtext/data/field.py:321: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train), lengths\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example #1\n",
      "Src :  , <unk>\n",
      "Trg :  , <unk> <unk>\n",
      "Pred:  , <unk> ,\n",
      "\n",
      "Example #2\n",
      "Src :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ . \"\n",
      "Trg :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ \"\n",
      "Pred:  <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "\n",
      "Example #3\n",
      "Src :  1 , енді бұл екі <unk> <unk> <unk> ##к . . .\n",
      "Trg :  <unk> , енді бұл екі <unk> <unk> <unk> ##к\n",
      "Pred:  өзен ұзындығы <unk> , <unk> <unk>\n",
      "\n",
      "Validation perplexity: 13.459437\n",
      "Epoch 1\n",
      "Epoch Step: 10 Loss: 45.937901 Tokens per Sec: 884.012016\n",
      "Epoch Step: 20 Loss: 28.128876 Tokens per Sec: 833.652090\n",
      "Epoch Step: 30 Loss: 58.787945 Tokens per Sec: 909.082494\n",
      "Epoch Step: 40 Loss: 11.356787 Tokens per Sec: 845.339112\n",
      "Epoch Step: 50 Loss: 29.745537 Tokens per Sec: 872.610286\n",
      "Epoch Step: 60 Loss: 26.659351 Tokens per Sec: 819.582027\n",
      "Epoch Step: 70 Loss: 22.467369 Tokens per Sec: 856.957329\n",
      "Epoch Step: 80 Loss: 59.596432 Tokens per Sec: 879.417464\n",
      "Epoch Step: 90 Loss: 7.796379 Tokens per Sec: 881.909145\n",
      "Epoch Step: 100 Loss: 26.128687 Tokens per Sec: 843.380019\n",
      "Epoch Step: 110 Loss: 16.693178 Tokens per Sec: 878.021847\n",
      "Epoch Step: 120 Loss: 37.654842 Tokens per Sec: 819.088291\n",
      "Epoch Step: 130 Loss: 7.041137 Tokens per Sec: 869.969625\n",
      "Epoch Step: 140 Loss: 19.175867 Tokens per Sec: 821.444790\n",
      "Epoch Step: 150 Loss: 80.034027 Tokens per Sec: 905.302885\n",
      "Epoch Step: 160 Loss: 32.481258 Tokens per Sec: 864.901442\n",
      "Epoch Step: 170 Loss: 3.531798 Tokens per Sec: 871.505848\n",
      "Epoch Step: 180 Loss: 32.060490 Tokens per Sec: 856.615544\n",
      "Epoch Step: 190 Loss: 20.788689 Tokens per Sec: 893.156430\n",
      "Epoch Step: 200 Loss: 54.932964 Tokens per Sec: 877.907256\n",
      "Epoch Step: 210 Loss: 9.514191 Tokens per Sec: 851.657106\n",
      "Epoch Step: 220 Loss: 57.388077 Tokens per Sec: 782.239316\n",
      "Epoch Step: 230 Loss: 24.219749 Tokens per Sec: 913.270889\n",
      "Epoch Step: 240 Loss: 42.706577 Tokens per Sec: 895.731615\n",
      "Epoch Step: 250 Loss: 22.234957 Tokens per Sec: 851.776594\n",
      "Epoch Step: 260 Loss: 2.802433 Tokens per Sec: 853.228396\n",
      "Epoch Step: 270 Loss: 40.165432 Tokens per Sec: 854.420526\n",
      "Epoch Step: 280 Loss: 28.070534 Tokens per Sec: 826.331715\n",
      "Epoch Step: 290 Loss: 27.213030 Tokens per Sec: 888.419927\n",
      "Epoch Step: 300 Loss: 43.278564 Tokens per Sec: 870.107183\n",
      "Epoch Step: 310 Loss: 49.998440 Tokens per Sec: 920.149098\n",
      "Epoch Step: 320 Loss: 36.201672 Tokens per Sec: 843.138949\n",
      "Epoch Step: 330 Loss: 17.027945 Tokens per Sec: 864.629573\n",
      "Epoch Step: 340 Loss: 34.827820 Tokens per Sec: 831.065829\n",
      "Epoch Step: 350 Loss: 64.435463 Tokens per Sec: 933.039170\n",
      "Epoch Step: 360 Loss: 5.390264 Tokens per Sec: 879.074668\n",
      "Epoch Step: 370 Loss: 25.960926 Tokens per Sec: 874.094765\n",
      "Epoch Step: 380 Loss: 68.651283 Tokens per Sec: 891.984439\n",
      "Epoch Step: 390 Loss: 13.257360 Tokens per Sec: 897.502310\n",
      "Epoch Step: 400 Loss: 16.114988 Tokens per Sec: 861.116452\n",
      "Epoch Step: 410 Loss: 58.925594 Tokens per Sec: 879.931942\n",
      "Epoch Step: 420 Loss: 48.845436 Tokens per Sec: 882.570539\n",
      "Epoch Step: 430 Loss: 40.014729 Tokens per Sec: 857.396146\n",
      "Epoch Step: 440 Loss: 39.156540 Tokens per Sec: 933.304113\n",
      "Epoch Step: 450 Loss: 12.776793 Tokens per Sec: 925.912958\n",
      "Epoch Step: 460 Loss: 42.814922 Tokens per Sec: 870.458441\n",
      "Epoch Step: 470 Loss: 5.281967 Tokens per Sec: 846.039274\n",
      "Epoch Step: 480 Loss: 13.754666 Tokens per Sec: 889.459286\n",
      "Epoch Step: 490 Loss: 3.541336 Tokens per Sec: 904.311276\n",
      "Epoch Step: 500 Loss: 41.761246 Tokens per Sec: 845.218651\n",
      "Epoch Step: 510 Loss: 46.242519 Tokens per Sec: 901.919936\n",
      "Epoch Step: 520 Loss: 47.309704 Tokens per Sec: 843.401675\n",
      "Epoch Step: 530 Loss: 8.360040 Tokens per Sec: 762.839689\n",
      "Epoch Step: 540 Loss: 45.026127 Tokens per Sec: 886.771998\n",
      "Epoch Step: 550 Loss: 47.811115 Tokens per Sec: 872.803574\n",
      "Epoch Step: 560 Loss: 32.326702 Tokens per Sec: 805.010479\n",
      "Epoch Step: 570 Loss: 29.955725 Tokens per Sec: 881.477541\n",
      "Epoch Step: 580 Loss: 42.061779 Tokens per Sec: 837.445380\n",
      "Epoch Step: 590 Loss: 97.638168 Tokens per Sec: 887.327715\n",
      "Epoch Step: 600 Loss: 26.186752 Tokens per Sec: 870.959386\n",
      "Epoch Step: 610 Loss: 65.155846 Tokens per Sec: 851.938049\n",
      "Epoch Step: 620 Loss: 40.267467 Tokens per Sec: 859.368982\n",
      "Epoch Step: 630 Loss: 30.026291 Tokens per Sec: 910.229194\n",
      "\n",
      "Example #1\n",
      "Src :  , <unk>\n",
      "Trg :  , <unk> <unk>\n",
      "Pred:  , <unk> , <unk>\n",
      "\n",
      "Example #2\n",
      "Src :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ . \"\n",
      "Trg :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ \"\n",
      "Pred:  <unk> <unk> <unk> \" , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "\n",
      "Example #3\n",
      "Src :  1 , енді бұл екі <unk> <unk> <unk> ##к . . .\n",
      "Trg :  <unk> , енді бұл екі <unk> <unk> <unk> ##к\n",
      "Pred:  , <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "\n",
      "Validation perplexity: 8.905369\n",
      "Epoch 2\n",
      "Epoch Step: 10 Loss: 22.947523 Tokens per Sec: 904.888083\n",
      "Epoch Step: 20 Loss: 35.534061 Tokens per Sec: 857.702326\n",
      "Epoch Step: 30 Loss: 34.035919 Tokens per Sec: 916.507058\n",
      "Epoch Step: 40 Loss: 33.180088 Tokens per Sec: 892.551851\n",
      "Epoch Step: 50 Loss: 28.883739 Tokens per Sec: 909.802824\n",
      "Epoch Step: 60 Loss: 34.861828 Tokens per Sec: 871.128919\n",
      "Epoch Step: 70 Loss: 19.294285 Tokens per Sec: 921.871046\n",
      "Epoch Step: 80 Loss: 24.566341 Tokens per Sec: 851.119603\n",
      "Epoch Step: 90 Loss: 11.161242 Tokens per Sec: 870.071000\n",
      "Epoch Step: 100 Loss: 20.776243 Tokens per Sec: 864.170729\n",
      "Epoch Step: 110 Loss: 34.135887 Tokens per Sec: 847.126122\n",
      "Epoch Step: 120 Loss: 24.114216 Tokens per Sec: 818.994742\n",
      "Epoch Step: 130 Loss: 19.784540 Tokens per Sec: 953.226838\n",
      "Epoch Step: 140 Loss: 41.410358 Tokens per Sec: 781.298808\n",
      "Epoch Step: 150 Loss: 26.305353 Tokens per Sec: 885.897283\n",
      "Epoch Step: 160 Loss: 4.137086 Tokens per Sec: 911.420024\n",
      "Epoch Step: 170 Loss: 27.242140 Tokens per Sec: 836.795310\n",
      "Epoch Step: 180 Loss: 24.345671 Tokens per Sec: 859.159127\n",
      "Epoch Step: 190 Loss: 14.584504 Tokens per Sec: 824.763479\n",
      "Epoch Step: 200 Loss: 37.448971 Tokens per Sec: 838.387979\n",
      "Epoch Step: 210 Loss: 22.333412 Tokens per Sec: 833.036158\n",
      "Epoch Step: 220 Loss: 50.677792 Tokens per Sec: 914.805681\n",
      "Epoch Step: 230 Loss: 21.534710 Tokens per Sec: 819.787529\n",
      "Epoch Step: 240 Loss: 2.900106 Tokens per Sec: 861.910351\n",
      "Epoch Step: 250 Loss: 23.817633 Tokens per Sec: 869.074629\n",
      "Epoch Step: 260 Loss: 6.560137 Tokens per Sec: 838.365200\n",
      "Epoch Step: 270 Loss: 8.216115 Tokens per Sec: 857.692041\n",
      "Epoch Step: 280 Loss: 2.830369 Tokens per Sec: 888.019280\n",
      "Epoch Step: 290 Loss: 30.966114 Tokens per Sec: 815.819525\n",
      "Epoch Step: 300 Loss: 28.692924 Tokens per Sec: 908.882737\n",
      "Epoch Step: 310 Loss: 27.160412 Tokens per Sec: 852.861842\n",
      "Epoch Step: 320 Loss: 69.563545 Tokens per Sec: 878.631424\n",
      "Epoch Step: 330 Loss: 13.403771 Tokens per Sec: 881.912227\n",
      "Epoch Step: 340 Loss: 14.763082 Tokens per Sec: 912.787456\n",
      "Epoch Step: 350 Loss: 17.020477 Tokens per Sec: 822.676020\n",
      "Epoch Step: 360 Loss: 59.311840 Tokens per Sec: 842.596310\n",
      "Epoch Step: 370 Loss: 27.939268 Tokens per Sec: 898.483991\n",
      "Epoch Step: 380 Loss: 34.579224 Tokens per Sec: 877.069120\n",
      "Epoch Step: 390 Loss: 7.310859 Tokens per Sec: 847.777992\n",
      "Epoch Step: 400 Loss: 35.666737 Tokens per Sec: 900.239102\n",
      "Epoch Step: 410 Loss: 8.575484 Tokens per Sec: 809.976750\n",
      "Epoch Step: 420 Loss: 11.322987 Tokens per Sec: 891.222532\n",
      "Epoch Step: 430 Loss: 4.344738 Tokens per Sec: 881.241205\n",
      "Epoch Step: 440 Loss: 15.050867 Tokens per Sec: 909.801749\n",
      "Epoch Step: 450 Loss: 11.841333 Tokens per Sec: 823.213164\n",
      "Epoch Step: 460 Loss: 11.305522 Tokens per Sec: 884.593013\n",
      "Epoch Step: 470 Loss: 38.032085 Tokens per Sec: 884.374792\n",
      "Epoch Step: 480 Loss: 44.366287 Tokens per Sec: 874.232236\n",
      "Epoch Step: 490 Loss: 21.254326 Tokens per Sec: 867.641715\n",
      "Epoch Step: 500 Loss: 47.024403 Tokens per Sec: 874.972246\n",
      "Epoch Step: 510 Loss: 7.675154 Tokens per Sec: 896.269201\n",
      "Epoch Step: 520 Loss: 20.851875 Tokens per Sec: 863.772793\n",
      "Epoch Step: 530 Loss: 35.535126 Tokens per Sec: 878.462568\n",
      "Epoch Step: 540 Loss: 15.949784 Tokens per Sec: 909.186365\n",
      "Epoch Step: 550 Loss: 50.505795 Tokens per Sec: 890.716905\n",
      "Epoch Step: 560 Loss: 20.105421 Tokens per Sec: 872.920088\n",
      "Epoch Step: 570 Loss: 36.276722 Tokens per Sec: 869.481624\n",
      "Epoch Step: 580 Loss: 8.378222 Tokens per Sec: 845.515003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 590 Loss: 1.550956 Tokens per Sec: 843.425600\n",
      "Epoch Step: 600 Loss: 21.007647 Tokens per Sec: 838.264857\n",
      "Epoch Step: 610 Loss: 20.758793 Tokens per Sec: 852.365125\n",
      "Epoch Step: 620 Loss: 29.375467 Tokens per Sec: 894.394458\n",
      "Epoch Step: 630 Loss: 3.611887 Tokens per Sec: 831.288034\n",
      "\n",
      "Example #1\n",
      "Src :  , <unk>\n",
      "Trg :  , <unk> <unk>\n",
      "Pred:  <unk> , <unk> <unk>\n",
      "\n",
      "Example #2\n",
      "Src :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ . \"\n",
      "Trg :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ \"\n",
      "Pred:  <unk> <unk> <unk> \" , <unk> \" <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##нде орналасқан\n",
      "\n",
      "Example #3\n",
      "Src :  1 , енді бұл екі <unk> <unk> <unk> ##к . . .\n",
      "Trg :  <unk> , енді бұл екі <unk> <unk> <unk> ##к\n",
      "Pred:  екі мың тоғыз жүз ел ##у <unk> <unk> <unk>\n",
      "\n",
      "Validation perplexity: 7.783367\n",
      "Epoch 3\n",
      "Epoch Step: 10 Loss: 28.463001 Tokens per Sec: 897.386610\n",
      "Epoch Step: 20 Loss: 25.053308 Tokens per Sec: 890.215991\n",
      "Epoch Step: 30 Loss: 28.707727 Tokens per Sec: 832.393626\n",
      "Epoch Step: 40 Loss: 44.312626 Tokens per Sec: 898.584582\n",
      "Epoch Step: 50 Loss: 48.835396 Tokens per Sec: 889.718006\n",
      "Epoch Step: 60 Loss: 13.749228 Tokens per Sec: 876.610413\n",
      "Epoch Step: 70 Loss: 14.594963 Tokens per Sec: 807.199402\n",
      "Epoch Step: 80 Loss: 32.009037 Tokens per Sec: 834.798961\n",
      "Epoch Step: 90 Loss: 29.998606 Tokens per Sec: 885.775345\n",
      "Epoch Step: 100 Loss: 7.389154 Tokens per Sec: 854.417298\n",
      "Epoch Step: 110 Loss: 51.196808 Tokens per Sec: 858.139460\n",
      "Epoch Step: 120 Loss: 9.113211 Tokens per Sec: 848.782690\n",
      "Epoch Step: 130 Loss: 14.112107 Tokens per Sec: 843.965384\n",
      "Epoch Step: 140 Loss: 0.970403 Tokens per Sec: 840.362949\n",
      "Epoch Step: 150 Loss: 21.991278 Tokens per Sec: 858.957790\n",
      "Epoch Step: 160 Loss: 12.441488 Tokens per Sec: 810.198105\n",
      "Epoch Step: 170 Loss: 18.008108 Tokens per Sec: 919.486477\n",
      "Epoch Step: 180 Loss: 6.639384 Tokens per Sec: 781.346069\n",
      "Epoch Step: 190 Loss: 20.222391 Tokens per Sec: 921.797483\n",
      "Epoch Step: 200 Loss: 32.410923 Tokens per Sec: 838.515771\n",
      "Epoch Step: 210 Loss: 21.414993 Tokens per Sec: 921.692119\n",
      "Epoch Step: 220 Loss: 55.134426 Tokens per Sec: 906.289879\n",
      "Epoch Step: 230 Loss: 6.158484 Tokens per Sec: 875.601288\n",
      "Epoch Step: 240 Loss: 11.448515 Tokens per Sec: 824.684901\n",
      "Epoch Step: 250 Loss: 18.355104 Tokens per Sec: 916.133864\n",
      "Epoch Step: 260 Loss: 16.697603 Tokens per Sec: 918.192644\n",
      "Epoch Step: 270 Loss: 18.267715 Tokens per Sec: 929.962659\n",
      "Epoch Step: 280 Loss: 23.029488 Tokens per Sec: 832.800254\n",
      "Epoch Step: 290 Loss: 30.166954 Tokens per Sec: 863.602692\n",
      "Epoch Step: 300 Loss: 8.421371 Tokens per Sec: 875.863383\n",
      "Epoch Step: 310 Loss: 29.640831 Tokens per Sec: 884.889297\n",
      "Epoch Step: 320 Loss: 5.860489 Tokens per Sec: 878.750321\n",
      "Epoch Step: 330 Loss: 63.678871 Tokens per Sec: 905.321256\n",
      "Epoch Step: 340 Loss: 54.887264 Tokens per Sec: 900.947553\n",
      "Epoch Step: 350 Loss: 65.142548 Tokens per Sec: 875.907266\n",
      "Epoch Step: 360 Loss: 49.386444 Tokens per Sec: 859.292316\n",
      "Epoch Step: 370 Loss: 9.911563 Tokens per Sec: 875.895423\n",
      "Epoch Step: 380 Loss: 36.586636 Tokens per Sec: 868.485816\n",
      "Epoch Step: 390 Loss: 20.722668 Tokens per Sec: 877.559958\n",
      "Epoch Step: 400 Loss: 6.776134 Tokens per Sec: 850.373743\n",
      "Epoch Step: 410 Loss: 7.801724 Tokens per Sec: 898.933778\n",
      "Epoch Step: 420 Loss: 8.514183 Tokens per Sec: 884.472722\n",
      "Epoch Step: 430 Loss: 6.814623 Tokens per Sec: 831.093048\n",
      "Epoch Step: 440 Loss: 13.972481 Tokens per Sec: 885.905760\n",
      "Epoch Step: 450 Loss: 7.738823 Tokens per Sec: 847.254172\n",
      "Epoch Step: 460 Loss: 4.726429 Tokens per Sec: 850.289957\n",
      "Epoch Step: 470 Loss: 15.929296 Tokens per Sec: 896.454843\n",
      "Epoch Step: 480 Loss: 12.955203 Tokens per Sec: 872.557265\n",
      "Epoch Step: 490 Loss: 25.145391 Tokens per Sec: 857.890879\n",
      "Epoch Step: 500 Loss: 2.291873 Tokens per Sec: 827.462994\n",
      "Epoch Step: 510 Loss: 4.270047 Tokens per Sec: 808.930376\n",
      "Epoch Step: 520 Loss: 17.860994 Tokens per Sec: 846.709142\n",
      "Epoch Step: 530 Loss: 9.348021 Tokens per Sec: 822.008864\n",
      "Epoch Step: 540 Loss: 25.054810 Tokens per Sec: 828.234615\n",
      "Epoch Step: 550 Loss: 4.701299 Tokens per Sec: 887.815422\n",
      "Epoch Step: 560 Loss: 20.913712 Tokens per Sec: 865.495048\n",
      "Epoch Step: 570 Loss: 14.916428 Tokens per Sec: 905.131007\n",
      "Epoch Step: 580 Loss: 43.696869 Tokens per Sec: 852.380574\n",
      "Epoch Step: 590 Loss: 46.453716 Tokens per Sec: 871.805740\n",
      "Epoch Step: 600 Loss: 15.703518 Tokens per Sec: 869.391217\n",
      "Epoch Step: 610 Loss: 7.604652 Tokens per Sec: 906.891528\n",
      "Epoch Step: 620 Loss: 10.846564 Tokens per Sec: 882.470675\n",
      "Epoch Step: 630 Loss: 20.910631 Tokens per Sec: 901.666048\n",
      "\n",
      "Example #1\n",
      "Src :  , <unk>\n",
      "Trg :  , <unk> <unk>\n",
      "Pred:  ,\n",
      "\n",
      "Example #2\n",
      "Src :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ . \"\n",
      "Trg :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ \"\n",
      "Pred:  <unk> , \" \" \" <unk> \" \" \" <unk> \" \" <unk> \" <unk> <unk> \" <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> , деді <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "\n",
      "Example #3\n",
      "Src :  1 , енді бұл екі <unk> <unk> <unk> ##к . . .\n",
      "Trg :  <unk> , енді бұл екі <unk> <unk> <unk> ##к\n",
      "Pred:  алып , <unk> <unk>\n",
      "\n",
      "Validation perplexity: 6.568071\n",
      "Epoch 4\n",
      "Epoch Step: 10 Loss: 18.363268 Tokens per Sec: 895.338246\n",
      "Epoch Step: 20 Loss: 14.484833 Tokens per Sec: 866.765946\n",
      "Epoch Step: 30 Loss: 1.026285 Tokens per Sec: 883.681909\n",
      "Epoch Step: 40 Loss: 2.553842 Tokens per Sec: 871.005970\n",
      "Epoch Step: 50 Loss: 38.866798 Tokens per Sec: 918.292541\n",
      "Epoch Step: 60 Loss: 11.310201 Tokens per Sec: 866.783435\n",
      "Epoch Step: 70 Loss: 0.873688 Tokens per Sec: 822.637165\n",
      "Epoch Step: 80 Loss: 40.365479 Tokens per Sec: 859.167706\n",
      "Epoch Step: 90 Loss: 4.176341 Tokens per Sec: 913.790385\n",
      "Epoch Step: 100 Loss: 15.427000 Tokens per Sec: 863.965755\n",
      "Epoch Step: 110 Loss: 8.207653 Tokens per Sec: 879.574263\n",
      "Epoch Step: 120 Loss: 0.524136 Tokens per Sec: 872.790532\n",
      "Epoch Step: 130 Loss: 1.255490 Tokens per Sec: 849.843159\n",
      "Epoch Step: 140 Loss: 25.565355 Tokens per Sec: 892.547534\n",
      "Epoch Step: 150 Loss: 15.887293 Tokens per Sec: 888.532183\n",
      "Epoch Step: 160 Loss: 8.995367 Tokens per Sec: 889.840809\n",
      "Epoch Step: 170 Loss: 15.089200 Tokens per Sec: 840.543415\n",
      "Epoch Step: 180 Loss: 7.987352 Tokens per Sec: 889.331876\n",
      "Epoch Step: 190 Loss: 6.997197 Tokens per Sec: 910.841399\n",
      "Epoch Step: 200 Loss: 12.043422 Tokens per Sec: 860.284574\n",
      "Epoch Step: 210 Loss: 42.448372 Tokens per Sec: 894.311975\n",
      "Epoch Step: 220 Loss: 7.822403 Tokens per Sec: 870.191564\n",
      "Epoch Step: 230 Loss: 10.650760 Tokens per Sec: 890.938427\n",
      "Epoch Step: 240 Loss: 42.371540 Tokens per Sec: 848.649485\n",
      "Epoch Step: 250 Loss: 22.250340 Tokens per Sec: 928.355377\n",
      "Epoch Step: 260 Loss: 7.536372 Tokens per Sec: 895.807333\n",
      "Epoch Step: 270 Loss: 41.573200 Tokens per Sec: 931.064579\n",
      "Epoch Step: 280 Loss: 11.447478 Tokens per Sec: 881.063754\n",
      "Epoch Step: 290 Loss: 11.518263 Tokens per Sec: 913.565369\n",
      "Epoch Step: 300 Loss: 17.309406 Tokens per Sec: 857.821840\n",
      "Epoch Step: 310 Loss: 22.864008 Tokens per Sec: 915.767550\n",
      "Epoch Step: 320 Loss: 21.668119 Tokens per Sec: 902.123195\n",
      "Epoch Step: 330 Loss: 6.249435 Tokens per Sec: 854.704241\n",
      "Epoch Step: 340 Loss: 4.594739 Tokens per Sec: 848.524476\n",
      "Epoch Step: 350 Loss: 16.755358 Tokens per Sec: 901.829501\n",
      "Epoch Step: 360 Loss: 8.829022 Tokens per Sec: 894.455662\n",
      "Epoch Step: 370 Loss: 28.750832 Tokens per Sec: 911.848254\n",
      "Epoch Step: 380 Loss: 3.044196 Tokens per Sec: 786.293505\n",
      "Epoch Step: 390 Loss: 24.152733 Tokens per Sec: 886.418133\n",
      "Epoch Step: 400 Loss: 15.405207 Tokens per Sec: 923.780542\n",
      "Epoch Step: 410 Loss: 1.125058 Tokens per Sec: 875.095167\n",
      "Epoch Step: 420 Loss: 11.761266 Tokens per Sec: 856.742234\n",
      "Epoch Step: 430 Loss: 8.740541 Tokens per Sec: 919.248162\n",
      "Epoch Step: 440 Loss: 2.965211 Tokens per Sec: 902.805910\n",
      "Epoch Step: 450 Loss: 3.720827 Tokens per Sec: 894.945773\n",
      "Epoch Step: 460 Loss: 4.741822 Tokens per Sec: 913.625849\n",
      "Epoch Step: 470 Loss: 19.027830 Tokens per Sec: 907.182690\n",
      "Epoch Step: 480 Loss: 8.122068 Tokens per Sec: 873.763688\n",
      "Epoch Step: 490 Loss: 0.131864 Tokens per Sec: 756.770157\n",
      "Epoch Step: 500 Loss: 13.483976 Tokens per Sec: 886.464576\n",
      "Epoch Step: 510 Loss: 20.725368 Tokens per Sec: 876.103276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 520 Loss: 12.549395 Tokens per Sec: 878.101768\n",
      "Epoch Step: 530 Loss: 12.627811 Tokens per Sec: 902.172887\n",
      "Epoch Step: 540 Loss: 13.755054 Tokens per Sec: 908.296006\n",
      "Epoch Step: 550 Loss: 8.469513 Tokens per Sec: 897.709083\n",
      "Epoch Step: 560 Loss: 31.365080 Tokens per Sec: 819.696447\n",
      "Epoch Step: 570 Loss: 20.102308 Tokens per Sec: 899.383032\n",
      "Epoch Step: 580 Loss: 18.145233 Tokens per Sec: 918.914260\n",
      "Epoch Step: 590 Loss: 12.686794 Tokens per Sec: 922.030201\n",
      "Epoch Step: 600 Loss: 12.391806 Tokens per Sec: 895.948986\n",
      "Epoch Step: 610 Loss: 2.245337 Tokens per Sec: 890.646444\n",
      "Epoch Step: 620 Loss: 16.645414 Tokens per Sec: 878.171045\n",
      "Epoch Step: 630 Loss: 2.815260 Tokens per Sec: 896.783019\n",
      "\n",
      "Example #1\n",
      "Src :  , <unk>\n",
      "Trg :  , <unk> <unk>\n",
      "Pred:  ,\n",
      "\n",
      "Example #2\n",
      "Src :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ . \"\n",
      "Trg :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ \"\n",
      "Pred:  <unk> , <unk> \" <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "\n",
      "Example #3\n",
      "Src :  1 , енді бұл екі <unk> <unk> <unk> ##к . . .\n",
      "Trg :  <unk> , енді бұл екі <unk> <unk> <unk> ##к\n",
      "Pred:  алып , <unk> <unk> <unk> <unk> <unk> ##к к <unk>\n",
      "\n",
      "Validation perplexity: 5.555367\n",
      "Epoch 5\n",
      "Epoch Step: 10 Loss: 6.697553 Tokens per Sec: 822.863720\n",
      "Epoch Step: 20 Loss: 5.917264 Tokens per Sec: 897.633959\n",
      "Epoch Step: 30 Loss: 9.504756 Tokens per Sec: 899.980358\n",
      "Epoch Step: 40 Loss: 17.260096 Tokens per Sec: 935.372428\n",
      "Epoch Step: 50 Loss: 15.970709 Tokens per Sec: 855.823072\n",
      "Epoch Step: 60 Loss: 11.527272 Tokens per Sec: 892.760174\n",
      "Epoch Step: 70 Loss: 4.048112 Tokens per Sec: 828.516768\n",
      "Epoch Step: 80 Loss: 3.456294 Tokens per Sec: 879.036225\n",
      "Epoch Step: 90 Loss: 28.782656 Tokens per Sec: 889.061041\n",
      "Epoch Step: 100 Loss: 8.994202 Tokens per Sec: 876.817384\n",
      "Epoch Step: 110 Loss: 1.714835 Tokens per Sec: 900.054234\n",
      "Epoch Step: 120 Loss: 2.465688 Tokens per Sec: 895.656264\n",
      "Epoch Step: 130 Loss: 11.514166 Tokens per Sec: 877.725458\n",
      "Epoch Step: 140 Loss: 1.562960 Tokens per Sec: 847.441305\n",
      "Epoch Step: 150 Loss: 2.454624 Tokens per Sec: 893.989766\n",
      "Epoch Step: 160 Loss: 15.404593 Tokens per Sec: 811.387090\n",
      "Epoch Step: 170 Loss: 0.352199 Tokens per Sec: 854.983896\n",
      "Epoch Step: 180 Loss: 8.193365 Tokens per Sec: 803.421596\n",
      "Epoch Step: 190 Loss: 0.934610 Tokens per Sec: 882.460387\n",
      "Epoch Step: 200 Loss: 9.877344 Tokens per Sec: 823.659183\n",
      "Epoch Step: 210 Loss: 10.304673 Tokens per Sec: 897.168659\n",
      "Epoch Step: 220 Loss: 11.608055 Tokens per Sec: 840.912627\n",
      "Epoch Step: 230 Loss: 0.170186 Tokens per Sec: 889.164754\n",
      "Epoch Step: 240 Loss: 10.670726 Tokens per Sec: 846.891322\n",
      "Epoch Step: 250 Loss: 9.746140 Tokens per Sec: 857.791894\n",
      "Epoch Step: 260 Loss: 5.483996 Tokens per Sec: 903.701089\n",
      "Epoch Step: 270 Loss: 23.602407 Tokens per Sec: 879.174969\n",
      "Epoch Step: 280 Loss: 8.836004 Tokens per Sec: 882.752136\n",
      "Epoch Step: 290 Loss: 11.760795 Tokens per Sec: 910.499564\n",
      "Epoch Step: 300 Loss: 10.383306 Tokens per Sec: 884.401825\n",
      "Epoch Step: 310 Loss: 5.919296 Tokens per Sec: 848.326189\n",
      "Epoch Step: 320 Loss: 7.855378 Tokens per Sec: 877.858509\n",
      "Epoch Step: 330 Loss: 11.592241 Tokens per Sec: 888.367304\n",
      "Epoch Step: 340 Loss: 16.475491 Tokens per Sec: 853.996634\n",
      "Epoch Step: 350 Loss: 4.444453 Tokens per Sec: 877.008761\n",
      "Epoch Step: 360 Loss: 8.966915 Tokens per Sec: 874.666182\n",
      "Epoch Step: 370 Loss: 33.013695 Tokens per Sec: 884.432408\n",
      "Epoch Step: 380 Loss: 1.944356 Tokens per Sec: 789.123063\n",
      "Epoch Step: 390 Loss: 6.771163 Tokens per Sec: 784.715435\n",
      "Epoch Step: 400 Loss: 8.878850 Tokens per Sec: 820.739731\n",
      "Epoch Step: 410 Loss: 8.923274 Tokens per Sec: 865.582287\n",
      "Epoch Step: 420 Loss: 1.237258 Tokens per Sec: 804.952366\n",
      "Epoch Step: 430 Loss: 14.913828 Tokens per Sec: 878.660291\n",
      "Epoch Step: 440 Loss: 24.232311 Tokens per Sec: 854.738679\n",
      "Epoch Step: 450 Loss: 32.756580 Tokens per Sec: 915.845713\n",
      "Epoch Step: 460 Loss: 23.760340 Tokens per Sec: 871.243416\n",
      "Epoch Step: 470 Loss: 3.115535 Tokens per Sec: 824.362512\n",
      "Epoch Step: 480 Loss: 10.325542 Tokens per Sec: 893.956454\n",
      "Epoch Step: 490 Loss: 11.260016 Tokens per Sec: 896.208658\n",
      "Epoch Step: 500 Loss: 17.583755 Tokens per Sec: 854.984653\n",
      "Epoch Step: 510 Loss: 2.735908 Tokens per Sec: 879.876487\n",
      "Epoch Step: 520 Loss: 24.127205 Tokens per Sec: 815.259373\n",
      "Epoch Step: 530 Loss: 11.135728 Tokens per Sec: 819.229929\n",
      "Epoch Step: 540 Loss: 39.472538 Tokens per Sec: 865.194274\n",
      "Epoch Step: 550 Loss: 11.233261 Tokens per Sec: 825.170885\n",
      "Epoch Step: 560 Loss: 23.797777 Tokens per Sec: 716.145538\n",
      "Epoch Step: 570 Loss: 10.298515 Tokens per Sec: 869.792807\n",
      "Epoch Step: 580 Loss: 26.379194 Tokens per Sec: 880.317672\n",
      "Epoch Step: 590 Loss: 2.615315 Tokens per Sec: 846.967071\n",
      "Epoch Step: 600 Loss: 1.710354 Tokens per Sec: 825.142801\n",
      "Epoch Step: 610 Loss: 3.330485 Tokens per Sec: 853.348622\n",
      "Epoch Step: 620 Loss: 5.531569 Tokens per Sec: 836.378078\n",
      "Epoch Step: 630 Loss: 36.323360 Tokens per Sec: 932.398625\n",
      "\n",
      "Example #1\n",
      "Src :  , <unk>\n",
      "Trg :  , <unk> <unk>\n",
      "Pred:  , <unk>\n",
      "\n",
      "Example #2\n",
      "Src :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ . \"\n",
      "Trg :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ \"\n",
      "Pred:  <unk> , \" ф <unk> <unk> <unk> <unk> <unk> ##ке <unk> <unk> <unk> <unk> керек\n",
      "\n",
      "Example #3\n",
      "Src :  1 , енді бұл екі <unk> <unk> <unk> ##к . . .\n",
      "Trg :  <unk> , енді бұл екі <unk> <unk> <unk> ##к\n",
      "Pred:  астана , <unk> <unk> екі жүз ал ##пы ##с үш\n",
      "\n",
      "Validation perplexity: 5.001900\n",
      "Epoch 6\n",
      "Epoch Step: 10 Loss: 3.742939 Tokens per Sec: 838.840895\n",
      "Epoch Step: 20 Loss: 3.294575 Tokens per Sec: 922.883068\n",
      "Epoch Step: 30 Loss: 18.033722 Tokens per Sec: 854.139814\n",
      "Epoch Step: 40 Loss: 1.431146 Tokens per Sec: 903.654989\n",
      "Epoch Step: 50 Loss: 5.042478 Tokens per Sec: 885.545249\n",
      "Epoch Step: 60 Loss: 17.978769 Tokens per Sec: 870.071775\n",
      "Epoch Step: 70 Loss: 0.192333 Tokens per Sec: 896.600722\n",
      "Epoch Step: 80 Loss: 2.476454 Tokens per Sec: 860.462197\n",
      "Epoch Step: 90 Loss: 4.154964 Tokens per Sec: 855.663289\n",
      "Epoch Step: 100 Loss: 1.935979 Tokens per Sec: 877.514739\n",
      "Epoch Step: 110 Loss: 18.713947 Tokens per Sec: 887.305199\n",
      "Epoch Step: 120 Loss: 37.924000 Tokens per Sec: 863.742761\n",
      "Epoch Step: 130 Loss: 1.184721 Tokens per Sec: 839.239169\n",
      "Epoch Step: 140 Loss: 3.916947 Tokens per Sec: 885.826420\n",
      "Epoch Step: 150 Loss: 1.081295 Tokens per Sec: 864.624073\n",
      "Epoch Step: 160 Loss: 3.142120 Tokens per Sec: 839.136698\n",
      "Epoch Step: 170 Loss: 4.763070 Tokens per Sec: 899.540578\n",
      "Epoch Step: 180 Loss: 1.177736 Tokens per Sec: 870.691312\n",
      "Epoch Step: 190 Loss: 0.216303 Tokens per Sec: 803.569556\n",
      "Epoch Step: 200 Loss: 5.836621 Tokens per Sec: 879.366696\n",
      "Epoch Step: 210 Loss: 10.141341 Tokens per Sec: 853.487279\n",
      "Epoch Step: 220 Loss: 8.375359 Tokens per Sec: 904.557051\n",
      "Epoch Step: 230 Loss: 0.190253 Tokens per Sec: 853.246812\n",
      "Epoch Step: 240 Loss: 2.924807 Tokens per Sec: 874.879246\n",
      "Epoch Step: 250 Loss: 24.644489 Tokens per Sec: 879.637577\n",
      "Epoch Step: 260 Loss: 5.406296 Tokens per Sec: 889.670131\n",
      "Epoch Step: 270 Loss: 4.988873 Tokens per Sec: 861.575051\n",
      "Epoch Step: 280 Loss: 2.290266 Tokens per Sec: 807.249729\n",
      "Epoch Step: 290 Loss: 2.289144 Tokens per Sec: 886.739824\n",
      "Epoch Step: 300 Loss: 16.198296 Tokens per Sec: 834.144926\n",
      "Epoch Step: 310 Loss: 13.879305 Tokens per Sec: 847.716282\n",
      "Epoch Step: 320 Loss: 1.893103 Tokens per Sec: 872.117135\n",
      "Epoch Step: 330 Loss: 0.092459 Tokens per Sec: 861.477187\n",
      "Epoch Step: 340 Loss: 3.434155 Tokens per Sec: 770.792001\n",
      "Epoch Step: 350 Loss: 8.389990 Tokens per Sec: 884.394482\n",
      "Epoch Step: 360 Loss: 10.267485 Tokens per Sec: 894.559160\n",
      "Epoch Step: 370 Loss: 14.062394 Tokens per Sec: 876.361563\n",
      "Epoch Step: 380 Loss: 9.726317 Tokens per Sec: 738.911149\n",
      "Epoch Step: 390 Loss: 11.379177 Tokens per Sec: 789.687724\n",
      "Epoch Step: 400 Loss: 14.336293 Tokens per Sec: 903.604301\n",
      "Epoch Step: 410 Loss: 1.592924 Tokens per Sec: 803.140672\n",
      "Epoch Step: 420 Loss: 6.761109 Tokens per Sec: 741.983599\n",
      "Epoch Step: 430 Loss: 9.720650 Tokens per Sec: 868.420549\n",
      "Epoch Step: 440 Loss: 14.289482 Tokens per Sec: 779.591088\n",
      "Epoch Step: 450 Loss: 20.113415 Tokens per Sec: 847.295017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 460 Loss: 15.880692 Tokens per Sec: 840.172714\n",
      "Epoch Step: 470 Loss: 6.945629 Tokens per Sec: 846.283237\n",
      "Epoch Step: 480 Loss: 5.519127 Tokens per Sec: 891.678426\n",
      "Epoch Step: 490 Loss: 10.981153 Tokens per Sec: 902.692052\n",
      "Epoch Step: 500 Loss: 0.959230 Tokens per Sec: 793.988505\n",
      "Epoch Step: 510 Loss: 23.513721 Tokens per Sec: 802.959256\n",
      "Epoch Step: 520 Loss: 7.902520 Tokens per Sec: 829.459810\n",
      "Epoch Step: 530 Loss: 13.647793 Tokens per Sec: 880.512101\n",
      "Epoch Step: 540 Loss: 15.330163 Tokens per Sec: 829.452259\n",
      "Epoch Step: 550 Loss: 9.538866 Tokens per Sec: 795.395203\n",
      "Epoch Step: 560 Loss: 5.797938 Tokens per Sec: 844.199793\n",
      "Epoch Step: 570 Loss: 5.063901 Tokens per Sec: 801.670963\n",
      "Epoch Step: 580 Loss: 3.484593 Tokens per Sec: 808.334364\n",
      "Epoch Step: 590 Loss: 28.796663 Tokens per Sec: 890.908619\n",
      "Epoch Step: 600 Loss: 11.312432 Tokens per Sec: 840.205128\n",
      "Epoch Step: 610 Loss: 8.585518 Tokens per Sec: 850.000338\n",
      "Epoch Step: 620 Loss: 17.138739 Tokens per Sec: 704.869100\n",
      "Epoch Step: 630 Loss: 10.049647 Tokens per Sec: 889.635989\n",
      "\n",
      "Example #1\n",
      "Src :  , <unk>\n",
      "Trg :  , <unk> <unk>\n",
      "Pred:  , <unk>\n",
      "\n",
      "Example #2\n",
      "Src :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ . \"\n",
      "Trg :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ \"\n",
      "Pred:  <unk> <unk> , \" <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> керек \" ала ##ң ##ға да жоқ\n",
      "\n",
      "Example #3\n",
      "Src :  1 , енді бұл екі <unk> <unk> <unk> ##к . . .\n",
      "Trg :  <unk> , енді бұл екі <unk> <unk> <unk> ##к\n",
      "Pred:  бір бүт ##ін он <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "\n",
      "Validation perplexity: 5.646099\n",
      "Epoch 7\n",
      "Epoch Step: 10 Loss: 43.387043 Tokens per Sec: 893.803784\n",
      "Epoch Step: 20 Loss: 3.514899 Tokens per Sec: 870.554758\n",
      "Epoch Step: 30 Loss: 11.271044 Tokens per Sec: 893.039259\n",
      "Epoch Step: 40 Loss: 0.024290 Tokens per Sec: 839.204411\n",
      "Epoch Step: 50 Loss: 3.117587 Tokens per Sec: 868.985221\n",
      "Epoch Step: 60 Loss: 1.930750 Tokens per Sec: 845.699995\n",
      "Epoch Step: 70 Loss: 13.440655 Tokens per Sec: 794.755716\n",
      "Epoch Step: 80 Loss: 4.125720 Tokens per Sec: 840.314678\n",
      "Epoch Step: 90 Loss: 12.288426 Tokens per Sec: 921.327635\n",
      "Epoch Step: 100 Loss: 13.977653 Tokens per Sec: 856.067762\n",
      "Epoch Step: 110 Loss: 15.527590 Tokens per Sec: 914.375483\n",
      "Epoch Step: 120 Loss: 7.973851 Tokens per Sec: 891.192476\n",
      "Epoch Step: 130 Loss: 3.509857 Tokens per Sec: 887.044291\n",
      "Epoch Step: 140 Loss: 1.967227 Tokens per Sec: 874.418894\n",
      "Epoch Step: 150 Loss: 14.306964 Tokens per Sec: 856.074282\n",
      "Epoch Step: 160 Loss: 0.979476 Tokens per Sec: 855.115540\n",
      "Epoch Step: 170 Loss: 3.655470 Tokens per Sec: 855.615811\n",
      "Epoch Step: 180 Loss: 7.824354 Tokens per Sec: 870.424722\n",
      "Epoch Step: 190 Loss: 2.973906 Tokens per Sec: 842.127971\n",
      "Epoch Step: 200 Loss: 8.621978 Tokens per Sec: 853.965066\n",
      "Epoch Step: 210 Loss: 0.577922 Tokens per Sec: 830.878432\n",
      "Epoch Step: 220 Loss: 7.606997 Tokens per Sec: 840.235731\n",
      "Epoch Step: 230 Loss: 2.450547 Tokens per Sec: 897.553627\n",
      "Epoch Step: 240 Loss: 0.284088 Tokens per Sec: 872.173065\n",
      "Epoch Step: 250 Loss: 7.125855 Tokens per Sec: 840.843898\n",
      "Epoch Step: 260 Loss: 0.012227 Tokens per Sec: 818.283883\n",
      "Epoch Step: 270 Loss: 3.914665 Tokens per Sec: 884.111100\n",
      "Epoch Step: 280 Loss: 5.636555 Tokens per Sec: 859.314898\n",
      "Epoch Step: 290 Loss: 7.537199 Tokens per Sec: 853.803658\n",
      "Epoch Step: 300 Loss: 6.595500 Tokens per Sec: 844.559742\n",
      "Epoch Step: 310 Loss: 16.885647 Tokens per Sec: 854.691588\n",
      "Epoch Step: 320 Loss: 14.356976 Tokens per Sec: 828.222193\n",
      "Epoch Step: 330 Loss: 10.605491 Tokens per Sec: 870.595972\n",
      "Epoch Step: 340 Loss: 0.602045 Tokens per Sec: 866.272630\n",
      "Epoch Step: 350 Loss: 1.334341 Tokens per Sec: 871.727950\n",
      "Epoch Step: 360 Loss: 2.378835 Tokens per Sec: 825.508205\n",
      "Epoch Step: 370 Loss: 13.923856 Tokens per Sec: 904.158700\n",
      "Epoch Step: 380 Loss: 40.363426 Tokens per Sec: 861.622778\n",
      "Epoch Step: 390 Loss: 15.762043 Tokens per Sec: 877.784079\n",
      "Epoch Step: 400 Loss: 21.803389 Tokens per Sec: 823.713849\n",
      "Epoch Step: 410 Loss: 1.157167 Tokens per Sec: 807.800175\n",
      "Epoch Step: 420 Loss: 10.730963 Tokens per Sec: 902.910107\n",
      "Epoch Step: 430 Loss: 0.753192 Tokens per Sec: 836.859490\n",
      "Epoch Step: 440 Loss: 2.496823 Tokens per Sec: 865.991595\n",
      "Epoch Step: 450 Loss: 2.245993 Tokens per Sec: 854.086485\n",
      "Epoch Step: 460 Loss: 12.429029 Tokens per Sec: 860.711649\n",
      "Epoch Step: 470 Loss: 4.812266 Tokens per Sec: 902.835975\n",
      "Epoch Step: 480 Loss: 0.676633 Tokens per Sec: 795.214864\n",
      "Epoch Step: 490 Loss: 10.910400 Tokens per Sec: 889.629381\n",
      "Epoch Step: 500 Loss: 9.394026 Tokens per Sec: 806.394347\n",
      "Epoch Step: 510 Loss: 5.345446 Tokens per Sec: 813.303836\n",
      "Epoch Step: 520 Loss: 16.045795 Tokens per Sec: 847.198443\n",
      "Epoch Step: 530 Loss: 2.592711 Tokens per Sec: 876.726422\n",
      "Epoch Step: 540 Loss: 1.905541 Tokens per Sec: 869.911676\n",
      "Epoch Step: 550 Loss: 7.453944 Tokens per Sec: 885.128463\n",
      "Epoch Step: 560 Loss: 9.746976 Tokens per Sec: 893.585757\n",
      "Epoch Step: 570 Loss: 6.060693 Tokens per Sec: 888.757518\n",
      "Epoch Step: 580 Loss: 0.027716 Tokens per Sec: 852.765551\n",
      "Epoch Step: 590 Loss: 0.534026 Tokens per Sec: 859.062263\n",
      "Epoch Step: 600 Loss: 8.045290 Tokens per Sec: 884.375138\n",
      "Epoch Step: 610 Loss: 45.289062 Tokens per Sec: 846.087847\n",
      "Epoch Step: 620 Loss: 3.661234 Tokens per Sec: 868.891807\n",
      "Epoch Step: 630 Loss: 35.006283 Tokens per Sec: 858.278897\n",
      "\n",
      "Example #1\n",
      "Src :  , <unk>\n",
      "Trg :  , <unk> <unk>\n",
      "Pred:  , <unk>\n",
      "\n",
      "Example #2\n",
      "Src :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ . \"\n",
      "Trg :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ \"\n",
      "Pred:  <unk> <unk> , \" ф <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> ##лық жоқ\n",
      "\n",
      "Example #3\n",
      "Src :  1 , енді бұл екі <unk> <unk> <unk> ##к . . .\n",
      "Trg :  <unk> , енді бұл екі <unk> <unk> <unk> ##к\n",
      "Pred:  екі бүт ##ін он екі <unk> екі\n",
      "\n",
      "Validation perplexity: 5.404946\n",
      "Epoch 8\n",
      "Epoch Step: 10 Loss: 5.934658 Tokens per Sec: 886.115571\n",
      "Epoch Step: 20 Loss: 0.185844 Tokens per Sec: 838.140908\n",
      "Epoch Step: 30 Loss: 1.723968 Tokens per Sec: 869.518021\n",
      "Epoch Step: 40 Loss: 14.113268 Tokens per Sec: 826.109633\n",
      "Epoch Step: 50 Loss: 4.027642 Tokens per Sec: 867.334532\n",
      "Epoch Step: 60 Loss: 6.879613 Tokens per Sec: 833.409917\n",
      "Epoch Step: 70 Loss: 0.003823 Tokens per Sec: 860.358756\n",
      "Epoch Step: 80 Loss: 5.479969 Tokens per Sec: 893.283556\n",
      "Epoch Step: 90 Loss: 7.505417 Tokens per Sec: 882.538087\n",
      "Epoch Step: 100 Loss: 7.529309 Tokens per Sec: 837.899918\n",
      "Epoch Step: 110 Loss: 21.133318 Tokens per Sec: 881.893419\n",
      "Epoch Step: 120 Loss: 9.849255 Tokens per Sec: 821.966732\n",
      "Epoch Step: 130 Loss: 0.842699 Tokens per Sec: 881.032909\n",
      "Epoch Step: 140 Loss: 2.760821 Tokens per Sec: 832.662093\n",
      "Epoch Step: 150 Loss: 3.507534 Tokens per Sec: 843.080564\n",
      "Epoch Step: 160 Loss: 1.436023 Tokens per Sec: 846.618332\n",
      "Epoch Step: 170 Loss: 4.515554 Tokens per Sec: 884.199229\n",
      "Epoch Step: 180 Loss: 6.761768 Tokens per Sec: 841.299274\n",
      "Epoch Step: 190 Loss: 6.190149 Tokens per Sec: 864.037006\n",
      "Epoch Step: 200 Loss: 7.171836 Tokens per Sec: 819.498173\n",
      "Epoch Step: 210 Loss: 0.009341 Tokens per Sec: 844.523996\n",
      "Epoch Step: 220 Loss: 0.921040 Tokens per Sec: 889.933142\n",
      "Epoch Step: 230 Loss: 6.636733 Tokens per Sec: 861.166254\n",
      "Epoch Step: 240 Loss: 3.386204 Tokens per Sec: 891.317349\n",
      "Epoch Step: 250 Loss: 8.052691 Tokens per Sec: 868.204797\n",
      "Epoch Step: 260 Loss: 11.494567 Tokens per Sec: 809.163019\n",
      "Epoch Step: 270 Loss: 5.500605 Tokens per Sec: 880.007013\n",
      "Epoch Step: 280 Loss: 6.965302 Tokens per Sec: 843.568657\n",
      "Epoch Step: 290 Loss: 5.655841 Tokens per Sec: 884.604829\n",
      "Epoch Step: 300 Loss: 5.636181 Tokens per Sec: 860.443402\n",
      "Epoch Step: 310 Loss: 8.678120 Tokens per Sec: 846.043851\n",
      "Epoch Step: 320 Loss: 3.431217 Tokens per Sec: 879.698352\n",
      "Epoch Step: 330 Loss: 2.141119 Tokens per Sec: 870.407678\n",
      "Epoch Step: 340 Loss: 6.157373 Tokens per Sec: 898.367355\n",
      "Epoch Step: 350 Loss: 3.718920 Tokens per Sec: 809.066188\n",
      "Epoch Step: 360 Loss: 9.053976 Tokens per Sec: 887.486396\n",
      "Epoch Step: 370 Loss: 1.223793 Tokens per Sec: 887.852625\n",
      "Epoch Step: 380 Loss: 4.882360 Tokens per Sec: 870.684012\n",
      "Epoch Step: 390 Loss: 1.230720 Tokens per Sec: 872.874946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 400 Loss: 0.794688 Tokens per Sec: 900.221098\n",
      "Epoch Step: 410 Loss: 1.992604 Tokens per Sec: 866.765913\n",
      "Epoch Step: 420 Loss: 10.812851 Tokens per Sec: 855.016334\n",
      "Epoch Step: 430 Loss: 0.161584 Tokens per Sec: 890.381302\n",
      "Epoch Step: 440 Loss: 12.876005 Tokens per Sec: 847.544780\n",
      "Epoch Step: 450 Loss: 0.272303 Tokens per Sec: 860.484047\n",
      "Epoch Step: 460 Loss: 4.758313 Tokens per Sec: 882.728361\n",
      "Epoch Step: 470 Loss: 4.906794 Tokens per Sec: 871.395000\n",
      "Epoch Step: 480 Loss: 1.577145 Tokens per Sec: 872.250551\n",
      "Epoch Step: 490 Loss: 18.483236 Tokens per Sec: 857.750506\n",
      "Epoch Step: 500 Loss: 0.279597 Tokens per Sec: 796.911130\n",
      "Epoch Step: 510 Loss: 3.026395 Tokens per Sec: 860.684880\n",
      "Epoch Step: 520 Loss: 32.411938 Tokens per Sec: 825.129635\n",
      "Epoch Step: 530 Loss: 8.072184 Tokens per Sec: 799.353671\n",
      "Epoch Step: 540 Loss: 0.266110 Tokens per Sec: 839.723530\n",
      "Epoch Step: 550 Loss: 7.806770 Tokens per Sec: 843.917661\n",
      "Epoch Step: 560 Loss: 4.031607 Tokens per Sec: 837.288209\n",
      "Epoch Step: 570 Loss: 20.140635 Tokens per Sec: 864.767629\n",
      "Epoch Step: 580 Loss: 2.331881 Tokens per Sec: 848.160632\n",
      "Epoch Step: 590 Loss: 5.702847 Tokens per Sec: 856.773247\n",
      "Epoch Step: 600 Loss: 15.344163 Tokens per Sec: 836.413957\n",
      "Epoch Step: 610 Loss: 14.883718 Tokens per Sec: 868.677707\n",
      "Epoch Step: 620 Loss: 4.350466 Tokens per Sec: 878.425065\n",
      "Epoch Step: 630 Loss: 0.711937 Tokens per Sec: 835.798858\n",
      "\n",
      "Example #1\n",
      "Src :  , <unk>\n",
      "Trg :  , <unk> <unk>\n",
      "Pred:  , <unk>\n",
      "\n",
      "Example #2\n",
      "Src :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ . \"\n",
      "Trg :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ \"\n",
      "Pred:  <unk> , <unk> \" ф <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> ##та ##ң\n",
      "\n",
      "Example #3\n",
      "Src :  1 , енді бұл екі <unk> <unk> <unk> ##к . . .\n",
      "Trg :  <unk> , енді бұл екі <unk> <unk> <unk> ##к\n",
      "Pred:  бір бүт ##ін он <unk> <unk> <unk> ##к ##к к ##ө\n",
      "\n",
      "Validation perplexity: 4.405752\n",
      "Epoch 9\n",
      "Epoch Step: 10 Loss: 1.797412 Tokens per Sec: 874.362305\n",
      "Epoch Step: 20 Loss: 7.530377 Tokens per Sec: 863.831055\n",
      "Epoch Step: 30 Loss: 7.006356 Tokens per Sec: 852.171590\n",
      "Epoch Step: 40 Loss: 0.061363 Tokens per Sec: 824.557476\n",
      "Epoch Step: 50 Loss: 1.482439 Tokens per Sec: 915.030646\n",
      "Epoch Step: 60 Loss: 2.132520 Tokens per Sec: 891.726249\n",
      "Epoch Step: 70 Loss: 0.250834 Tokens per Sec: 814.398831\n",
      "Epoch Step: 80 Loss: 1.826577 Tokens per Sec: 860.966407\n",
      "Epoch Step: 90 Loss: 16.042252 Tokens per Sec: 856.872131\n",
      "Epoch Step: 100 Loss: 0.141460 Tokens per Sec: 813.617656\n",
      "Epoch Step: 110 Loss: 11.692205 Tokens per Sec: 885.304484\n",
      "Epoch Step: 120 Loss: 3.621461 Tokens per Sec: 807.482623\n",
      "Epoch Step: 130 Loss: 0.065802 Tokens per Sec: 823.449153\n",
      "Epoch Step: 140 Loss: 8.905306 Tokens per Sec: 865.782455\n",
      "Epoch Step: 150 Loss: 1.438469 Tokens per Sec: 928.784083\n",
      "Epoch Step: 160 Loss: 4.827950 Tokens per Sec: 879.089234\n",
      "Epoch Step: 170 Loss: 5.456982 Tokens per Sec: 874.703200\n",
      "Epoch Step: 180 Loss: 3.506582 Tokens per Sec: 889.618667\n",
      "Epoch Step: 190 Loss: 1.008299 Tokens per Sec: 869.435559\n",
      "Epoch Step: 200 Loss: 4.295468 Tokens per Sec: 837.217043\n",
      "Epoch Step: 210 Loss: 1.064639 Tokens per Sec: 917.063532\n",
      "Epoch Step: 220 Loss: 10.676273 Tokens per Sec: 844.051384\n",
      "Epoch Step: 230 Loss: 2.314129 Tokens per Sec: 863.494476\n",
      "Epoch Step: 240 Loss: 1.760208 Tokens per Sec: 848.791662\n",
      "Epoch Step: 250 Loss: 9.352258 Tokens per Sec: 851.178886\n",
      "Epoch Step: 260 Loss: 0.165249 Tokens per Sec: 779.441946\n",
      "Epoch Step: 270 Loss: 7.482254 Tokens per Sec: 828.184978\n",
      "Epoch Step: 280 Loss: 0.006773 Tokens per Sec: 864.541989\n",
      "Epoch Step: 290 Loss: 0.093243 Tokens per Sec: 868.631619\n",
      "Epoch Step: 300 Loss: 7.818779 Tokens per Sec: 882.327513\n",
      "Epoch Step: 310 Loss: 5.001671 Tokens per Sec: 836.407794\n",
      "Epoch Step: 320 Loss: 0.247429 Tokens per Sec: 800.499413\n",
      "Epoch Step: 330 Loss: 5.956487 Tokens per Sec: 910.638638\n",
      "Epoch Step: 340 Loss: 5.935643 Tokens per Sec: 820.561609\n",
      "Epoch Step: 350 Loss: 11.594967 Tokens per Sec: 871.346284\n",
      "Epoch Step: 360 Loss: 7.433208 Tokens per Sec: 835.318344\n",
      "Epoch Step: 370 Loss: 1.030514 Tokens per Sec: 875.113935\n",
      "Epoch Step: 380 Loss: 8.332910 Tokens per Sec: 863.611135\n",
      "Epoch Step: 390 Loss: 19.420929 Tokens per Sec: 899.913016\n",
      "Epoch Step: 400 Loss: 1.781363 Tokens per Sec: 803.936460\n",
      "Epoch Step: 410 Loss: 7.431148 Tokens per Sec: 871.899745\n",
      "Epoch Step: 420 Loss: 0.564761 Tokens per Sec: 747.923561\n",
      "Epoch Step: 430 Loss: 0.258477 Tokens per Sec: 857.167543\n",
      "Epoch Step: 440 Loss: 7.174091 Tokens per Sec: 830.030307\n",
      "Epoch Step: 450 Loss: 0.080310 Tokens per Sec: 853.505782\n",
      "Epoch Step: 460 Loss: 6.628869 Tokens per Sec: 864.837886\n",
      "Epoch Step: 470 Loss: 8.569103 Tokens per Sec: 905.808265\n",
      "Epoch Step: 480 Loss: 4.986148 Tokens per Sec: 872.331052\n",
      "Epoch Step: 490 Loss: 16.683165 Tokens per Sec: 835.313176\n",
      "Epoch Step: 500 Loss: 0.823660 Tokens per Sec: 828.856858\n",
      "Epoch Step: 510 Loss: 0.427906 Tokens per Sec: 844.200041\n",
      "Epoch Step: 520 Loss: 6.638526 Tokens per Sec: 834.627168\n",
      "Epoch Step: 530 Loss: 8.483591 Tokens per Sec: 876.855034\n",
      "Epoch Step: 540 Loss: 2.734481 Tokens per Sec: 849.557129\n",
      "Epoch Step: 550 Loss: 5.899782 Tokens per Sec: 837.201007\n",
      "Epoch Step: 560 Loss: 7.253945 Tokens per Sec: 829.234365\n",
      "Epoch Step: 570 Loss: 3.282387 Tokens per Sec: 856.766535\n",
      "Epoch Step: 580 Loss: 3.052440 Tokens per Sec: 841.327656\n",
      "Epoch Step: 590 Loss: 4.663609 Tokens per Sec: 841.330895\n",
      "Epoch Step: 600 Loss: 1.515254 Tokens per Sec: 841.433717\n",
      "Epoch Step: 610 Loss: 15.528179 Tokens per Sec: 835.812076\n",
      "Epoch Step: 620 Loss: 1.510755 Tokens per Sec: 807.116044\n",
      "Epoch Step: 630 Loss: 0.065763 Tokens per Sec: 846.691665\n",
      "\n",
      "Example #1\n",
      "Src :  , <unk>\n",
      "Trg :  , <unk> <unk>\n",
      "Pred:  , <unk>\n",
      "\n",
      "Example #2\n",
      "Src :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ . \"\n",
      "Trg :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ \"\n",
      "Pred:  <unk> , т \" ф <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ырық қ ##ырық екі жоқ\n",
      "\n",
      "Example #3\n",
      "Src :  1 , енді бұл екі <unk> <unk> <unk> ##к . . .\n",
      "Trg :  <unk> , енді бұл екі <unk> <unk> <unk> ##к\n",
      "Pred:  бір бүт ##ін он ##нан екі <unk> <unk> <unk> <unk>\n",
      "\n",
      "Validation perplexity: 4.225211\n"
     ]
    }
   ],
   "source": [
    "dev_perplexities = train(model, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def plot_perplexity(perplexities):\n",
    "    \"\"\"plot perplexities\"\"\"\n",
    "    plt.title(\"Perplexity per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.plot(perplexities)\n",
    "\n",
    "plot_perplexity(dev_perplexities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "555"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [\" \".join(example.trg) for example in valid_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = []\n",
    "alphas = []  \n",
    "for batch in valid_iter:\n",
    "  batch = rebatch(PAD_INDEX, batch)\n",
    "  pred, attention = greedy_decode(\n",
    "    model, batch.src, batch.src_mask, batch.src_lengths, max_len=25,\n",
    "    sos_index=TRG.vocab.stoi[SOS_TOKEN],\n",
    "    eos_index=TRG.vocab.stoi[EOS_TOKEN])\n",
    "  hypotheses.append(pred)\n",
    "  alphas.append(attention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = [lookup_words(x, TRG.vocab) for x in hypotheses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = [\" \".join(x) for x in hypotheses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555\n",
      "бір бүт ##ін он ##нан екі <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "print(len(hypotheses))\n",
    "print(hypotheses[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = sacrebleu.raw_corpus_bleu(hypotheses, [references], .01).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.929048736931263\n"
     ]
    }
   ],
   "source": [
    "print(bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(src, trg, scores):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.pcolor(scores, cmap='viridis')\n",
    "\n",
    "    ax.set_xticklabels(trg, minor=False, rotation='vertical')\n",
    "    ax.set_yticklabels(src, minor=False)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    # and the x-ticks on top\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.set_xticks(np.arange(scores.shape[1]) + 0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(scores.shape[0]) + 0.5, minor=False)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    plt.colorbar(heatmap)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бір бүт ##ін он ##нан екі <unk> <unk> <unk> <unk>'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src ['1', ',', 'енді', 'бұл', 'екі', 'ортаға', 'жылдам', 'келді', '##к', '.', '.', '.', '</s>']\n",
      "ref ['1', ',', 'енді', 'бұл', 'екі', 'ортаға', 'жылдам', 'келді', '##к', '</s>']\n",
      "pred ['бір', 'бүт', '##ін', 'он', '##нан', 'екі', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 2\n",
    "src = valid_data[idx].src + [\"</s>\"]\n",
    "trg = valid_data[idx].trg + [\"</s>\"]\n",
    "pred = hypotheses[idx].split() + [\"</s>\"]\n",
    "pred_att = alphas[idx][0].T[:, :len(pred)]\n",
    "print(\"src\", src)\n",
    "print(\"ref\", trg)\n",
    "print(\"pred\", pred)\n",
    "plot_heatmap(src, pred, pred_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36pyt14",
   "language": "python",
   "name": "py36pyt14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
