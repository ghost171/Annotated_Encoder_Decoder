{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# we will use CUDA if it is available\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE=torch.device('cuda') # or set to 'cpu'\n",
    "print(\"CUDA:\", USE_CUDA)\n",
    "print(DEVICE)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.5\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, src_lengths, max_len=100, sos_index=1, eos_index=None):\n",
    "    \"\"\"Greedily decode a sentence.\"\"\"\n",
    "    src_lengths = src_lengths.cpu()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden, encoder_final = model.encode(src, src_mask, src_lengths)\n",
    "        prev_y = torch.ones(1, 1).fill_(sos_index).type_as(src)\n",
    "        trg_mask = torch.ones_like(prev_y)\n",
    "\n",
    "    output = []\n",
    "    attention_scores = []\n",
    "    hidden = None\n",
    "\n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            out, hidden, pre_output = model.decode(\n",
    "              encoder_hidden, encoder_final, src_mask,\n",
    "              prev_y, trg_mask, hidden)\n",
    "            \n",
    "            prob = model.generator(pre_output[:, -1])\n",
    "\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data.item()\n",
    "        output.append(next_word)\n",
    "        prev_y = torch.ones(1, 1).type_as(src).fill_(next_word)\n",
    "        attention_scores.append(model.decoder.attention.alphas.cpu().numpy())\n",
    "    \n",
    "    output = np.array(output)\n",
    "        \n",
    "    \n",
    "    if eos_index is not None:\n",
    "        first_eos = np.where(output==eos_index)[0]\n",
    "        if len(first_eos) > 0:\n",
    "            output = output[:first_eos[0]]      \n",
    "    \n",
    "    return output, np.concatenate(attention_scores, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_words(x, vocab=None):\n",
    "    if vocab is not None:\n",
    "        x = [vocab.itos[i] for i in x]\n",
    "\n",
    "    return [str(t) for t in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_examples(example_iter, model, n=2, max_len=100, \n",
    "                   sos_index=1, \n",
    "                   src_eos_index=None, \n",
    "                   trg_eos_index=None, \n",
    "                   src_vocab=None, trg_vocab=None):\n",
    "    \"\"\"Prints N examples. Assumes batch size of 1.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    print()\n",
    "    \n",
    "    if src_vocab is not None and trg_vocab is not None:\n",
    "        src_eos_index = src_vocab.stoi[EOS_TOKEN]\n",
    "        trg_sos_index = trg_vocab.stoi[SOS_TOKEN]\n",
    "        trg_eos_index = trg_vocab.stoi[EOS_TOKEN]\n",
    "    else:\n",
    "        src_eos_index = None\n",
    "        trg_sos_index = 1\n",
    "        trg_eos_index = None\n",
    "        \n",
    "    for i, batch in enumerate(example_iter):\n",
    "      \n",
    "        src = batch.src.cpu().numpy()[0, :]\n",
    "        trg = batch.trg_y.cpu().numpy()[0, :]\n",
    "\n",
    "        # remove </s> (if it is there)\n",
    "        src = src[:-1] if src[-1] == src_eos_index else src\n",
    "        trg = trg[:-1] if trg[-1] == trg_eos_index else trg      \n",
    "      \n",
    "        result, _ = greedy_decode(\n",
    "          model, batch.src, batch.src_mask, batch.src_lengths,\n",
    "          max_len=max_len, sos_index=trg_sos_index, eos_index=trg_eos_index)\n",
    "        print(\"Example #%d\" % (i+1))\n",
    "        print(\"Src : \", \" \".join(lookup_words(src, vocab=src_vocab)))\n",
    "        print(\"Trg : \", \" \".join(lookup_words(trg, vocab=trg_vocab)))\n",
    "        print(\"Pred: \", \" \".join(lookup_words(result, vocab=trg_vocab)))\n",
    "        print()\n",
    "        \n",
    "        count += 1\n",
    "        if count == n:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.trg_embed = trg_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask, src_lengths, trg_lengths):\n",
    "        \"\"\"Take in and process masked src and target sequences.\"\"\"\n",
    "        src_lengths = src_lengths.cpu()\n",
    "        encoder_hidden, encoder_final = self.encode(src, src_mask, src_lengths)\n",
    "        return self.decode(encoder_hidden, encoder_final, src_mask, trg, trg_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask, src_lengths):\n",
    "        return self.encoder(self.src_embed(src), src_mask, src_lengths)\n",
    "    \n",
    "    def decode(self, encoder_hidden, encoder_final, src_mask, trg, trg_mask,\n",
    "               decoder_hidden=None):\n",
    "        return self.decoder(self.trg_embed(trg), encoder_hidden, encoder_final,\n",
    "                            src_mask, trg_mask, hidden=decoder_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encodes a sequence of word embeddings\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, \n",
    "                          batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x, mask, lengths):\n",
    "        \"\"\"\n",
    "        Applies a bidirectional GRU to sequence of embeddings x.\n",
    "        The input mini-batch x needs to be sorted by length.\n",
    "        x should have dimensions [batch, time, dim].\n",
    "        \"\"\"\n",
    "        packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        output, final = self.rnn(packed)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        # we need to manually concatenate the final states for both directions\n",
    "        fwd_final = final[0:final.size(0):2]\n",
    "        bwd_final = final[1:final.size(0):2]\n",
    "        final = torch.cat([fwd_final, bwd_final], dim=2)  \n",
    "\n",
    "        return output, final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"A conditional RNN decoder with attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, emb_size, hidden_size, attention, num_layers=1, dropout=0.5,\n",
    "                 bridge=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attention = attention\n",
    "        self.dropout = dropout\n",
    "                 \n",
    "        self.rnn = nn.GRU(emb_size + 2*hidden_size, hidden_size, num_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "                 \n",
    "        # to initialize from the final encoder state\n",
    "        self.bridge = nn.Linear(2*hidden_size, hidden_size, bias=True) if bridge else None\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        self.pre_output_layer = nn.Linear(hidden_size + 2*hidden_size + emb_size,\n",
    "                                          hidden_size, bias=False)\n",
    "        \n",
    "    def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n",
    "        \"\"\"Perform a single decoder step (1 word)\"\"\"\n",
    "\n",
    "        # compute context vector using attention mechanism\n",
    "        query = hidden[-1].unsqueeze(1)  \n",
    "        context, attn_probs = self.attention(query=query, proj_key=proj_key,value=encoder_hidden, mask=src_mask)\n",
    "\n",
    "        # update rnn hidden state\n",
    "        rnn_input = torch.cat([prev_embed, context], dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "\n",
    "        pre_output = torch.cat([prev_embed, output, context], dim=2)\n",
    "        pre_output = self.dropout_layer(pre_output)\n",
    "        pre_output = self.pre_output_layer(pre_output)\n",
    "\n",
    "        return output, hidden, pre_output\n",
    "\n",
    "    def forward(self, trg_embed, encoder_hidden, encoder_final, \n",
    "            src_mask, trg_mask, hidden=None, max_len=None):\n",
    "        \"\"\"Unroll the decoder one step at a time.\"\"\"\n",
    "\n",
    "        # the maximum number of steps to unroll the RNN\n",
    "        if max_len is None:\n",
    "            max_len = trg_mask.size(-1)\n",
    "\n",
    "        # initialize decoder hidden state\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(encoder_final)\n",
    "\n",
    "        # pre-compute projected encoder hidden states\n",
    "        # (the \"keys\" for the attention mechanism)\n",
    "        # this is only done for efficiency\n",
    "        proj_key = self.attention.key_layer(encoder_hidden)\n",
    "\n",
    "        # here we store all intermediate hidden states and pre-output vectors\n",
    "        decoder_states = []\n",
    "        pre_output_vectors = []\n",
    "\n",
    "        # unroll the decoder RNN for max_len steps\n",
    "        for i in range(max_len):\n",
    "            prev_embed = trg_embed[:, i].unsqueeze(1)\n",
    "            output, hidden, pre_output = self.forward_step(\n",
    "              prev_embed, encoder_hidden, src_mask, proj_key, hidden)\n",
    "            decoder_states.append(output)\n",
    "            pre_output_vectors.append(pre_output)\n",
    "\n",
    "        decoder_states = torch.cat(decoder_states, dim=1)\n",
    "        pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n",
    "        return decoder_states, hidden, pre_output_vectors  # [B, N, D]\n",
    "\n",
    "\n",
    "\n",
    "    def init_hidden(self, encoder_final):\n",
    "        \"\"\"Returns the initial decoder state,\n",
    "        conditioned on the final encoder state.\"\"\"\n",
    "\n",
    "        if encoder_final is None:\n",
    "            return None  # start with zeros\n",
    "\n",
    "        return torch.tanh(self.bridge(encoder_final))            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, key_size=None, query_size=None):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        \n",
    "        # We assume a bi-directional encoder so key_size is 2*hidden_size\n",
    "        key_size = 2 * hidden_size if key_size is None else key_size\n",
    "        query_size = hidden_size if query_size is None else query_size\n",
    "\n",
    "        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n",
    "        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n",
    "        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "        # to store attention scores\n",
    "        self.alphas = None\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, query=None, proj_key=None, value=None, mask=None):\n",
    "        assert mask is not None, \"mask is required\"\n",
    "\n",
    "        # We first project the query (the decoder state).\n",
    "        # The projected keys (the encoder states) were already pre-computated.\n",
    "        query = self.query_layer(query)\n",
    "        \n",
    "        # Calculate scores.\n",
    "        scores = self.energy_layer(torch.tanh(query + proj_key))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "        \n",
    "        # Mask out invalid positions.\n",
    "        # The mask marks valid positions so we invert it using `mask & 0`.\n",
    "        scores.data.masked_fill_(mask == 0, -float('inf'))\n",
    "        \n",
    "        # Turn scores to probabilities.\n",
    "        alphas = F.softmax(scores, dim=-1)\n",
    "        self.alphas = alphas        \n",
    "        \n",
    "        # The context vector is the weighted sum of the values.\n",
    "        context = torch.bmm(alphas, value)\n",
    "        \n",
    "        # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n",
    "        return context, alphas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, emb_size=256, hidden_size=512, num_layers=1, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "\n",
    "    attention = BahdanauAttention(hidden_size)\n",
    "\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(emb_size, hidden_size, num_layers=num_layers, dropout=dropout),\n",
    "        Decoder(emb_size, hidden_size, attention, num_layers=num_layers, dropout=dropout),\n",
    "        nn.Embedding(src_vocab, emb_size),\n",
    "        nn.Embedding(tgt_vocab, emb_size),\n",
    "        Generator(hidden_size, tgt_vocab))\n",
    "\n",
    "    return model.cuda() if USE_CUDA else model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"Object for holding a batch of data with mask during training.\n",
    "    Input is a batch from a torch text iterator.\n",
    "    \"\"\"\n",
    "    def __init__(self, src, trg, pad_index=0):\n",
    "        \n",
    "        src, src_lengths = src\n",
    "        \n",
    "        self.src = src\n",
    "        self.src_lengths = src_lengths\n",
    "        self.src_mask = (src != pad_index).unsqueeze(-2)\n",
    "        self.nseqs = src.size(0)\n",
    "        \n",
    "        self.trg = None\n",
    "        self.trg_y = None\n",
    "        self.trg_mask = None\n",
    "        self.trg_lengths = None\n",
    "        self.ntokens = None\n",
    "\n",
    "        if trg is not None:\n",
    "            trg, trg_lengths = trg\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_lengths = trg_lengths\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = (self.trg_y != pad_index)\n",
    "            self.ntokens = (self.trg_y != pad_index).data.sum().item()\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            self.src = self.src.cuda()\n",
    "            self.src_mask = self.src_mask.cuda()\n",
    "\n",
    "            if trg is not None:\n",
    "                self.trg = self.trg.cuda()\n",
    "                self.trg_y = self.trg_y.cuda()\n",
    "                self.trg_mask = self.trg_mask.cuda()\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute, print_every=50):\n",
    "    \"\"\"Standard Training and Logging Function\"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    print_tokens = 0\n",
    "\n",
    "    for i, batch in enumerate(data_iter, 1):\n",
    "        \n",
    "        out, _, pre_output = model.forward(batch.src, batch.trg,\n",
    "                                           batch.src_mask, batch.trg_mask,\n",
    "                                           batch.src_lengths, batch.trg_lengths)\n",
    "        loss = loss_compute(pre_output, batch.trg_y, batch.nseqs)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        print_tokens += batch.ntokens\n",
    "        \n",
    "        if model.training and i % print_every == 0:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.nseqs, print_tokens / elapsed))\n",
    "            start = time.time()\n",
    "            print_tokens = 0\n",
    "\n",
    "    return math.exp(total_loss / float(total_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"\"\"A simple loss compute and train function.\"\"\"\n",
    "\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
    "                              y.contiguous().view(-1))\n",
    "        loss = loss / norm\n",
    "\n",
    "        if self.opt is not None:\n",
    "            loss.backward()          \n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "        return loss.data.item() * norm\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LC_ALL=\"en_US.UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(filepaths):\n",
    "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "  data = []\n",
    "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "    de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)],\n",
    "                            dtype=torch.long)\n",
    "    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)],\n",
    "                            dtype=torch.long)\n",
    "    data.append((de_tensor_, en_tensor_))\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_model(model_url, max_seq_length):\n",
    "    with tf.device('/cpu:0'):\n",
    "        labse_layer = hub.KerasLayer(model_url, trainable=False)\n",
    "\n",
    "      # Define input.\n",
    "        input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                             name=\"input_word_ids\")\n",
    "        input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                     name=\"input_mask\")\n",
    "        segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                          name=\"segment_ids\")\n",
    "\n",
    "      # LaBSE layer.\n",
    "        pooled_output,  _ = labse_layer([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "      # The embedding is l2 normalized.\n",
    "        pooled_output = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.nn.l2_normalize(x, axis=1))(pooled_output)\n",
    "\n",
    "      # Define model.\n",
    "        labse_model = tf.keras.Model(\n",
    "        inputs=[input_word_ids, input_mask, segment_ids],\n",
    "        outputs=pooled_output)\n",
    "    return labse_model, labse_layer\n",
    "max_seq_length = 10\n",
    "labse_model, labse_layer = get_model(model_url=\"https://tfhub.dev/google/LaBSE/1\", max_seq_length=max_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import tokenization\n",
    "\n",
    "vocab_file = labse_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "with tf.device('/cpu:0'):\n",
    "    do_lower_case = labse_layer.resolved_object.do_lower_case.numpy()\n",
    "    tokenizer = bert.tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(filepath, tokenizer):\n",
    "  counter = Counter()\n",
    "  with io.open(filepath, encoding=\"utf8\") as f:\n",
    "    for string_ in f:\n",
    "      counter.update(tokenizer(string_))\n",
    "  return build_vocab_from_iterator(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "ininormer = tokenizer\n",
    "\n",
    "def tokenize_kz_unnormalized(text):\n",
    "    out = [tok for tok in ininormer.tokenize(text)]\n",
    "    return out\n",
    "\n",
    "def tokenize_kz_normalized(text):\n",
    "    out = [tok for tok in ininormer.tokenize(text)]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_TOKEN = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\"    \n",
    "SOS_TOKEN = \"<s>\"\n",
    "EOS_TOKEN = \"</s>\"\n",
    "LOWER = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = data.Field(tokenize=tokenize_kz_unnormalized, batch_first=True, lower=LOWER, include_lengths=True, unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=None, eos_token=EOS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG = data.Field(tokenize=tokenize_kz_normalized, batch_first=True, lower=LOWER, include_lengths=True, unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import TranslationDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset = TranslationDataset(path='./', exts=('data_1.ut', 'data_1.nt'), fields=(SRC, TRG))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "\n",
    "if True:\n",
    "\n",
    "    UNK_TOKEN = \"<unk>\"\n",
    "    PAD_TOKEN = \"<pad>\"    \n",
    "    SOS_TOKEN = \"<s>\"\n",
    "    EOS_TOKEN = \"</s>\"\n",
    "    LOWER = True\n",
    "    \n",
    "    # we include lengths to provide to the RNNs\n",
    "    SRC = data.Field(tokenize=tokenize_kz_unnormalized,\n",
    "                         batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                         unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=None, eos_token=EOS_TOKEN)\n",
    "                     \n",
    "    TRG = data.Field(tokenize=tokenize_kz_normalized,\n",
    "                         batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                         unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)                     \n",
    "\n",
    "    MAX_LEN = 25  # NOTE: we filter out a lot of sentences for speed\n",
    "    train_data, valid_data, test_data = text_dataset.splits(path='./', exts=('.ut', '.nt'), fields=(SRC, TRG), filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN)\n",
    "    MIN_FREQ = 5  # NOTE: we limit the vocabulary to frequent words for speed\n",
    "    SRC.build_vocab(train_data.src, min_freq=MIN_FREQ)\n",
    "    TRG.build_vocab(train_data.trg, min_freq=MIN_FREQ)\n",
    "    \n",
    "    PAD_INDEX = TRG.vocab.stoi[PAD_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['жамбыл',\n",
       " ',',\n",
       " 'батыс',\n",
       " 'қазақстан',\n",
       " 'облыс',\n",
       " '##тарының',\n",
       " 'кей',\n",
       " 'жерлер',\n",
       " '##інде',\n",
       " 'т',\n",
       " '##ұм',\n",
       " '##ан',\n",
       " 'к',\n",
       " '##үт',\n",
       " '##іледі',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_data[4])['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set sizes (number of sentence pairs):\n",
      "train 638\n",
      "valid 555\n",
      "test 525 \n",
      "\n",
      "First training example:\n",
      "src: — ы ##м - м ! — деп орнынан түр ##еге ##лді .\n",
      "trg: — ы ##мм ! — деп орнынан түр ##еге ##лді \n",
      "\n",
      "Most common words (src):\n",
      "         .        609\n",
      "         ,        234\n",
      "         -        100\n",
      "         —         70\n",
      "       ##н         59\n",
      "         \"         49\n",
      "      ##ға         41\n",
      "         қ         39\n",
      "   бойынша         37\n",
      "       ##ы         36 \n",
      "\n",
      "Most common words (trg):\n",
      "         ,        213\n",
      "       жүз         79\n",
      "       екі         74\n",
      "         —         70\n",
      "       ##н         61\n",
      "       мың         61\n",
      "         қ         56\n",
      "       бір         56\n",
      "         \"         49\n",
      "         к         41 \n",
      "\n",
      "First 10 words (src):\n",
      "00 <unk>\n",
      "01 <pad>\n",
      "02 </s>\n",
      "03 .\n",
      "04 ,\n",
      "05 -\n",
      "06 —\n",
      "07 ##н\n",
      "08 \"\n",
      "09 ##ға \n",
      "\n",
      "First 10 words (trg):\n",
      "00 <unk>\n",
      "01 <pad>\n",
      "02 <s>\n",
      "03 </s>\n",
      "04 ,\n",
      "05 жүз\n",
      "06 екі\n",
      "07 —\n",
      "08 ##н\n",
      "09 мың \n",
      "\n",
      "Number of Unnormalized words (types): 320\n",
      "Number of normalizeds (types): 330 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_data_info(train_data, valid_data, test_data, src_field, trg_field):\n",
    "    \"\"\" This prints some useful stuff about our data sets. \"\"\"\n",
    "\n",
    "    print(\"Data set sizes (number of sentence pairs):\")\n",
    "    print('train', len(train_data))\n",
    "    print('valid', len(valid_data))\n",
    "    print('test', len(test_data), \"\\n\")\n",
    "\n",
    "    print(\"First training example:\")\n",
    "    print(\"src:\", \" \".join(vars(train_data[0])['src']))\n",
    "    print(\"trg:\", \" \".join(vars(train_data[0])['trg']), \"\\n\")\n",
    "\n",
    "    print(\"Most common words (src):\")\n",
    "    print(\"\\n\".join([\"%10s %10d\" % x for x in src_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
    "    print(\"Most common words (trg):\")\n",
    "    print(\"\\n\".join([\"%10s %10d\" % x for x in trg_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
    "\n",
    "    print(\"First 10 words (src):\")\n",
    "    print(\"\\n\".join(\n",
    "        '%02d %s' % (i, t) for i, t in enumerate(src_field.vocab.itos[:10])), \"\\n\")\n",
    "    print(\"First 10 words (trg):\")\n",
    "    print(\"\\n\".join(\n",
    "        '%02d %s' % (i, t) for i, t in enumerate(trg_field.vocab.itos[:10])), \"\\n\")\n",
    "\n",
    "    print(\"Number of Unnormalized words (types):\", len(src_field.vocab))\n",
    "    print(\"Number of normalizeds (types):\", len(trg_field.vocab), \"\\n\")\n",
    "    \n",
    "    \n",
    "print_data_info(train_data, valid_data, test_data, SRC, TRG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchtext.datasets.translation.TranslationDataset"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = data.BucketIterator(train_data, batch_size=1, train=True, sort_within_batch=True, \n",
    "                                 sort_key=lambda x: (len(x.src), len(x.trg)), repeat=False,\n",
    "                                 device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_iter = data.Iterator(valid_data, batch_size=1, train=False, sort=False, repeat=False, \n",
    "                           device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rebatch(pad_idx, batch):\n",
    "    \"\"\"Wrap torchtext batch into our own Batch class for pre-processing\"\"\"\n",
    "    return Batch(batch.src, batch.trg, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs=10, lr=0.0003, print_every=100):\n",
    "    \"\"\"Train a model on IWSLT\"\"\"\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        model.cuda()\n",
    "\n",
    "    # optionally add label smoothing; see the Annotated Transformer\n",
    "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    dev_perplexities = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "      \n",
    "        print(\"Epoch\", epoch)\n",
    "        model.train()\n",
    "        train_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in train_iter), \n",
    "                                     model,\n",
    "                                     SimpleLossCompute(model.generator, criterion, optim),\n",
    "                                     print_every=print_every)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            print_examples((rebatch(PAD_INDEX, x) for x in valid_iter), \n",
    "                           model, n=3, src_vocab=SRC.vocab, trg_vocab=TRG.vocab)  \n",
    "            \n",
    "            dev_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in valid_iter), \n",
    "                                       model, \n",
    "                                       SimpleLossCompute(model.generator, criterion, None))\n",
    "            print(\"Validation perplexity: %f\" % dev_perplexity)\n",
    "            dev_perplexities.append(dev_perplexity)\n",
    "        \n",
    "    return dev_perplexities\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghost/anaconda3/envs/py36pyt14/lib/python3.6/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = make_model(len(SRC.vocab), len(TRG.vocab),\n",
    "                   emb_size=256, hidden_size=256,\n",
    "                   num_layers=1, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch Step: 10 Loss: 30.002338 Tokens per Sec: 64.925918\n",
      "Epoch Step: 20 Loss: 52.839489 Tokens per Sec: 737.592705\n",
      "Epoch Step: 30 Loss: 56.454655 Tokens per Sec: 808.928662\n",
      "Epoch Step: 40 Loss: 24.021862 Tokens per Sec: 814.027533\n",
      "Epoch Step: 50 Loss: 27.581514 Tokens per Sec: 795.939358\n",
      "Epoch Step: 60 Loss: 18.111464 Tokens per Sec: 805.768068\n",
      "Epoch Step: 70 Loss: 65.179932 Tokens per Sec: 695.759662\n",
      "Epoch Step: 80 Loss: 11.914173 Tokens per Sec: 771.760779\n",
      "Epoch Step: 90 Loss: 39.820831 Tokens per Sec: 720.809211\n",
      "Epoch Step: 100 Loss: 49.527702 Tokens per Sec: 694.939791\n",
      "Epoch Step: 110 Loss: 109.642006 Tokens per Sec: 781.845764\n",
      "Epoch Step: 120 Loss: 17.025387 Tokens per Sec: 707.842311\n",
      "Epoch Step: 130 Loss: 38.526962 Tokens per Sec: 664.394020\n",
      "Epoch Step: 140 Loss: 27.590942 Tokens per Sec: 750.757981\n",
      "Epoch Step: 150 Loss: 66.042053 Tokens per Sec: 692.540783\n",
      "Epoch Step: 160 Loss: 47.856693 Tokens per Sec: 708.814987\n",
      "Epoch Step: 170 Loss: 93.357170 Tokens per Sec: 835.979952\n",
      "Epoch Step: 180 Loss: 30.830166 Tokens per Sec: 794.982759\n",
      "Epoch Step: 190 Loss: 20.684397 Tokens per Sec: 784.525421\n",
      "Epoch Step: 200 Loss: 104.574936 Tokens per Sec: 805.016223\n",
      "Epoch Step: 210 Loss: 65.564278 Tokens per Sec: 787.245563\n",
      "Epoch Step: 220 Loss: 43.824036 Tokens per Sec: 526.146192\n",
      "Epoch Step: 230 Loss: 50.906952 Tokens per Sec: 661.851486\n",
      "Epoch Step: 240 Loss: 33.887630 Tokens per Sec: 698.590528\n",
      "Epoch Step: 250 Loss: 26.923197 Tokens per Sec: 550.245848\n",
      "Epoch Step: 260 Loss: 64.192207 Tokens per Sec: 770.165188\n",
      "Epoch Step: 270 Loss: 7.547037 Tokens per Sec: 749.729029\n",
      "Epoch Step: 280 Loss: 71.179291 Tokens per Sec: 743.211550\n",
      "Epoch Step: 290 Loss: 32.630062 Tokens per Sec: 701.909297\n",
      "Epoch Step: 300 Loss: 56.396782 Tokens per Sec: 732.531982\n",
      "Epoch Step: 310 Loss: 17.298130 Tokens per Sec: 776.653082\n",
      "Epoch Step: 320 Loss: 53.002583 Tokens per Sec: 808.112626\n",
      "Epoch Step: 330 Loss: 27.421835 Tokens per Sec: 767.830329\n",
      "Epoch Step: 340 Loss: 9.609838 Tokens per Sec: 775.522749\n",
      "Epoch Step: 350 Loss: 30.303612 Tokens per Sec: 786.625764\n",
      "Epoch Step: 360 Loss: 83.525459 Tokens per Sec: 821.869898\n",
      "Epoch Step: 370 Loss: 48.157196 Tokens per Sec: 793.644513\n",
      "Epoch Step: 380 Loss: 45.642155 Tokens per Sec: 804.784914\n",
      "Epoch Step: 390 Loss: 39.338470 Tokens per Sec: 755.394254\n",
      "Epoch Step: 400 Loss: 24.712137 Tokens per Sec: 759.746702\n",
      "Epoch Step: 410 Loss: 19.040573 Tokens per Sec: 756.490653\n",
      "Epoch Step: 420 Loss: 62.103436 Tokens per Sec: 774.139490\n",
      "Epoch Step: 430 Loss: 49.391232 Tokens per Sec: 754.302909\n",
      "Epoch Step: 440 Loss: 28.961952 Tokens per Sec: 798.401926\n",
      "Epoch Step: 450 Loss: 67.594643 Tokens per Sec: 824.000346\n",
      "Epoch Step: 460 Loss: 40.915386 Tokens per Sec: 712.127278\n",
      "Epoch Step: 470 Loss: 36.890160 Tokens per Sec: 718.442592\n",
      "Epoch Step: 480 Loss: 31.802567 Tokens per Sec: 789.286912\n",
      "Epoch Step: 490 Loss: 61.825230 Tokens per Sec: 810.011678\n",
      "Epoch Step: 500 Loss: 9.040115 Tokens per Sec: 740.340345\n",
      "Epoch Step: 510 Loss: 30.663568 Tokens per Sec: 739.554822\n",
      "Epoch Step: 520 Loss: 23.120724 Tokens per Sec: 815.987332\n",
      "Epoch Step: 530 Loss: 54.234020 Tokens per Sec: 830.701028\n",
      "Epoch Step: 540 Loss: 39.346519 Tokens per Sec: 764.009034\n",
      "Epoch Step: 550 Loss: 45.720360 Tokens per Sec: 764.821383\n",
      "Epoch Step: 560 Loss: 21.838186 Tokens per Sec: 821.196299\n",
      "Epoch Step: 570 Loss: 25.929935 Tokens per Sec: 713.065139\n",
      "Epoch Step: 580 Loss: 14.572430 Tokens per Sec: 780.860766\n",
      "Epoch Step: 590 Loss: 49.408836 Tokens per Sec: 785.732568\n",
      "Epoch Step: 600 Loss: 36.868767 Tokens per Sec: 755.538209\n",
      "Epoch Step: 610 Loss: 36.572853 Tokens per Sec: 747.847214\n",
      "Epoch Step: 620 Loss: 54.164062 Tokens per Sec: 768.029773\n",
      "Epoch Step: 630 Loss: 34.009300 Tokens per Sec: 754.569139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghost/anaconda3/envs/py36pyt14/lib/python3.6/site-packages/torchtext/data/field.py:321: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train), lengths\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example #1\n",
      "Src :  , <unk>\n",
      "Trg :  , <unk> <unk>\n",
      "Pred:  ,\n",
      "\n",
      "Example #2\n",
      "Src :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ . \"\n",
      "Trg :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ \"\n",
      "Pred:  <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "\n",
      "Example #3\n",
      "Src :  1 , енді бұл екі <unk> <unk> <unk> ##к . . .\n",
      "Trg :  <unk> , енді бұл екі <unk> <unk> <unk> ##к\n",
      "Pred:  өзен <unk> , <unk> <unk>\n",
      "\n",
      "Validation perplexity: 14.173353\n",
      "Epoch 1\n",
      "Epoch Step: 10 Loss: 72.082909 Tokens per Sec: 673.714824\n",
      "Epoch Step: 20 Loss: 31.870096 Tokens per Sec: 725.970868\n",
      "Epoch Step: 30 Loss: 17.021917 Tokens per Sec: 786.709569\n",
      "Epoch Step: 40 Loss: 1.603067 Tokens per Sec: 782.013416\n",
      "Epoch Step: 50 Loss: 59.180050 Tokens per Sec: 726.422677\n",
      "Epoch Step: 60 Loss: 24.595709 Tokens per Sec: 813.233592\n",
      "Epoch Step: 70 Loss: 62.910248 Tokens per Sec: 811.097384\n",
      "Epoch Step: 80 Loss: 78.679398 Tokens per Sec: 794.606908\n",
      "Epoch Step: 90 Loss: 17.428286 Tokens per Sec: 747.740128\n",
      "Epoch Step: 100 Loss: 25.019188 Tokens per Sec: 773.773101\n",
      "Epoch Step: 110 Loss: 35.499218 Tokens per Sec: 782.769354\n",
      "Epoch Step: 120 Loss: 23.723743 Tokens per Sec: 804.915278\n",
      "Epoch Step: 130 Loss: 50.930611 Tokens per Sec: 679.230561\n",
      "Epoch Step: 140 Loss: 42.381863 Tokens per Sec: 802.515057\n",
      "Epoch Step: 150 Loss: 61.482792 Tokens per Sec: 793.102659\n",
      "Epoch Step: 160 Loss: 17.689404 Tokens per Sec: 806.289075\n",
      "Epoch Step: 170 Loss: 35.581470 Tokens per Sec: 761.766276\n",
      "Epoch Step: 180 Loss: 21.126545 Tokens per Sec: 826.750092\n",
      "Epoch Step: 190 Loss: 21.457932 Tokens per Sec: 819.129385\n",
      "Epoch Step: 200 Loss: 8.712150 Tokens per Sec: 842.160695\n",
      "Epoch Step: 210 Loss: 22.464821 Tokens per Sec: 809.840478\n",
      "Epoch Step: 220 Loss: 58.689171 Tokens per Sec: 810.827937\n",
      "Epoch Step: 230 Loss: 13.503866 Tokens per Sec: 794.626220\n",
      "Epoch Step: 240 Loss: 61.211750 Tokens per Sec: 795.167938\n",
      "Epoch Step: 250 Loss: 42.149807 Tokens per Sec: 756.094502\n",
      "Epoch Step: 260 Loss: 45.983486 Tokens per Sec: 711.675639\n",
      "Epoch Step: 270 Loss: 37.598419 Tokens per Sec: 620.433829\n",
      "Epoch Step: 280 Loss: 44.762451 Tokens per Sec: 725.419207\n",
      "Epoch Step: 290 Loss: 46.788170 Tokens per Sec: 779.741053\n",
      "Epoch Step: 300 Loss: 43.824635 Tokens per Sec: 787.120027\n",
      "Epoch Step: 310 Loss: 47.071918 Tokens per Sec: 799.414449\n",
      "Epoch Step: 320 Loss: 33.262585 Tokens per Sec: 757.631010\n",
      "Epoch Step: 330 Loss: 39.347023 Tokens per Sec: 812.411632\n",
      "Epoch Step: 340 Loss: 31.600121 Tokens per Sec: 731.851042\n",
      "Epoch Step: 350 Loss: 29.241451 Tokens per Sec: 774.556266\n",
      "Epoch Step: 360 Loss: 82.089363 Tokens per Sec: 804.547865\n",
      "Epoch Step: 370 Loss: 29.192707 Tokens per Sec: 824.458601\n",
      "Epoch Step: 380 Loss: 28.745703 Tokens per Sec: 806.398996\n",
      "Epoch Step: 390 Loss: 57.924053 Tokens per Sec: 732.215732\n",
      "Epoch Step: 400 Loss: 22.536459 Tokens per Sec: 737.630837\n",
      "Epoch Step: 410 Loss: 22.189482 Tokens per Sec: 821.642082\n",
      "Epoch Step: 420 Loss: 14.094295 Tokens per Sec: 792.000147\n",
      "Epoch Step: 430 Loss: 33.306030 Tokens per Sec: 825.902819\n",
      "Epoch Step: 440 Loss: 54.493702 Tokens per Sec: 771.541567\n",
      "Epoch Step: 450 Loss: 29.549049 Tokens per Sec: 721.823681\n",
      "Epoch Step: 460 Loss: 30.981623 Tokens per Sec: 700.523676\n",
      "Epoch Step: 470 Loss: 41.059101 Tokens per Sec: 799.174662\n",
      "Epoch Step: 480 Loss: 28.294746 Tokens per Sec: 786.182863\n",
      "Epoch Step: 490 Loss: 23.773483 Tokens per Sec: 706.379352\n",
      "Epoch Step: 500 Loss: 9.374016 Tokens per Sec: 656.307544\n",
      "Epoch Step: 510 Loss: 57.754719 Tokens per Sec: 738.589531\n",
      "Epoch Step: 520 Loss: 34.063919 Tokens per Sec: 802.200249\n",
      "Epoch Step: 530 Loss: 6.246332 Tokens per Sec: 792.225146\n",
      "Epoch Step: 540 Loss: 21.767826 Tokens per Sec: 838.497058\n",
      "Epoch Step: 550 Loss: 10.561395 Tokens per Sec: 756.720176\n",
      "Epoch Step: 560 Loss: 81.505173 Tokens per Sec: 812.366732\n",
      "Epoch Step: 570 Loss: 20.582846 Tokens per Sec: 728.767441\n",
      "Epoch Step: 580 Loss: 5.216488 Tokens per Sec: 824.339658\n",
      "Epoch Step: 590 Loss: 69.221542 Tokens per Sec: 821.630321\n",
      "Epoch Step: 600 Loss: 22.918612 Tokens per Sec: 837.736979\n",
      "Epoch Step: 610 Loss: 33.060947 Tokens per Sec: 780.059145\n",
      "Epoch Step: 620 Loss: 77.674698 Tokens per Sec: 807.110734\n",
      "Epoch Step: 630 Loss: 6.260849 Tokens per Sec: 824.621986\n",
      "\n",
      "Example #1\n",
      "Src :  , <unk>\n",
      "Trg :  , <unk> <unk>\n",
      "Pred:  , <unk> , <unk>\n",
      "\n",
      "Example #2\n",
      "Src :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ . \"\n",
      "Trg :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ \"\n",
      "Pred:  <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \" <unk> <unk>\n",
      "\n",
      "Example #3\n",
      "Src :  1 , енді бұл екі <unk> <unk> <unk> ##к . . .\n",
      "Trg :  <unk> , енді бұл екі <unk> <unk> <unk> ##к\n",
      "Pred:  к ##з , <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "\n",
      "Validation perplexity: 10.147358\n",
      "Epoch 2\n",
      "Epoch Step: 10 Loss: 18.876078 Tokens per Sec: 815.002987\n",
      "Epoch Step: 20 Loss: 20.884769 Tokens per Sec: 760.724315\n",
      "Epoch Step: 30 Loss: 8.405499 Tokens per Sec: 792.420969\n",
      "Epoch Step: 40 Loss: 49.946030 Tokens per Sec: 848.632781\n",
      "Epoch Step: 50 Loss: 31.969139 Tokens per Sec: 835.513102\n",
      "Epoch Step: 60 Loss: 39.306133 Tokens per Sec: 740.948004\n",
      "Epoch Step: 70 Loss: 62.153358 Tokens per Sec: 829.306050\n",
      "Epoch Step: 80 Loss: 4.807813 Tokens per Sec: 768.664502\n",
      "Epoch Step: 90 Loss: 52.484196 Tokens per Sec: 815.197498\n",
      "Epoch Step: 100 Loss: 17.782698 Tokens per Sec: 856.612813\n",
      "Epoch Step: 110 Loss: 58.210697 Tokens per Sec: 803.922782\n",
      "Epoch Step: 120 Loss: 64.841621 Tokens per Sec: 824.655032\n",
      "Epoch Step: 130 Loss: 26.449499 Tokens per Sec: 811.320897\n",
      "Epoch Step: 140 Loss: 55.895870 Tokens per Sec: 790.305683\n",
      "Epoch Step: 150 Loss: 36.080887 Tokens per Sec: 787.092956\n",
      "Epoch Step: 160 Loss: 27.840061 Tokens per Sec: 809.434459\n",
      "Epoch Step: 170 Loss: 51.383114 Tokens per Sec: 827.859727\n",
      "Epoch Step: 180 Loss: 31.451338 Tokens per Sec: 834.390780\n",
      "Epoch Step: 190 Loss: 15.848716 Tokens per Sec: 785.020473\n",
      "Epoch Step: 200 Loss: 41.803623 Tokens per Sec: 839.185210\n",
      "Epoch Step: 210 Loss: 28.829647 Tokens per Sec: 830.591528\n",
      "Epoch Step: 220 Loss: 21.124416 Tokens per Sec: 807.809500\n",
      "Epoch Step: 230 Loss: 8.054322 Tokens per Sec: 815.122133\n",
      "Epoch Step: 240 Loss: 24.775301 Tokens per Sec: 800.905021\n",
      "Epoch Step: 250 Loss: 60.745480 Tokens per Sec: 799.318513\n",
      "Epoch Step: 260 Loss: 18.209152 Tokens per Sec: 803.192539\n",
      "Epoch Step: 270 Loss: 51.437843 Tokens per Sec: 792.377132\n",
      "Epoch Step: 280 Loss: 9.590152 Tokens per Sec: 796.645711\n",
      "Epoch Step: 290 Loss: 50.735355 Tokens per Sec: 835.562527\n",
      "Epoch Step: 300 Loss: 17.871170 Tokens per Sec: 798.181053\n",
      "Epoch Step: 310 Loss: 33.352932 Tokens per Sec: 752.930643\n",
      "Epoch Step: 320 Loss: 46.183441 Tokens per Sec: 808.756282\n",
      "Epoch Step: 330 Loss: 43.108135 Tokens per Sec: 792.856117\n",
      "Epoch Step: 340 Loss: 11.197464 Tokens per Sec: 814.274367\n",
      "Epoch Step: 350 Loss: 4.128613 Tokens per Sec: 733.402062\n",
      "Epoch Step: 360 Loss: 26.958029 Tokens per Sec: 787.012644\n",
      "Epoch Step: 370 Loss: 13.650084 Tokens per Sec: 807.157764\n",
      "Epoch Step: 380 Loss: 27.333521 Tokens per Sec: 756.900336\n",
      "Epoch Step: 390 Loss: 94.327354 Tokens per Sec: 813.386871\n",
      "Epoch Step: 400 Loss: 57.653217 Tokens per Sec: 777.413998\n",
      "Epoch Step: 410 Loss: 42.276031 Tokens per Sec: 739.379912\n",
      "Epoch Step: 420 Loss: 15.538089 Tokens per Sec: 809.231291\n",
      "Epoch Step: 430 Loss: 49.165142 Tokens per Sec: 729.207830\n",
      "Epoch Step: 440 Loss: 51.652416 Tokens per Sec: 750.221094\n",
      "Epoch Step: 450 Loss: 55.810867 Tokens per Sec: 805.929763\n",
      "Epoch Step: 460 Loss: 71.897789 Tokens per Sec: 748.129694\n",
      "Epoch Step: 470 Loss: 20.115946 Tokens per Sec: 736.208374\n",
      "Epoch Step: 480 Loss: 3.935513 Tokens per Sec: 772.591534\n",
      "Epoch Step: 490 Loss: 17.432671 Tokens per Sec: 785.315302\n",
      "Epoch Step: 500 Loss: 11.392637 Tokens per Sec: 689.832003\n",
      "Epoch Step: 510 Loss: 28.402308 Tokens per Sec: 803.504893\n",
      "Epoch Step: 520 Loss: 54.885128 Tokens per Sec: 760.991034\n",
      "Epoch Step: 530 Loss: 23.381321 Tokens per Sec: 789.806228\n",
      "Epoch Step: 540 Loss: 26.452280 Tokens per Sec: 828.668181\n",
      "Epoch Step: 550 Loss: 38.362267 Tokens per Sec: 807.831618\n",
      "Epoch Step: 560 Loss: 51.142063 Tokens per Sec: 838.400197\n",
      "Epoch Step: 570 Loss: 43.603687 Tokens per Sec: 814.075102\n",
      "Epoch Step: 580 Loss: 27.353914 Tokens per Sec: 776.790172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 590 Loss: 29.487797 Tokens per Sec: 801.532598\n",
      "Epoch Step: 600 Loss: 37.868858 Tokens per Sec: 806.798623\n",
      "Epoch Step: 610 Loss: 17.115625 Tokens per Sec: 742.403100\n",
      "Epoch Step: 620 Loss: 50.686188 Tokens per Sec: 765.512284\n",
      "Epoch Step: 630 Loss: 41.771175 Tokens per Sec: 718.617571\n",
      "\n",
      "Example #1\n",
      "Src :  , <unk>\n",
      "Trg :  , <unk> <unk>\n",
      "Pred:  ,\n",
      "\n",
      "Example #2\n",
      "Src :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ . \"\n",
      "Trg :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ \"\n",
      "Pred:  <unk> <unk> <unk> \" , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##мен <unk> <unk> <unk>\n",
      "\n",
      "Example #3\n",
      "Src :  1 , енді бұл екі <unk> <unk> <unk> ##к . . .\n",
      "Trg :  <unk> , енді бұл екі <unk> <unk> <unk> ##к\n",
      "Pred:  астана <unk> , <unk> <unk> <unk> <unk>\n",
      "\n",
      "Validation perplexity: 7.599001\n",
      "Epoch 3\n",
      "Epoch Step: 10 Loss: 6.284361 Tokens per Sec: 826.418205\n",
      "Epoch Step: 20 Loss: 43.167645 Tokens per Sec: 752.018198\n",
      "Epoch Step: 30 Loss: 8.780941 Tokens per Sec: 783.616448\n",
      "Epoch Step: 40 Loss: 22.458284 Tokens per Sec: 780.556758\n",
      "Epoch Step: 50 Loss: 10.096614 Tokens per Sec: 760.998119\n",
      "Epoch Step: 60 Loss: 18.372652 Tokens per Sec: 776.189532\n",
      "Epoch Step: 70 Loss: 16.598436 Tokens per Sec: 807.355392\n",
      "Epoch Step: 80 Loss: 23.822096 Tokens per Sec: 840.252468\n",
      "Epoch Step: 90 Loss: 45.074097 Tokens per Sec: 825.995781\n",
      "Epoch Step: 100 Loss: 40.339546 Tokens per Sec: 780.739504\n",
      "Epoch Step: 110 Loss: 16.791384 Tokens per Sec: 803.376623\n",
      "Epoch Step: 120 Loss: 18.766857 Tokens per Sec: 789.521852\n",
      "Epoch Step: 130 Loss: 12.720564 Tokens per Sec: 791.758672\n",
      "Epoch Step: 140 Loss: 13.374785 Tokens per Sec: 816.282830\n",
      "Epoch Step: 150 Loss: 11.688786 Tokens per Sec: 790.121419\n",
      "Epoch Step: 160 Loss: 10.366329 Tokens per Sec: 788.037905\n",
      "Epoch Step: 170 Loss: 27.556793 Tokens per Sec: 835.353282\n",
      "Epoch Step: 180 Loss: 48.027557 Tokens per Sec: 791.984759\n",
      "Epoch Step: 190 Loss: 11.345434 Tokens per Sec: 819.982970\n",
      "Epoch Step: 200 Loss: 21.863188 Tokens per Sec: 789.920574\n",
      "Epoch Step: 210 Loss: 52.297764 Tokens per Sec: 791.071405\n",
      "Epoch Step: 220 Loss: 19.860973 Tokens per Sec: 756.968707\n",
      "Epoch Step: 230 Loss: 14.915804 Tokens per Sec: 738.554053\n",
      "Epoch Step: 240 Loss: 18.611012 Tokens per Sec: 755.172031\n",
      "Epoch Step: 250 Loss: 41.484928 Tokens per Sec: 716.660874\n",
      "Epoch Step: 260 Loss: 13.015790 Tokens per Sec: 738.315220\n",
      "Epoch Step: 270 Loss: 21.778957 Tokens per Sec: 740.041942\n",
      "Epoch Step: 280 Loss: 61.761841 Tokens per Sec: 824.978784\n",
      "Epoch Step: 290 Loss: 25.982323 Tokens per Sec: 786.999795\n",
      "Epoch Step: 300 Loss: 8.965481 Tokens per Sec: 812.839339\n",
      "Epoch Step: 310 Loss: 38.320030 Tokens per Sec: 828.970692\n",
      "Epoch Step: 320 Loss: 13.871094 Tokens per Sec: 758.128983\n",
      "Epoch Step: 330 Loss: 63.400841 Tokens per Sec: 737.244902\n",
      "Epoch Step: 340 Loss: 30.367617 Tokens per Sec: 828.660137\n",
      "Epoch Step: 350 Loss: 0.607108 Tokens per Sec: 618.716708\n",
      "Epoch Step: 360 Loss: 50.014690 Tokens per Sec: 646.638611\n",
      "Epoch Step: 370 Loss: 20.411444 Tokens per Sec: 763.952335\n",
      "Epoch Step: 380 Loss: 14.036833 Tokens per Sec: 756.026628\n",
      "Epoch Step: 390 Loss: 12.202434 Tokens per Sec: 791.194570\n",
      "Epoch Step: 400 Loss: 34.840687 Tokens per Sec: 776.990814\n",
      "Epoch Step: 410 Loss: 9.344378 Tokens per Sec: 827.278895\n",
      "Epoch Step: 420 Loss: 45.802177 Tokens per Sec: 776.771633\n",
      "Epoch Step: 430 Loss: 39.735416 Tokens per Sec: 809.004774\n",
      "Epoch Step: 440 Loss: 24.661823 Tokens per Sec: 758.586091\n",
      "Epoch Step: 450 Loss: 14.332058 Tokens per Sec: 745.246944\n",
      "Epoch Step: 460 Loss: 36.210686 Tokens per Sec: 724.512544\n",
      "Epoch Step: 470 Loss: 4.040725 Tokens per Sec: 772.082018\n",
      "Epoch Step: 480 Loss: 22.269918 Tokens per Sec: 830.586621\n",
      "Epoch Step: 490 Loss: 0.523678 Tokens per Sec: 793.894393\n",
      "Epoch Step: 500 Loss: 24.972622 Tokens per Sec: 783.797781\n",
      "Epoch Step: 510 Loss: 7.818331 Tokens per Sec: 855.831586\n",
      "Epoch Step: 520 Loss: 24.284910 Tokens per Sec: 762.153123\n",
      "Epoch Step: 530 Loss: 19.140804 Tokens per Sec: 813.745222\n",
      "Epoch Step: 540 Loss: 0.592517 Tokens per Sec: 788.432422\n",
      "Epoch Step: 550 Loss: 9.829929 Tokens per Sec: 817.468374\n",
      "Epoch Step: 560 Loss: 2.524326 Tokens per Sec: 774.343785\n",
      "Epoch Step: 570 Loss: 58.563217 Tokens per Sec: 762.239938\n",
      "Epoch Step: 580 Loss: 22.445087 Tokens per Sec: 774.191154\n",
      "Epoch Step: 590 Loss: 8.176760 Tokens per Sec: 798.096647\n",
      "Epoch Step: 600 Loss: 9.916236 Tokens per Sec: 741.125306\n",
      "Epoch Step: 610 Loss: 14.568933 Tokens per Sec: 836.991171\n",
      "Epoch Step: 620 Loss: 16.859638 Tokens per Sec: 750.076917\n",
      "Epoch Step: 630 Loss: 21.010639 Tokens per Sec: 830.149182\n",
      "\n",
      "Example #1\n",
      "Src :  , <unk>\n",
      "Trg :  , <unk> <unk>\n",
      "Pred:  ,\n",
      "\n",
      "Example #2\n",
      "Src :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ . \"\n",
      "Trg :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ \"\n",
      "Pred:  <unk> <unk> , \" <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##сіз\n",
      "\n",
      "Example #3\n",
      "Src :  1 , енді бұл екі <unk> <unk> <unk> ##к . . .\n",
      "Trg :  <unk> , енді бұл екі <unk> <unk> <unk> ##к\n",
      "Pred:  к <unk> , деді <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##к <unk> <unk> <unk>\n",
      "\n",
      "Validation perplexity: 5.843449\n",
      "Epoch 4\n",
      "Epoch Step: 10 Loss: 53.268021 Tokens per Sec: 786.927371\n",
      "Epoch Step: 20 Loss: 8.149383 Tokens per Sec: 806.241783\n",
      "Epoch Step: 30 Loss: 15.742183 Tokens per Sec: 823.271290\n",
      "Epoch Step: 40 Loss: 28.731052 Tokens per Sec: 822.372237\n",
      "Epoch Step: 50 Loss: 0.923570 Tokens per Sec: 785.813749\n",
      "Epoch Step: 60 Loss: 12.930118 Tokens per Sec: 807.011484\n",
      "Epoch Step: 70 Loss: 12.409860 Tokens per Sec: 828.588469\n",
      "Epoch Step: 80 Loss: 14.761871 Tokens per Sec: 796.927087\n",
      "Epoch Step: 90 Loss: 12.447799 Tokens per Sec: 798.726847\n",
      "Epoch Step: 100 Loss: 9.387261 Tokens per Sec: 727.653213\n",
      "Epoch Step: 110 Loss: 36.796455 Tokens per Sec: 810.492616\n",
      "Epoch Step: 120 Loss: 50.260628 Tokens per Sec: 780.122687\n",
      "Epoch Step: 130 Loss: 27.359337 Tokens per Sec: 759.272746\n",
      "Epoch Step: 140 Loss: 1.162203 Tokens per Sec: 795.039713\n",
      "Epoch Step: 150 Loss: 9.785986 Tokens per Sec: 815.398369\n",
      "Epoch Step: 160 Loss: 0.140622 Tokens per Sec: 859.664685\n",
      "Epoch Step: 170 Loss: 66.568123 Tokens per Sec: 812.921602\n",
      "Epoch Step: 180 Loss: 17.647339 Tokens per Sec: 805.014917\n",
      "Epoch Step: 190 Loss: 6.208073 Tokens per Sec: 756.636650\n",
      "Epoch Step: 200 Loss: 2.899339 Tokens per Sec: 822.451253\n",
      "Epoch Step: 210 Loss: 17.174955 Tokens per Sec: 759.989362\n",
      "Epoch Step: 220 Loss: 35.462631 Tokens per Sec: 800.301284\n",
      "Epoch Step: 230 Loss: 35.086102 Tokens per Sec: 796.668253\n",
      "Epoch Step: 240 Loss: 11.463165 Tokens per Sec: 824.351060\n",
      "Epoch Step: 250 Loss: 24.624786 Tokens per Sec: 797.162696\n",
      "Epoch Step: 260 Loss: 21.171812 Tokens per Sec: 816.356960\n",
      "Epoch Step: 270 Loss: 43.506157 Tokens per Sec: 814.821798\n",
      "Epoch Step: 280 Loss: 6.328445 Tokens per Sec: 793.248983\n",
      "Epoch Step: 290 Loss: 5.755631 Tokens per Sec: 775.527908\n",
      "Epoch Step: 300 Loss: 7.942912 Tokens per Sec: 837.838317\n",
      "Epoch Step: 310 Loss: 1.024974 Tokens per Sec: 766.964633\n",
      "Epoch Step: 320 Loss: 33.820301 Tokens per Sec: 824.203171\n",
      "Epoch Step: 330 Loss: 5.795905 Tokens per Sec: 797.496199\n",
      "Epoch Step: 340 Loss: 4.613551 Tokens per Sec: 777.336838\n",
      "Epoch Step: 350 Loss: 11.953929 Tokens per Sec: 734.760680\n",
      "Epoch Step: 360 Loss: 11.118770 Tokens per Sec: 838.445667\n",
      "Epoch Step: 370 Loss: 24.002151 Tokens per Sec: 841.784438\n",
      "Epoch Step: 380 Loss: 10.366880 Tokens per Sec: 829.606963\n",
      "Epoch Step: 390 Loss: 29.087538 Tokens per Sec: 799.375242\n",
      "Epoch Step: 400 Loss: 16.203188 Tokens per Sec: 824.397373\n",
      "Epoch Step: 410 Loss: 11.648436 Tokens per Sec: 764.210883\n",
      "Epoch Step: 420 Loss: 10.909230 Tokens per Sec: 786.724623\n",
      "Epoch Step: 430 Loss: 52.204956 Tokens per Sec: 832.979961\n",
      "Epoch Step: 440 Loss: 16.273388 Tokens per Sec: 775.820761\n",
      "Epoch Step: 450 Loss: 12.212206 Tokens per Sec: 802.891271\n",
      "Epoch Step: 460 Loss: 16.500906 Tokens per Sec: 832.728473\n",
      "Epoch Step: 470 Loss: 8.088671 Tokens per Sec: 777.794155\n",
      "Epoch Step: 480 Loss: 46.551998 Tokens per Sec: 783.413269\n",
      "Epoch Step: 490 Loss: 2.445931 Tokens per Sec: 732.608377\n",
      "Epoch Step: 500 Loss: 8.067381 Tokens per Sec: 839.841692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 510 Loss: 0.873957 Tokens per Sec: 810.143534\n",
      "Epoch Step: 520 Loss: 4.855072 Tokens per Sec: 811.400600\n",
      "Epoch Step: 530 Loss: 10.548916 Tokens per Sec: 806.001141\n",
      "Epoch Step: 540 Loss: 33.067776 Tokens per Sec: 784.571069\n",
      "Epoch Step: 550 Loss: 13.327781 Tokens per Sec: 754.408228\n",
      "Epoch Step: 560 Loss: 5.237005 Tokens per Sec: 758.482279\n",
      "Epoch Step: 570 Loss: 13.086868 Tokens per Sec: 760.498474\n",
      "Epoch Step: 580 Loss: 23.406284 Tokens per Sec: 778.563358\n",
      "Epoch Step: 590 Loss: 4.359571 Tokens per Sec: 851.032592\n",
      "Epoch Step: 600 Loss: 2.403132 Tokens per Sec: 792.297580\n",
      "Epoch Step: 610 Loss: 6.469162 Tokens per Sec: 765.102361\n",
      "Epoch Step: 620 Loss: 34.794270 Tokens per Sec: 845.300999\n",
      "Epoch Step: 630 Loss: 25.545683 Tokens per Sec: 764.148873\n",
      "\n",
      "Example #1\n",
      "Src :  , <unk>\n",
      "Trg :  , <unk> <unk>\n",
      "Pred:  , <unk>\n",
      "\n",
      "Example #2\n",
      "Src :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ . \"\n",
      "Trg :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ \"\n",
      "Pred:  <unk> , \" <unk> \" <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ің <unk> <unk>\n",
      "\n",
      "Example #3\n",
      "Src :  1 , енді бұл екі <unk> <unk> <unk> ##к . . .\n",
      "Trg :  <unk> , енді бұл екі <unk> <unk> <unk> ##к\n",
      "Pred:  алып , <unk> <unk> <unk> <unk> ##к к <unk> <unk> ##к <unk> <unk>\n",
      "\n",
      "Validation perplexity: 5.734421\n",
      "Epoch 5\n",
      "Epoch Step: 10 Loss: 3.112057 Tokens per Sec: 812.897318\n",
      "Epoch Step: 20 Loss: 13.344222 Tokens per Sec: 810.313117\n",
      "Epoch Step: 30 Loss: 27.239639 Tokens per Sec: 823.336022\n",
      "Epoch Step: 40 Loss: 10.150456 Tokens per Sec: 788.153683\n",
      "Epoch Step: 50 Loss: 28.492445 Tokens per Sec: 861.501048\n",
      "Epoch Step: 60 Loss: 11.511173 Tokens per Sec: 752.430683\n",
      "Epoch Step: 70 Loss: 8.283454 Tokens per Sec: 781.777658\n",
      "Epoch Step: 80 Loss: 6.437848 Tokens per Sec: 789.231388\n",
      "Epoch Step: 90 Loss: 0.027533 Tokens per Sec: 787.357614\n",
      "Epoch Step: 100 Loss: 23.596712 Tokens per Sec: 821.492110\n",
      "Epoch Step: 110 Loss: 7.155337 Tokens per Sec: 781.107807\n",
      "Epoch Step: 120 Loss: 8.119792 Tokens per Sec: 755.910856\n",
      "Epoch Step: 130 Loss: 30.742432 Tokens per Sec: 819.902050\n",
      "Epoch Step: 140 Loss: 4.994061 Tokens per Sec: 819.228236\n",
      "Epoch Step: 150 Loss: 1.052620 Tokens per Sec: 831.157561\n",
      "Epoch Step: 160 Loss: 23.603378 Tokens per Sec: 780.152098\n",
      "Epoch Step: 170 Loss: 6.926347 Tokens per Sec: 822.739834\n",
      "Epoch Step: 180 Loss: 0.384250 Tokens per Sec: 811.799636\n",
      "Epoch Step: 190 Loss: 8.136333 Tokens per Sec: 817.893516\n",
      "Epoch Step: 200 Loss: 2.135975 Tokens per Sec: 804.443812\n",
      "Epoch Step: 210 Loss: 35.135777 Tokens per Sec: 842.618674\n",
      "Epoch Step: 220 Loss: 15.782563 Tokens per Sec: 834.358557\n",
      "Epoch Step: 230 Loss: 0.153099 Tokens per Sec: 813.516375\n",
      "Epoch Step: 240 Loss: 1.260084 Tokens per Sec: 759.038104\n",
      "Epoch Step: 250 Loss: 11.891881 Tokens per Sec: 814.879717\n",
      "Epoch Step: 260 Loss: 58.662361 Tokens per Sec: 830.038414\n",
      "Epoch Step: 270 Loss: 26.872641 Tokens per Sec: 882.881762\n",
      "Epoch Step: 280 Loss: 33.352226 Tokens per Sec: 783.705085\n",
      "Epoch Step: 290 Loss: 7.331310 Tokens per Sec: 766.960291\n",
      "Epoch Step: 300 Loss: 0.435066 Tokens per Sec: 820.082108\n",
      "Epoch Step: 310 Loss: 0.172198 Tokens per Sec: 817.477049\n",
      "Epoch Step: 320 Loss: 2.549168 Tokens per Sec: 823.183723\n",
      "Epoch Step: 330 Loss: 4.077929 Tokens per Sec: 833.905179\n",
      "Epoch Step: 340 Loss: 13.796076 Tokens per Sec: 835.590062\n",
      "Epoch Step: 350 Loss: 9.970075 Tokens per Sec: 811.452789\n",
      "Epoch Step: 360 Loss: 7.237679 Tokens per Sec: 800.172461\n",
      "Epoch Step: 370 Loss: 17.782059 Tokens per Sec: 794.672886\n",
      "Epoch Step: 380 Loss: 14.085068 Tokens per Sec: 815.754288\n",
      "Epoch Step: 390 Loss: 15.394156 Tokens per Sec: 760.748470\n",
      "Epoch Step: 400 Loss: 12.649995 Tokens per Sec: 814.876111\n",
      "Epoch Step: 410 Loss: 7.032361 Tokens per Sec: 825.541394\n",
      "Epoch Step: 420 Loss: 18.367670 Tokens per Sec: 828.331900\n",
      "Epoch Step: 430 Loss: 26.291897 Tokens per Sec: 767.375777\n",
      "Epoch Step: 440 Loss: 11.930086 Tokens per Sec: 792.724249\n",
      "Epoch Step: 450 Loss: 7.909752 Tokens per Sec: 798.716362\n",
      "Epoch Step: 460 Loss: 33.538795 Tokens per Sec: 846.820917\n",
      "Epoch Step: 470 Loss: 52.443676 Tokens per Sec: 778.414579\n",
      "Epoch Step: 480 Loss: 3.136798 Tokens per Sec: 820.849188\n",
      "Epoch Step: 490 Loss: 13.307339 Tokens per Sec: 797.485884\n",
      "Epoch Step: 500 Loss: 23.027800 Tokens per Sec: 816.145458\n",
      "Epoch Step: 510 Loss: 0.763508 Tokens per Sec: 809.768889\n",
      "Epoch Step: 520 Loss: 0.772092 Tokens per Sec: 761.100766\n",
      "Epoch Step: 530 Loss: 4.148059 Tokens per Sec: 737.150720\n",
      "Epoch Step: 540 Loss: 0.211360 Tokens per Sec: 828.099832\n",
      "Epoch Step: 550 Loss: 5.012893 Tokens per Sec: 795.383027\n",
      "Epoch Step: 560 Loss: 4.254527 Tokens per Sec: 778.825966\n",
      "Epoch Step: 570 Loss: 31.570314 Tokens per Sec: 786.071843\n",
      "Epoch Step: 580 Loss: 1.223130 Tokens per Sec: 831.196826\n",
      "Epoch Step: 590 Loss: 19.404684 Tokens per Sec: 818.582688\n",
      "Epoch Step: 600 Loss: 3.504997 Tokens per Sec: 788.140279\n",
      "Epoch Step: 610 Loss: 12.025143 Tokens per Sec: 817.785942\n",
      "Epoch Step: 620 Loss: 10.944478 Tokens per Sec: 822.066529\n",
      "Epoch Step: 630 Loss: 11.674640 Tokens per Sec: 740.400339\n",
      "\n",
      "Example #1\n",
      "Src :  , <unk>\n",
      "Trg :  , <unk> <unk>\n",
      "Pred:  ,\n",
      "\n",
      "Example #2\n",
      "Src :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ . \"\n",
      "Trg :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ \"\n",
      "Pred:  <unk> <unk> , \" <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "\n",
      "Example #3\n",
      "Src :  1 , енді бұл екі <unk> <unk> <unk> ##к . . .\n",
      "Trg :  <unk> , енді бұл екі <unk> <unk> <unk> ##к\n",
      "Pred:  бір , п <unk> <unk> <unk> <unk>\n",
      "\n",
      "Validation perplexity: 5.208562\n",
      "Epoch 6\n",
      "Epoch Step: 10 Loss: 15.425719 Tokens per Sec: 787.561628\n",
      "Epoch Step: 20 Loss: 7.063123 Tokens per Sec: 794.289338\n",
      "Epoch Step: 30 Loss: 26.501905 Tokens per Sec: 830.831290\n",
      "Epoch Step: 40 Loss: 4.055940 Tokens per Sec: 772.705784\n",
      "Epoch Step: 50 Loss: 13.112844 Tokens per Sec: 776.040943\n",
      "Epoch Step: 60 Loss: 11.106215 Tokens per Sec: 812.121560\n",
      "Epoch Step: 70 Loss: 4.961779 Tokens per Sec: 797.711776\n",
      "Epoch Step: 80 Loss: 4.043712 Tokens per Sec: 782.935760\n",
      "Epoch Step: 90 Loss: 14.847943 Tokens per Sec: 816.227230\n",
      "Epoch Step: 100 Loss: 4.171154 Tokens per Sec: 822.646552\n",
      "Epoch Step: 110 Loss: 8.942870 Tokens per Sec: 788.936621\n",
      "Epoch Step: 120 Loss: 8.275094 Tokens per Sec: 867.865841\n",
      "Epoch Step: 130 Loss: 1.786020 Tokens per Sec: 814.303560\n",
      "Epoch Step: 140 Loss: 4.496198 Tokens per Sec: 838.321644\n",
      "Epoch Step: 150 Loss: 15.192925 Tokens per Sec: 823.972347\n",
      "Epoch Step: 160 Loss: 15.723246 Tokens per Sec: 782.616334\n",
      "Epoch Step: 170 Loss: 10.344934 Tokens per Sec: 794.888638\n",
      "Epoch Step: 180 Loss: 17.251549 Tokens per Sec: 823.973554\n",
      "Epoch Step: 190 Loss: 17.377237 Tokens per Sec: 829.549978\n",
      "Epoch Step: 200 Loss: 7.771242 Tokens per Sec: 825.340930\n",
      "Epoch Step: 210 Loss: 8.336985 Tokens per Sec: 828.482068\n",
      "Epoch Step: 220 Loss: 8.399570 Tokens per Sec: 817.389891\n",
      "Epoch Step: 230 Loss: 5.980383 Tokens per Sec: 835.325286\n",
      "Epoch Step: 240 Loss: 1.887955 Tokens per Sec: 790.884917\n",
      "Epoch Step: 250 Loss: 3.777574 Tokens per Sec: 749.577001\n",
      "Epoch Step: 260 Loss: 1.208432 Tokens per Sec: 836.657267\n",
      "Epoch Step: 270 Loss: 6.060400 Tokens per Sec: 846.111079\n",
      "Epoch Step: 280 Loss: 15.688745 Tokens per Sec: 841.226002\n",
      "Epoch Step: 290 Loss: 7.776155 Tokens per Sec: 851.337223\n",
      "Epoch Step: 300 Loss: 20.809628 Tokens per Sec: 835.399248\n",
      "Epoch Step: 310 Loss: 3.971509 Tokens per Sec: 803.617500\n",
      "Epoch Step: 320 Loss: 4.230225 Tokens per Sec: 725.691523\n",
      "Epoch Step: 330 Loss: 3.049859 Tokens per Sec: 816.608525\n",
      "Epoch Step: 340 Loss: 7.584731 Tokens per Sec: 836.007656\n",
      "Epoch Step: 350 Loss: 4.013914 Tokens per Sec: 803.441580\n",
      "Epoch Step: 360 Loss: 12.636404 Tokens per Sec: 854.137662\n",
      "Epoch Step: 370 Loss: 1.161178 Tokens per Sec: 817.164793\n",
      "Epoch Step: 380 Loss: 3.069545 Tokens per Sec: 801.132501\n",
      "Epoch Step: 390 Loss: 5.552160 Tokens per Sec: 752.877655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 400 Loss: 2.694133 Tokens per Sec: 808.798055\n",
      "Epoch Step: 410 Loss: 7.129378 Tokens per Sec: 822.391963\n",
      "Epoch Step: 420 Loss: 8.878822 Tokens per Sec: 862.072358\n",
      "Epoch Step: 430 Loss: 1.996761 Tokens per Sec: 837.960272\n",
      "Epoch Step: 440 Loss: 4.999410 Tokens per Sec: 802.585203\n",
      "Epoch Step: 450 Loss: 1.024964 Tokens per Sec: 787.794815\n",
      "Epoch Step: 460 Loss: 6.456123 Tokens per Sec: 824.336558\n",
      "Epoch Step: 470 Loss: 7.022364 Tokens per Sec: 813.097233\n",
      "Epoch Step: 480 Loss: 5.691071 Tokens per Sec: 835.141125\n",
      "Epoch Step: 490 Loss: 3.061993 Tokens per Sec: 812.928385\n",
      "Epoch Step: 500 Loss: 4.843996 Tokens per Sec: 817.523438\n",
      "Epoch Step: 510 Loss: 7.054939 Tokens per Sec: 864.901845\n",
      "Epoch Step: 520 Loss: 20.410400 Tokens per Sec: 834.474593\n",
      "Epoch Step: 530 Loss: 29.121027 Tokens per Sec: 840.068952\n",
      "Epoch Step: 540 Loss: 10.032423 Tokens per Sec: 801.244366\n",
      "Epoch Step: 550 Loss: 6.592482 Tokens per Sec: 829.623535\n",
      "Epoch Step: 560 Loss: 21.573689 Tokens per Sec: 855.283198\n",
      "Epoch Step: 570 Loss: 18.845646 Tokens per Sec: 862.987464\n",
      "Epoch Step: 580 Loss: 8.297089 Tokens per Sec: 776.473575\n",
      "Epoch Step: 590 Loss: 1.940296 Tokens per Sec: 776.911648\n",
      "Epoch Step: 600 Loss: 16.083015 Tokens per Sec: 814.696998\n",
      "Epoch Step: 610 Loss: 3.216544 Tokens per Sec: 806.178081\n",
      "Epoch Step: 620 Loss: 6.578724 Tokens per Sec: 751.762483\n",
      "Epoch Step: 630 Loss: 1.199026 Tokens per Sec: 822.133544\n",
      "\n",
      "Example #1\n",
      "Src :  , <unk>\n",
      "Trg :  , <unk> <unk>\n",
      "Pred:  ,\n",
      "\n",
      "Example #2\n",
      "Src :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ . \"\n",
      "Trg :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ \"\n",
      "Pred:  <unk> , <unk> \" \" астана \" <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ <unk> <unk>\n",
      "\n",
      "Example #3\n",
      "Src :  1 , енді бұл екі <unk> <unk> <unk> ##к . . .\n",
      "Trg :  <unk> , енді бұл екі <unk> <unk> <unk> ##к\n",
      "Pred:  бір , <unk> бұл туралы <unk> <unk> <unk>\n",
      "\n",
      "Validation perplexity: 4.704565\n",
      "Epoch 7\n",
      "Epoch Step: 10 Loss: 2.287721 Tokens per Sec: 810.592455\n",
      "Epoch Step: 20 Loss: 15.236963 Tokens per Sec: 849.512954\n",
      "Epoch Step: 30 Loss: 13.097878 Tokens per Sec: 838.525390\n",
      "Epoch Step: 40 Loss: 6.877117 Tokens per Sec: 828.528134\n",
      "Epoch Step: 50 Loss: 7.022439 Tokens per Sec: 877.821270\n",
      "Epoch Step: 60 Loss: 1.050292 Tokens per Sec: 841.954469\n",
      "Epoch Step: 70 Loss: 0.006345 Tokens per Sec: 802.122371\n",
      "Epoch Step: 80 Loss: 4.243286 Tokens per Sec: 867.726475\n",
      "Epoch Step: 90 Loss: 1.487756 Tokens per Sec: 845.695782\n",
      "Epoch Step: 100 Loss: 9.130439 Tokens per Sec: 810.941428\n",
      "Epoch Step: 110 Loss: 4.405820 Tokens per Sec: 785.712845\n",
      "Epoch Step: 120 Loss: 0.492063 Tokens per Sec: 793.058146\n",
      "Epoch Step: 130 Loss: 13.420917 Tokens per Sec: 754.146013\n",
      "Epoch Step: 140 Loss: 17.962240 Tokens per Sec: 862.595078\n",
      "Epoch Step: 150 Loss: 8.724192 Tokens per Sec: 838.355790\n",
      "Epoch Step: 160 Loss: 0.690825 Tokens per Sec: 813.266240\n",
      "Epoch Step: 170 Loss: 2.822344 Tokens per Sec: 849.630359\n",
      "Epoch Step: 180 Loss: 14.540215 Tokens per Sec: 832.200973\n",
      "Epoch Step: 190 Loss: 0.116360 Tokens per Sec: 784.764376\n",
      "Epoch Step: 200 Loss: 0.514397 Tokens per Sec: 831.146827\n",
      "Epoch Step: 210 Loss: 20.131292 Tokens per Sec: 812.305475\n",
      "Epoch Step: 220 Loss: 0.676270 Tokens per Sec: 784.457975\n",
      "Epoch Step: 230 Loss: 7.469385 Tokens per Sec: 804.442747\n",
      "Epoch Step: 240 Loss: 10.150035 Tokens per Sec: 808.234823\n",
      "Epoch Step: 250 Loss: 2.584336 Tokens per Sec: 846.331010\n",
      "Epoch Step: 260 Loss: 6.838927 Tokens per Sec: 845.024325\n",
      "Epoch Step: 270 Loss: 7.108252 Tokens per Sec: 830.197061\n",
      "Epoch Step: 280 Loss: 11.612013 Tokens per Sec: 840.001759\n",
      "Epoch Step: 290 Loss: 0.120715 Tokens per Sec: 772.624702\n",
      "Epoch Step: 300 Loss: 2.812575 Tokens per Sec: 829.712514\n",
      "Epoch Step: 310 Loss: 7.803750 Tokens per Sec: 820.851427\n",
      "Epoch Step: 320 Loss: 27.058430 Tokens per Sec: 850.885151\n",
      "Epoch Step: 330 Loss: 0.170596 Tokens per Sec: 751.881930\n",
      "Epoch Step: 340 Loss: 9.474198 Tokens per Sec: 839.919437\n",
      "Epoch Step: 350 Loss: 10.962116 Tokens per Sec: 780.179960\n",
      "Epoch Step: 360 Loss: 0.980103 Tokens per Sec: 792.180902\n",
      "Epoch Step: 370 Loss: 0.176886 Tokens per Sec: 798.226509\n",
      "Epoch Step: 380 Loss: 5.797288 Tokens per Sec: 795.690187\n",
      "Epoch Step: 390 Loss: 4.086532 Tokens per Sec: 798.960379\n",
      "Epoch Step: 400 Loss: 3.074502 Tokens per Sec: 842.128258\n",
      "Epoch Step: 410 Loss: 11.972057 Tokens per Sec: 771.148795\n",
      "Epoch Step: 420 Loss: 1.781849 Tokens per Sec: 817.308724\n",
      "Epoch Step: 430 Loss: 0.114737 Tokens per Sec: 770.925938\n",
      "Epoch Step: 440 Loss: 3.141950 Tokens per Sec: 796.382635\n",
      "Epoch Step: 450 Loss: 25.546705 Tokens per Sec: 823.949835\n",
      "Epoch Step: 460 Loss: 33.699215 Tokens per Sec: 719.828057\n",
      "Epoch Step: 470 Loss: 0.010781 Tokens per Sec: 788.816211\n",
      "Epoch Step: 480 Loss: 0.741265 Tokens per Sec: 747.527050\n",
      "Epoch Step: 490 Loss: 11.880774 Tokens per Sec: 645.444219\n",
      "Epoch Step: 500 Loss: 1.623259 Tokens per Sec: 762.880193\n",
      "Epoch Step: 510 Loss: 3.145564 Tokens per Sec: 792.631750\n",
      "Epoch Step: 520 Loss: 8.351992 Tokens per Sec: 798.638363\n",
      "Epoch Step: 530 Loss: 8.939583 Tokens per Sec: 762.731449\n",
      "Epoch Step: 540 Loss: 0.464230 Tokens per Sec: 734.192816\n",
      "Epoch Step: 550 Loss: 5.623312 Tokens per Sec: 716.189479\n",
      "Epoch Step: 560 Loss: 3.380995 Tokens per Sec: 821.087195\n",
      "Epoch Step: 570 Loss: 1.463856 Tokens per Sec: 763.532832\n",
      "Epoch Step: 580 Loss: 11.095838 Tokens per Sec: 747.008620\n",
      "Epoch Step: 590 Loss: 6.286747 Tokens per Sec: 755.140851\n",
      "Epoch Step: 600 Loss: 3.972219 Tokens per Sec: 749.455322\n",
      "Epoch Step: 610 Loss: 5.431561 Tokens per Sec: 801.296256\n",
      "Epoch Step: 620 Loss: 3.327930 Tokens per Sec: 814.997039\n",
      "Epoch Step: 630 Loss: 17.342768 Tokens per Sec: 830.086182\n",
      "\n",
      "Example #1\n",
      "Src :  , <unk>\n",
      "Trg :  , <unk> <unk>\n",
      "Pred:  , <unk>\n",
      "\n",
      "Example #2\n",
      "Src :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ . \"\n",
      "Trg :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ \"\n",
      "Pred:  <unk> , \" <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##сі қ ##ырық алты жоқ\n",
      "\n",
      "Example #3\n",
      "Src :  1 , енді бұл екі <unk> <unk> <unk> ##к . . .\n",
      "Trg :  <unk> , енді бұл екі <unk> <unk> <unk> ##к\n",
      "Pred:  бір мың тоғыз жүз он ##нан екі <unk> <unk>\n",
      "\n",
      "Validation perplexity: 5.746947\n",
      "Epoch 8\n",
      "Epoch Step: 10 Loss: 6.061620 Tokens per Sec: 817.883427\n",
      "Epoch Step: 20 Loss: 5.089676 Tokens per Sec: 800.455896\n",
      "Epoch Step: 30 Loss: 0.902079 Tokens per Sec: 805.272559\n",
      "Epoch Step: 40 Loss: 0.721141 Tokens per Sec: 783.755336\n",
      "Epoch Step: 50 Loss: 0.475755 Tokens per Sec: 808.120390\n",
      "Epoch Step: 60 Loss: 3.614298 Tokens per Sec: 846.237256\n",
      "Epoch Step: 70 Loss: 0.573142 Tokens per Sec: 841.377571\n",
      "Epoch Step: 80 Loss: 4.949042 Tokens per Sec: 770.587906\n",
      "Epoch Step: 90 Loss: 4.686710 Tokens per Sec: 797.983421\n",
      "Epoch Step: 100 Loss: 1.924940 Tokens per Sec: 786.625450\n",
      "Epoch Step: 110 Loss: 21.053097 Tokens per Sec: 798.206609\n",
      "Epoch Step: 120 Loss: 1.541801 Tokens per Sec: 757.068668\n",
      "Epoch Step: 130 Loss: 4.812459 Tokens per Sec: 799.389755\n",
      "Epoch Step: 140 Loss: 0.186309 Tokens per Sec: 783.483948\n",
      "Epoch Step: 150 Loss: 6.685025 Tokens per Sec: 773.758481\n",
      "Epoch Step: 160 Loss: 8.219894 Tokens per Sec: 749.080734\n",
      "Epoch Step: 170 Loss: 5.731474 Tokens per Sec: 804.947193\n",
      "Epoch Step: 180 Loss: 0.680632 Tokens per Sec: 809.237734\n",
      "Epoch Step: 190 Loss: 3.355050 Tokens per Sec: 818.355309\n",
      "Epoch Step: 200 Loss: 34.879654 Tokens per Sec: 814.207956\n",
      "Epoch Step: 210 Loss: 1.655295 Tokens per Sec: 750.883930\n",
      "Epoch Step: 220 Loss: 11.153767 Tokens per Sec: 735.178680\n",
      "Epoch Step: 230 Loss: 2.002893 Tokens per Sec: 790.371258\n",
      "Epoch Step: 240 Loss: 6.484116 Tokens per Sec: 751.190088\n",
      "Epoch Step: 250 Loss: 1.934522 Tokens per Sec: 807.786580\n",
      "Epoch Step: 260 Loss: 10.507496 Tokens per Sec: 814.006140\n",
      "Epoch Step: 270 Loss: 3.855562 Tokens per Sec: 791.021642\n",
      "Epoch Step: 280 Loss: 8.756445 Tokens per Sec: 822.514203\n",
      "Epoch Step: 290 Loss: 3.352202 Tokens per Sec: 789.514205\n",
      "Epoch Step: 300 Loss: 6.672839 Tokens per Sec: 797.873242\n",
      "Epoch Step: 310 Loss: 5.385087 Tokens per Sec: 827.459443\n",
      "Epoch Step: 320 Loss: 0.989499 Tokens per Sec: 755.226375\n",
      "Epoch Step: 330 Loss: 8.509116 Tokens per Sec: 784.565388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 340 Loss: 6.385601 Tokens per Sec: 750.616258\n",
      "Epoch Step: 350 Loss: 1.202150 Tokens per Sec: 791.087294\n",
      "Epoch Step: 360 Loss: 6.849794 Tokens per Sec: 780.246658\n",
      "Epoch Step: 370 Loss: 1.720154 Tokens per Sec: 747.086483\n",
      "Epoch Step: 380 Loss: 32.199570 Tokens per Sec: 724.551665\n",
      "Epoch Step: 390 Loss: 6.840900 Tokens per Sec: 740.117788\n",
      "Epoch Step: 400 Loss: 1.317819 Tokens per Sec: 800.966646\n",
      "Epoch Step: 410 Loss: 0.451894 Tokens per Sec: 747.621370\n",
      "Epoch Step: 420 Loss: 7.830525 Tokens per Sec: 778.451993\n",
      "Epoch Step: 430 Loss: 2.014314 Tokens per Sec: 783.733019\n",
      "Epoch Step: 440 Loss: 17.791538 Tokens per Sec: 775.827777\n",
      "Epoch Step: 450 Loss: 9.376375 Tokens per Sec: 700.700916\n",
      "Epoch Step: 460 Loss: 10.326880 Tokens per Sec: 731.748085\n",
      "Epoch Step: 470 Loss: 0.241662 Tokens per Sec: 738.390470\n",
      "Epoch Step: 480 Loss: 13.146062 Tokens per Sec: 777.588993\n",
      "Epoch Step: 490 Loss: 20.627201 Tokens per Sec: 798.170613\n",
      "Epoch Step: 500 Loss: 15.851270 Tokens per Sec: 819.690408\n",
      "Epoch Step: 510 Loss: 10.988064 Tokens per Sec: 774.859655\n",
      "Epoch Step: 520 Loss: 9.248749 Tokens per Sec: 850.105141\n",
      "Epoch Step: 530 Loss: 5.363181 Tokens per Sec: 710.843523\n",
      "Epoch Step: 540 Loss: 3.901191 Tokens per Sec: 743.556011\n",
      "Epoch Step: 550 Loss: 0.297398 Tokens per Sec: 766.131263\n",
      "Epoch Step: 560 Loss: 1.770554 Tokens per Sec: 815.240104\n",
      "Epoch Step: 570 Loss: 8.070807 Tokens per Sec: 751.091678\n",
      "Epoch Step: 580 Loss: 5.319499 Tokens per Sec: 769.660332\n",
      "Epoch Step: 590 Loss: 0.578152 Tokens per Sec: 855.714393\n",
      "Epoch Step: 600 Loss: 12.066291 Tokens per Sec: 776.272051\n",
      "Epoch Step: 610 Loss: 6.374150 Tokens per Sec: 706.607268\n",
      "Epoch Step: 620 Loss: 4.130051 Tokens per Sec: 832.321298\n",
      "Epoch Step: 630 Loss: 1.692499 Tokens per Sec: 797.932433\n",
      "\n",
      "Example #1\n",
      "Src :  , <unk>\n",
      "Trg :  , <unk> <unk>\n",
      "Pred:  ,\n",
      "\n",
      "Example #2\n",
      "Src :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ . \"\n",
      "Trg :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ \"\n",
      "Pred:  <unk> <unk> \" , \" <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##сі қ ##ырық төрт та <unk>\n",
      "\n",
      "Example #3\n",
      "Src :  1 , енді бұл екі <unk> <unk> <unk> ##к . . .\n",
      "Trg :  <unk> , енді бұл екі <unk> <unk> <unk> ##к\n",
      "Pred:  бір бүт ##ін он ##нан екі <unk> <unk> <unk> ##к <unk>\n",
      "\n",
      "Validation perplexity: 4.867179\n",
      "Epoch 9\n",
      "Epoch Step: 10 Loss: 2.429310 Tokens per Sec: 847.639163\n",
      "Epoch Step: 20 Loss: 5.476478 Tokens per Sec: 794.396227\n",
      "Epoch Step: 30 Loss: 7.344764 Tokens per Sec: 817.613744\n",
      "Epoch Step: 40 Loss: 19.596375 Tokens per Sec: 818.931088\n",
      "Epoch Step: 50 Loss: 3.292972 Tokens per Sec: 768.147350\n",
      "Epoch Step: 60 Loss: 5.543668 Tokens per Sec: 799.744905\n",
      "Epoch Step: 70 Loss: 7.182645 Tokens per Sec: 834.221872\n",
      "Epoch Step: 80 Loss: 18.377998 Tokens per Sec: 814.214611\n",
      "Epoch Step: 90 Loss: 5.589583 Tokens per Sec: 704.700923\n",
      "Epoch Step: 100 Loss: 13.690355 Tokens per Sec: 729.512703\n",
      "Epoch Step: 110 Loss: 2.506666 Tokens per Sec: 814.516063\n",
      "Epoch Step: 120 Loss: 5.063890 Tokens per Sec: 719.523551\n",
      "Epoch Step: 130 Loss: 0.103002 Tokens per Sec: 795.899130\n",
      "Epoch Step: 140 Loss: 1.273063 Tokens per Sec: 828.469857\n",
      "Epoch Step: 150 Loss: 10.021105 Tokens per Sec: 832.018365\n",
      "Epoch Step: 160 Loss: 0.139181 Tokens per Sec: 762.295610\n",
      "Epoch Step: 170 Loss: 7.337168 Tokens per Sec: 836.473050\n",
      "Epoch Step: 180 Loss: 0.241027 Tokens per Sec: 822.069258\n",
      "Epoch Step: 190 Loss: 0.210783 Tokens per Sec: 784.790860\n",
      "Epoch Step: 200 Loss: 13.935259 Tokens per Sec: 732.201562\n",
      "Epoch Step: 210 Loss: 6.640142 Tokens per Sec: 758.938044\n",
      "Epoch Step: 220 Loss: 8.984738 Tokens per Sec: 793.087797\n",
      "Epoch Step: 230 Loss: 3.290822 Tokens per Sec: 763.887762\n",
      "Epoch Step: 240 Loss: 4.364544 Tokens per Sec: 788.577102\n",
      "Epoch Step: 250 Loss: 0.442534 Tokens per Sec: 752.345425\n",
      "Epoch Step: 260 Loss: 4.127658 Tokens per Sec: 644.097512\n",
      "Epoch Step: 270 Loss: 17.896885 Tokens per Sec: 614.590059\n",
      "Epoch Step: 280 Loss: 7.717191 Tokens per Sec: 604.679327\n",
      "Epoch Step: 290 Loss: 0.160866 Tokens per Sec: 657.969112\n",
      "Epoch Step: 300 Loss: 11.371134 Tokens per Sec: 688.921763\n",
      "Epoch Step: 310 Loss: 0.355723 Tokens per Sec: 631.143456\n",
      "Epoch Step: 320 Loss: 7.568530 Tokens per Sec: 694.409139\n",
      "Epoch Step: 330 Loss: 5.715345 Tokens per Sec: 742.874153\n",
      "Epoch Step: 340 Loss: 0.953014 Tokens per Sec: 802.297432\n",
      "Epoch Step: 350 Loss: 2.227412 Tokens per Sec: 788.850239\n",
      "Epoch Step: 360 Loss: 0.827549 Tokens per Sec: 700.331784\n",
      "Epoch Step: 370 Loss: 9.362766 Tokens per Sec: 766.900015\n",
      "Epoch Step: 380 Loss: 0.400206 Tokens per Sec: 785.814205\n",
      "Epoch Step: 390 Loss: 1.908100 Tokens per Sec: 809.174500\n",
      "Epoch Step: 400 Loss: 0.226224 Tokens per Sec: 758.709753\n",
      "Epoch Step: 410 Loss: 3.177968 Tokens per Sec: 745.305539\n",
      "Epoch Step: 420 Loss: 3.808864 Tokens per Sec: 793.490441\n",
      "Epoch Step: 430 Loss: 0.588932 Tokens per Sec: 758.579034\n",
      "Epoch Step: 440 Loss: 3.224527 Tokens per Sec: 806.309776\n",
      "Epoch Step: 450 Loss: 8.529344 Tokens per Sec: 812.645211\n",
      "Epoch Step: 460 Loss: 1.399263 Tokens per Sec: 784.426234\n",
      "Epoch Step: 470 Loss: 7.073699 Tokens per Sec: 789.535717\n",
      "Epoch Step: 480 Loss: 6.976316 Tokens per Sec: 762.965841\n",
      "Epoch Step: 490 Loss: 1.312981 Tokens per Sec: 798.623487\n",
      "Epoch Step: 500 Loss: 0.339363 Tokens per Sec: 777.102658\n",
      "Epoch Step: 510 Loss: 1.048988 Tokens per Sec: 834.700776\n",
      "Epoch Step: 520 Loss: 15.424119 Tokens per Sec: 785.163065\n",
      "Epoch Step: 530 Loss: 12.095951 Tokens per Sec: 807.436450\n",
      "Epoch Step: 540 Loss: 10.382026 Tokens per Sec: 728.754950\n",
      "Epoch Step: 550 Loss: 6.160320 Tokens per Sec: 802.349727\n",
      "Epoch Step: 560 Loss: 4.945528 Tokens per Sec: 822.339417\n",
      "Epoch Step: 570 Loss: 4.287764 Tokens per Sec: 773.252723\n",
      "Epoch Step: 580 Loss: 3.396619 Tokens per Sec: 776.294883\n",
      "Epoch Step: 590 Loss: 12.589294 Tokens per Sec: 809.382122\n",
      "Epoch Step: 600 Loss: 1.301643 Tokens per Sec: 757.884096\n",
      "Epoch Step: 610 Loss: 11.484231 Tokens per Sec: 739.190506\n",
      "Epoch Step: 620 Loss: 8.382931 Tokens per Sec: 795.864582\n",
      "Epoch Step: 630 Loss: 0.422341 Tokens per Sec: 781.894050\n",
      "\n",
      "Example #1\n",
      "Src :  , <unk>\n",
      "Trg :  , <unk> <unk>\n",
      "Pred:  , <unk>\n",
      "\n",
      "Example #2\n",
      "Src :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ . \"\n",
      "Trg :  <unk> , \" \" \" өз <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> қ ##ұ ##қы жоқ \"\n",
      "Pred:  <unk> , <unk> \" ф <unk> <unk> <unk> <unk> <unk> <unk> <unk> ##ке <unk> ##тары да ресейде\n",
      "\n",
      "Example #3\n",
      "Src :  1 , енді бұл екі <unk> <unk> <unk> ##к . . .\n",
      "Trg :  <unk> , енді бұл екі <unk> <unk> <unk> ##к\n",
      "Pred:  бір бүт ##ін жүз ##ден екі бұл <unk> <unk>\n",
      "\n",
      "Validation perplexity: 4.776834\n"
     ]
    }
   ],
   "source": [
    "dev_perplexities = train(model, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXhU9dn/8fedlYRAAiSEJQkQQHZwieCKVmWzblWrgtpqXdo+Pl2eLlZ97PLU/dfW1tYuWtdatbUqtXUFrbhUCwYUCAFB2QOBsIUlQLb798dMNMRsQCYnmfm8rmuuzJxZzp2jfM7J93vOPebuiIhI7IgLugAREWlfCn4RkRij4BcRiTEKfhGRGKPgFxGJMQp+EZEYo+CXTsnM5pjZ1W3wOUvM7NQ2KCkqmZmb2ZCg65C2peCXNmNmq81sr5ntNrNNZvawmaUFXVdz3H2Uu88BMLOfmNmfAy6pSQ22b93t3qDrks5HwS9t7Wx3TwOOBo4Fbj7YDzCzhDavqhOxkKb+bZ7t7mn1bv/drsVJVFDwS0S4ewnwEjAawMzSzexBM9toZiVmdquZxYefu8LM/m1mvzSzbcBP6i37jZmVm9kyMzu9qfWZ2VfMbKmZbTezV8xsQHj5CWa2xcxyw4/HmdkOMxsefrzazM4ws6nATcDF4SPphWb2RTOb32A93zWzvzdRwxwzu8PM5oVrfs7MetZ7/jgzeye8/oX1h5jC773NzP4NVAD5B7O9W9peZtbPzP5hZtvM7CMzu6bec/FmdpOZfWxmu8xsft32CjvDzFaEt+1vzcwOpjbpeBT8EhHh4DgTeD+86FGgGhgCHAVMBuqP0U8AVgK9gdsaLMsEfgw8Wz9I663rPEKhfT6QBbwFPAng7u8A9wGPmlkK8Bhws7svq/8Z7v4ycDvw1/CR9DjgH8AgMxtR76WXhT+jKV8CvgL0C/++vw7X2B94AbgV6Al8D3jGzLLqvfdy4FqgG7CmmXU0pbnt9SSwPlzXhcDt9XYM3wGmE/rv1T1cf0W9zz2L0F9v44CLgCmHUJt0JO6um25tcgNWA7uBHYSC63dACpAN7AdS6r12OvB6+P4VwNoGn3UFsAGwesvmAZeH788Brg7ffwm4qt7r4ggF14Dw40RgPrAYeLnBZ64Gzgjf/wnw5wZ1/B64LXx/FLAdSG7i958D3Fnv8UigEogHfgA81uD1rwBfrvfenx7E9q27XdPS9gJygRqgW73n7gAeCd//EDi3iXU6cFK9x08BNwT9/5puh3fTEb+0tfPcPcPdB7j7f7n7XmAAofDdGB7m2EHoKLx3vfeta+SzSjycNmFrCB2xNjQAuKfeZ28DDOgP4O5VwCOEhp1+0eAzW/IoMCM8vHE58JS772/m9fV/jzWEfu/McI1frKsxXOdJQN8m3tuUuu1bd/tjveea2l79gG3uvqvBc/3D93OBj5tZZ2m9+xVAh56wl5Yp+KU9rCN0xJ9ZL7C6u/uoeq9pLIz7NxhPziN0VNvY53+1QSCmeGiYp26Y5cfAw8AvzCy5iTo/U4O7/4fQUfvJwAyaH+aBUIjWr7cK2BKu8bEGNXZ19zubW/9Bamp7bQB6mlm3Bs+VhO+vAwYf5rqlE1HwS8S5+0ZgFqHQ7W5mcWY22MxOaeGtvYFvmlmimX0RGAG82Mjr/gDcaGaj4JOJ5C+G7xuho/0HgauAjcAtTaxvEzCwkTNq/gTcC1S7+9st1HyZmY00s1Tgp8DT7l4D/Bk428ymhCdTu5jZqWaW08LnHYxGt5e7rwPeAe4Ir3csoW3xePh9DwC3mNnQ0AlFNtbMerVhXdLBKPilvXwJSAKKCY2TP82BwxyNmQsMJXTEfBtwobtvbfgid58J3AX8xcx2AkXAtPDT3yQ0x/DD8DDIlcCVZnZyI+v7W/jnVjNbUG/5Y4SGiVo62q977SOEhke6hNdPOHzPJTQJXUboKPv7HPy/wX/agefxz6z3XHPbazowkNDR/0zgx+4+O/zc3YTG7mcBOwntJFMOsi7pROzghjtF2oeZXUFo8vakDlBLCrAZONrdVzTzujmEJocfaK/a6q37CjrI9pKOT0f8Ii37OvBec6Ev0pnE9BWSIi0xs9WEzhA6L+BSRNqMhnpERGKMhnpERGJMpxjqyczM9IEDBwZdhohIpzJ//vwt7p7VcHmnCP6BAwdSWFgYdBkiIp2KmTXa80lDPSIiMUbBLyISYxT8IiIxRsEvIhJjFPwiIjFGwS8iEmMU/CIiMSaqg//tFVv43ZyPgi5DRKRDiergf3NFGb+YtZxNO/cFXYqISIcRseA3s4fMbLOZFTXy3PfMzM0sM1LrB5g+Po+aWuep91rzVaYiIrEhkkf8jwBTGy40s1xgErA2gusGYFBmV04Y3Iu/vLeOmlp1IRURgQgGv7u/CWxr5KlfAtdz+F8s3SozJuRRsmMvby4va4/ViYh0eO06xm9m5wAl7r6wFa+91swKzaywrOzQQ3vyyD5kpiXx+NyI/4EhItIptFvwm1kq8L/Aj1rzene/390L3L0gK+szXUVbLSkhjguPyeVfyzaxsXzvIX+OiEi0aM8j/sHAIGBh+OvscoAFZtYn0iuePj6XWoe/apJXRKT9gt/dF7t7b3cf6O4DgfXA0e5eGul1D+jVlZOHZvLX99ZRXVMb6dWJiHRokTyd80ngXWCYma03s6sita7WmDE+j43l+5jzoSZ5RSS2RewbuNx9egvPD4zUuhtzxshssrol88S8tZwxMrs9Vy0i0qFE9ZW79SXGx3FRQQ5zPtxMyQ5N8opI7IqZ4Ae45Ng8HPjrPJ3aKSKxK6aCP7dnKhOHZvHXQk3yikjsiqngh9CVvJt27ue1ZZuDLkVEJBAxF/ynD+9NdvdkntCVvCISo2Iu+BPi47i4IJc3V5SxbltF0OWIiLS7mAt+gIvH52HAX97TUb+IxJ6YDP7+GSmcOqw3TxWup0qTvCISY2Iy+CF0JW/Zrv28Wrwp6FJERNpVzAb/qcOy6JvehSd0Tr+IxJiYDf6E+DguPjaXt1ZsYc3WPUGXIyLSbmI2+AEuPjaXOIMn56lds4jEjpgO/r7pKZw2PJun56+jslqTvCISG2I6+AEunZDHlt2VzCqO+NcCiIh0CDEf/BOPyKJ/Roqu5BWRmBHzwR8fZ1xybC7vfLyVVVs0ySsi0S/mgx/gomNziY8zntSpnSISAxT8QHb3LpwxojdPz1/P/uqaoMsREYkoBX/YjAkD2LankpeLNMkrItFNwR928pBMcntqkldEop+CPywuzrjk2DzmrtrGR5t3B12OiEjEKPjr+WJBDgma5BWRKKfgr6d3ty5MHpXNMwvWs69Kk7wiEp0U/A3MGD+AHRVVvFS0MehSREQiQsHfwAmDezGgV6omeUUkain4G4iLM6aPz+O91dtZvmlX0OWIiLQ5BX8jLjwmh8R401G/iEQlBX8jMtOSmTKqD89qkldEopCCvwkzJuSxc181zy/SJK+IRBcFfxOOz+9FfmZXnpi7JuhSRETalIK/CWahSd4Fa3ewrHRn0OWIiLQZBX8zLjgmh6T4OE3yikhUiVjwm9lDZrbZzIrqLfuZmS0zs0VmNtPMMiK1/rbQs2sS08b0YeaCEioqq4MuR0SkTUTyiP8RYGqDZbOB0e4+FlgO3BjB9beJGePz2LW/mucXapJXRKJDxILf3d8EtjVYNsvd6w6d/wPkRGr9bWX8oJ4M6Z3G42rcJiJRIsgx/q8ALwW4/lYxM2aMz2Phuh0s2VAedDkiIoctkOA3s/8FqoHHm3nNtWZWaGaFZWVl7VdcIy44OofkBE3yikh0aPfgN7MvA2cBl7q7N/U6d7/f3QvcvSArK6v9CmxEemoinx/bl+c+2MCe/ZrkFZHOrV2D38ymAj8AznH3ivZc9+G6dEIeu/dX84+FG4IuRUTksETydM4ngXeBYWa23syuAu4FugGzzewDM/tDpNbf1o7O68Gw7G4a7hGRTi8hUh/s7tMbWfxgpNYXaWbGjAl5/PgfS1i8vpwxOelBlyQickh05e5BOO+o/nRJjOOJeerfIyKdl4L/IKSnJHL22H4898EGdu2rCrocEZFDouA/SDMm5FFRWcNzH2iSV0Q6JwX/QToyN4MRfbvzxNy1NHM2qohIh6XgP0h1k7zFG3eycL2u5BWRzkfBfwjOO7IfqUnx+pIWEemUFPyHoFuXRM4Z149/LtzITk3yikgno+A/RDMm5LG3qoa/v18SdCkiIgdFwX+IxuZkMLq/JnlFpPNR8B+GGeMHsKx0FwvW7gi6FBGRVlPwH4ZzjuxH16R49e8RkU5FwX8Y0pITOPeo/jy/aAPlFZrkFZHOQcF/mGaMz2N/dS3Pvr8+6FJERFpFwX+YRvdPZ1xOuiZ5RaTTUPC3gRkT8lixeTeFa7YHXYqISIsU/G3g7HH96JacoEleEekUFPxtIDUpgfOO6s8LizeyfU9l0OWIiDRLwd9GZkzIo7K6lmcWaJJXRDo2BX8bGdG3O0flZfDEPE3yikjHpuBvQzPG57GybA9zV20LuhQRkSYp+NvQWWP70a2LJnlFpGNT8LehlKR4Ljg6h5eLStmmSV4R6aAU/G1sxoQ8KmtqeXr+uqBLERFplIK/jR2R3Y2CAT14ct46TfKKSIek4I+AGRPyWLVlD+9+vDXoUkREPkPBHwFnjulLekoij8/TJK+IdDytCn4z+7mZjYp0MdGiS2JoknfWklK27N4fdDkiIgdo7RH/MuB+M5trZl8zs/RIFhUNZkzIparG+VuhruQVkY6lVcHv7g+4+4nAl4CBwCIze8LMPhfJ4jqzIb27MX5QT56ct5baWk3yikjH0eoxfjOLB4aHb1uAhcB3zOwvEaqt07t0Qh5rt1Xw74+3BF2KiMgnWjvGfzeh4Z4zgdvd/Rh3v8vdzwaOimSBndnU0X3okZqoK3lFpENp7RF/ETDO3b/q7vMaPDe+jWuKGskJ8Vx4TA6zizexede+oMsREQFaH/yXuntF/QVm9hqAu5e3eVVRZPr4PKprNckrIh1Hs8FvZl3MrCeQaWY9zKxn+DYQ6NfCex8ys81mVlRvWU8zm21mK8I/e7TFL9GR5WelcfLQTP741kqd2ikiHUJLR/xfBeYTmtBdEL4/H3gO+G0L730EmNpg2Q3Aa+4+FHgt/Djq/eiskezZX80tzxcHXYqISPPB7+73uPsg4HvuPqjebZy739vCe98EGjamPxd4NHz/UeC8Qy28Mxma3Y2vnzqE5z7YwOsfbg66HBGJcS0N9ZwWvltiZuc3vB3C+rLdfSNA+GfvZtZ9rZkVmllhWVnZIayqY7nuc4PJz+rKzTOLqKisDrocEYlhLQ31nBL+eXYjt7MiWBfufr+7F7h7QVZWViRX1S6SE+K58/yxlOzYy92zlgddjojEsITmnnT3H4d/XtlG69tkZn3dfaOZ9QViatxj/KCeTB+fx0P/XsU5R/ZjbE5G0CWJSAxq7QVcj9Xvz2NmA+pO5zxI/wC+HL7/ZUKTxDHlhmnDyUxL5oZnFlNVUxt0OSISg1p7Hv/bwFwzO9PMrgFmA79q7g1m9iTwLjDMzNab2VXAncAkM1sBTAo/jinpKYn83zmjKN64k4feXhV0OSISg5od6qnj7veZ2RLgdUJ9eo5y99IW3jO9iadOP7gSo8/U0X04Y0Q2v3x1OdNG9yWvV2rQJYlIDGntUM/lwEOEunM+ArxoZuMiWFdUMzNuOW8UCXFx3DRzsb6iUUTaVWuHei4ATnL3J939RuBrfHo+vhyCvukpXD91GG9/tIWZ75cEXY6IxJDW9uM/z90313s8DzVnO2yXTRjA0XkZ3PJ8MVvVzkFE2klrh3qOMLPX6vrumNlY4PqIVhYD4uKMO84fy+791dz6wtKgyxGRGNHaoZ4/AjcCVQDuvgi4JFJFxZJhfbrxtVMGM/P9Et5Y3vmvUBaRjq+1wZ/aSB9+9R1oI9d9bgj5WV3535mL1c5BRCKutcG/xcwGAw5gZhcCGyNWVYzpkhjP7V8Yw/rte/nVqyuCLkdEolxrg/864D5guJmVAN8Gvh6xqmLQcfm9uOTYXB54ayVFJfpuGxGJnNae1bPS3c8AsoDh7n6Su6+OaGUx6MZpI+jZNZkfPLOIarVzEJEIafbKXTP7ThPLAXD3uyNQU8xKTw21c7juiQU8/O/VXDMxP+iSRCQKtXTE362Fm7SxM8f04YwRvbl79nLWbato+Q0iIgfJOkO7gIKCAi8sLAy6jHazYcdeJt39BkcP6MGfvjL+k7+wREQOhpnNd/eChstbewFXvpn908zKwl+g/pyZaRwiQvplpPD9KcN4a8UW/v6B2jmISNtq7Vk9TwBPAX2BfsDfgCcjVZTA5ccP5MjcDG55finb9lQGXY6IRJHWBr+5+2PuXh2+/ZnwOf0SGfFxxp0XjGHn3ipufaE46HJEJIq0NvhfN7MbzGxg+Nu3rgdeMLOeZtYzkgXGsuF9uvPVU/J5dkEJb61QOwcRaRutmtw1s+a+KsrdPaLj/bE2uVvfvqoapt3zFjW1zivfnkhKUnzQJYlIJ3HIk7tmFgdc5u6DmrhpkjeC6to5rN1Wwa9eWx50OSISBVoMfnevBX7eDrVIE44f3IuLC3J54K1VaucgIoettWP8s8zsAtMJ5YG58czh9EhN5MZnF6udg4gcltYG/3cIncJZaWY7zWyXme2MYF3SQEZqEj8+exSLS8p55J3VQZcjIp1Ya5u0dXP3OHdPdPfu4cfdI12cHOissX05bXhvfjFL7RxE5NC19spdM7PLzOyH4ce5Zqbv3G1nZsYt543GDG7+exGdod2GiHQ8rR3q+R1wPDAj/Hg38NuIVCTN6p+RwvcmD+ON5WX8Y+GGoMsRkU6otcE/wd2vA/YBuPt2ICliVUmzvnzCQMblZvDTfxazXe0cROQgtTb4q8wsnk+/ejEL0KklAYmPM+48fwzle6u47cWlQZcjIp1Ma4P/18BMoLeZ3Qa8DdwesaqkRSP6dueaifk8PX89//5oS9DliEgn0tqzeh4HrgfuIPQl6+e5+98iWZi07FunD2Vgr1RumrmYfVU1QZcjIp1Es8FvZl3M7Ntmdi9wCnCfu9/r7hpf6ADq2jms2VrBPa+tCLocEekkWjrifxQoABYD01Drhg7nhCGZfPGYHO5/cyXFG3RNnYi0rKXgH+nul7n7fcCFwMR2qEkO0k1njiAjJZEbn11ETa3O7ReR5rUU/FV1d9y9OsK1yCHq0TWJH509koXr1c5BRFrWUvCPC/fm2Wlmu4Cx6tXTMZ0zrh+nDsviF7M+ZP12tXMQkaY1G/zuHh/uzVPXnyehLXr1mNn/mNkSMysysyfNrMuhfpaEmBm3njcad/ih2jmISDNaex5/mzGz/sA3gQJ3Hw3EA5e0dx3RKKdHKt+dfASvf1jGPxdtDLocEemg2j34wxKAFDNLAFIBNZ1pI1eeOIixOen89J9L2FGhdg4i8lntHvzuXkLotNC1hC4GK3f3WQ1fZ2bXmlmhmRWWlemLxlsr1M5hLNsrqrhd7RxEpBFBDPX0AM4FBgH9gK5mdlnD17n7/e5e4O4FWVlZ7V1mpzayX3euOTmfpwrX887HaucgIgcKYqjnDGCVu5e5exXwLHBCAHVEtW+dPpS8nqnc9KzaOYjIgYII/rXAcWaWGv4O39MBjUm0sZSkUDuH1Vsr+M2/1M5BRD4VxBj/XOBpYAGhVhBxwP3tXUcsOGloJhccncN9b6xk6UZddiEiIYGc1ePuP3b34e4+2t0vd/f9QdQRC27+/Ai6pyRyw7OL1c5BRIDgTueUdtKjaxI/OmskC9ft4E/vrg66HBHpABT8MeDcI/sx8YgsfvbKh5Ts2Bt0OSISMAV/DDAzbgu3c/ivP89n93712xOJZQr+GJHbM5VfTz+Kog07+epjheyv1imeIrFKwR9DJo3M5v9dMJZ/f7SVb//lA032isQoBX+MueCYHG7+/AheKirl5r8vVhdPkRiUEHQB0v6uPjmf7RWV/Pb1j+mRmsT1U4cHXZKItCMFf4z63uRhbK+o4ndzQuF/zcT8oEsSkXai4I9RZsYt546mvKKK215cSnpqIhcV5AZdloi0AwV/DIuPM+6+eBw791VxwzOLSE9JZMqoPkGXJSIRpsndGJecEM8fLjuGMTkZfOPJ93n3461BlyQiEabgF7omJ/DIFceS1zOVa/5USFFJedAliUgEKfgFCPX0eeyq8aSnJPLlh+axsmx30CWJSIQo+OUTfdNTeOyq8QBc/uA8Nparr49INFLwywHys9J49CvjKd9bxeUPzmP7Hn1hu0i0UfDLZ4zun84fv1TA2m0VXPHIe+xRUzeRqKLgl0YdP7gX904/iqKScr725/lq6iYSRRT80qTJo/pw1wVjeWvFFr7z14Vq6iYSJXQBlzTrwmNy2FFRya0vLKV7SiK3f2E0ZhZ0WSJyGBT80qKrT85n255KfjfnY3p2TeT7U9TUTaQzU/BLq3x/yrADOnpefbKauol0Vgp+aRUz49bzxlC+t4pbX1hKRmoSFx6TE3RZInIIFPzSavFxxi8vPpKdewv5Qbip26SR2UGXJSIHSWf1yEFJTojnvsuPYXT/dK57YgH/WammbiKdjYJfDlrX5AQeDjd1u/pRNXUT6WwU/HJIeqqpm0inpeCXQ9awqVtp+b6AKxKR1lDwy2HJz0rjkSvrmrrNVVM3kU5AwS+HbUxOqKnbmm0VXKmmbiIdnoJf2sTxg3vxm+lHsWj9DjV1E+ngFPzSZqaM6sOddU3dnlJTN5GOShdwSZu6qCCX8ooqbntxKekpidx2npq6iXQ0Cn5pc9dMzGdbRSW/n/MxPVOT+N6UYUGXJCL1BBL8ZpYBPACMBhz4iru/G0QtEhnXTxnG9j2V3Pv6R2SkJqqpm0gHEtQR/z3Ay+5+oZklAakB1SERYmbc9oUx7KgINXXrkZrEBWrqJtIhtPvkrpl1ByYCDwK4e6W772jvOiTy4uOMe6YfyYlDenH9M4t4tXhT0CWJCMGc1ZMPlAEPm9n7ZvaAmXVt+CIzu9bMCs2ssKysrP2rlDYRaupWwOh+3bnuiQXMVVM3kcAFEfwJwNHA7939KGAPcEPDF7n7/e5e4O4FWVlZ7V2jtKG05AQevnI8OT1S1NRNpAMIYox/PbDe3eeGHz9NI8Ev0SXU1G0CF/7+Ha54eB7nHtmf9JREMlITSU9JpHtK6GdGyqePE+N1mYlIJLR78Lt7qZmtM7Nh7v4hcDpQ3N51SPvrl5HCY1dP4LrHF/DkvLVUVDZ/dW/XpHjSUxJJT00iPSUhdL9uB5Ga9MnOov4Oo26nER+nawdEmhLUWT3fAB4Pn9GzErgyoDqknQ3OSuPlb08EoLK6lp37qthRUUX53ip27g39LN/76bJPb5Ws2rLnk8f7qmqbXU+35ATSUxMP2FmEdiKf3s9MS+b4wb3o3iWxPX51aUFldS1PFa4jLTmBs8f10847ggIJfnf/ACgIYt3ScSQlxJGZlkxmWvJBv3dfVc0BO4rGdxaf3lZs3s2OitDOpbLm051GUnwcJw7pxbTRfZk0MpseXZPa8leUVnp/7XZueGYxH27aBcAf3viYH0wbzqlHZOnK7wgw947fT6WgoMALCwuDLkOigLuzr6qW8r1VrN1WwawlpbxUVErJjr3ExxnH5fdk2ui+TB6VTe9uXYIuN+pVVFbz81eW8/A7q+jdLZlbzh1NZU0tP3vlQ9ZsreD4/F7ceOZwxuZkBF1qp2Rm8939MwfZCn6Jee5OUclOXirayMtFpazcsgczOHZAT6aO7sPU0X3ol5ESdJlR583lZdw0czHrt+/l0gl5/GDa8E+G3Sqra3li7hp+/a+P2LankrPG9uX7U4YxoNdnzvyWZij4RVrB3Vm+aTcvLg7tBOqGHsblZjBtdB+mje6j8DlM2/dUcusLS3lmwXryM7tyx/ljmJDfq9HX7tpXxf1vruSBt1ZRXVvLpRMG8I3ThtDrEIYHY5GCX+QQrCzbzUtFpbxcVMri8PUHI/p2/2QnMDS7W8AVdh7uzvOLNvJ//1zCjooqvnpKPt84bShdEuNbfO/mnfv45asreKpwHSmJ8Xx1Yj5XnTyI1CT1mWyOgl/kMK3bVsEr4TmB+Wu2AzCkdxrTwsNBI/t210RkEzaW7+WHfy/i1aWbGdM/nbsuGMvIft0P+nM+2ryb//fyMmYVb6J3t2S+fcYRXFSQQ4Ku+WiUgl+kDW3auS+0E1hcytxVW6l1yOuZ+slO4MjcDO0EgNpa5/F5a7nrpWVU19by3UnDuPLEgYcd1IWrt3HHS8uYv2Y7g7O6cv3U4Uwema1t3oCCXyRCtu7ez6ziTbxUVMo7H22hutbpm96FKaP6cOaYvhwzoEdMnpP+cdlubnxmMfNWb+PEIb24/Qtj2nR+xN2ZVbyJu15exsqyPRwzoAc3nTmcYwb0bLN1dHYKfpF2UF5RxatLQzuBN1eUUVldS2ZaMlNGZTNtdF8m5PeM+lYUVTW13P/mSu55bQVdEuK4+ayRfPGYnIgdjVfX1PJU4Xp++epyynbtZ/LIbK6fOpwhvdMisr7ORMEv0s5276/m9WWbebmolH8t28zeqhoyUhOZNCKbaWP6cOKQTJITWp7Y7EwWrd/B9U8vYlnpLs4c04efnDOq3a6HqKis5sG3VnHfmyvZW1XDRQW5/M8ZQ+ndPXavx1DwiwRoX1UNbywv4+WiUl4t3sSu/dV0S07gtBG9mTa6DxOPyOrUZ6hUVFbzy9nLefDtVWSmJXPLeaOZMqpPILVs3b2f3/zrIx6fu4aEuDiuPnkQ107Mp1sMtuZQ8It0EPura3jno628VLSR2cWb2F5RRXJCHCcPzWTyyD6cNqL3IbWxCMrbK7Zw48xFrNu2l+nj87hh2nDSU4IP2TVb9/CzVz7k+UUb6dU1iW+cNoQZEwaQlBDdQ231KfhFOqDqmlrmrdrGrOJNzC7eRMmOvZhBwYAeTB7Zh0kjsxmY2TEvGCuvqOLWF4r52/z1DApfiHVcExdiBWnhuh3c+dIy3l25lQG9Uvne5GGcNbZvTJwBpOAX6eDcneKNO5m1JLQTKN64EwVnkscAAAgRSURBVIAjstM+2QmMzUkPPLDcnZeKSvnRc0vYXlHJtRPz+dbprbsQKyjuzpzlZdz10jKWle5ibE46N0wbzgmDM4MuLaIU/CKdzLptFcwO/yUwb/U2amqdPt27MGlkNpNGZnNcfq92H7YoLd/HD58rYnbxJkb3786d549ldP/0dq3hcNTUOjPfL+HuWR+yoXwfpw7L4oZpwxne5+AvJusMFPwindj2PZX8a9lmZhdv4o3lZeytqqFbcgKfG96bSSOzOXVYVkQnL2trnb+8t447XlxKZU0t35l0BFedNKjTXjG7r6qGR99ZzW9f/4hd+6s5/6gcvjv5iKhrxqfgF4kS+6pqeHvFFmYXb+LVpZvYuqeSxHjjhMGZn/w1kN2GpzCuLNvNjc8uZu6qbRyf34s7zh/TYecdDtaOikp+N+djHnlnNQBXnjiQ/zplCOmpwU9OtwUFv0gUqql13l+7nVnFm3hlSSlrtlYAcGRuBpNGZjNlVDaDs9IOaV6gqqaWP761kl+9uoLkhDhu/vwILirIDXyOIRLWb6/g7tnLmfl+Cd27JPLfnxvCCUN60S05kbQuCaQlJ3TKs4EU/CJRzt1ZsXk3s4s3MWtJKQvXh7qJ5md2ZdLIbCaPyubI3Na1jygqKef6pxdRvHEnU0f14afnjoqJC6GKN+zkrpeX8cbyss88l5wQR7fwTqBbl0TSkhNI65JAt+SE0PIuCaSFdxTdw6+re23d+9K6JLTrldsKfpEYU1q+j9lLQzuB/6zcSlWNk5mWxBkjQjuBEwZnfuZMnL2VNfzq1eU88PYqenZN4pZzRzF1dN+AfoPgFJWUU7JjL7v2VbN7XxW791eza181u/ZXs3tfNbvqLav/s6a25TztkhhHWnJivZ3IpzuF7vV2KHXPHZff65CH7hT8IjFs574q5nxYxqwlpcz5sIzd+6tJTYrnlCOymDwqm9OGZbNkQzk3zlzMmq0VXHJsLjeeOaJDXIjVWdR9reeufVWf7CBCO4Wqz+wgdtXbeexuZFn9/cejXxnPKUdkHVJNCn4RAUJXDv9n5TZmLSnl1aWb2LRzP/FxRk2tM6BXKnecPybqz2/vyNydisqaT3YGfdO70DX50Np5KPhF5DNqa51FJeXMLi4lNSmBq04a1KEvxJKD01Twd96uUCJy2OLijCNzMzgyNyPoUqQddb7zk0RE5LAo+EVEYoyCX0Qkxij4RURijIJfRCTGKPhFRGKMgl9EJMYo+EVEYkynuHLXzMqANYf49kxgSxuW09lpe3xK2+JA2h4HiobtMcDdP9Pop1ME/+Ews8LGLlmOVdoen9K2OJC2x4GieXtoqEdEJMYo+EVEYkwsBP/9QRfQwWh7fErb4kDaHgeK2u0R9WP8IiJyoFg44hcRkXoU/CIiMSaqg9/MpprZh2b2kZndEHQ9QTGzXDN73cyWmtkSM/tW0DV1BGYWb2bvm9nzQdcSNDPLMLOnzWxZ+P+T44OuKShm9j/hfydFZvakmR3aN513YFEb/GYWD/wWmAaMBKab2chgqwpMNfBddx8BHAdcF8Pbor5vAUuDLqKDuAd42d2HA+OI0e1iZv2BbwIF7j4aiAcuCbaqthe1wQ+MBz5y95XuXgn8BTg34JoC4e4b3X1B+P4uQv+o+wdbVbDMLAf4PPBA0LUEzcy6AxOBBwHcvdLddwRbVaASgBQzSwBSgQ0B19Pmojn4+wPr6j1eT4yHHYCZDQSOAuYGW0ngfgVcD9QGXUgHkA+UAQ+Hh74eMLOuQRcVBHcvAX4OrAU2AuXuPivYqtpeNAe/NbIsps9dNbM04Bng2+6+M+h6gmJmZwGb3X1+0LV0EAnA0cDv3f0oYA8Qk3NiZtaD0MjAIKAf0NXMLgu2qrYXzcG/Hsit9ziHKPyTrbXMLJFQ6D/u7s8GXU/ATgTOMbPVhIYATzOzPwdbUqDWA+vdve6vwKcJ7Qhi0RnAKncvc/cq4FnghIBranPRHPzvAUPNbJCZJRGaoPlHwDUFwsyM0PjtUne/O+h6gubuN7p7jrsPJPT/xb/cPeqO6lrL3UuBdWY2LLzodKA4wJKCtBY4zsxSw/9uTicKJ7oTgi4gUty92sz+G3iF0Mz8Q+6+JOCygnIicDmw2Mw+CC+7yd1fDLAm6Vi+ATwePkhaCVwZcD2BcPe5ZvY0sIDQ2XDvE4WtG9SyQUQkxkTzUI+IiDRCwS8iEmMU/CIiMUbBLyISYxT8IiIxRsEvAphZjZl9UO/WZleumtlAMytqq88TOVxRex6/yEHa6+5HBl2ESHvQEb9IM8xstZndZWbzwrch4eUDzOw1M1sU/pkXXp5tZjPNbGH4Vne5f7yZ/THc532WmaUE9ktJzFPwi4SkNBjqubjeczvdfTxwL6GunoTv/8ndxwKPA78OL/818Ia7jyPU76buavGhwG/dfRSwA7ggwr+PSJN05a4IYGa73T2tkeWrgdPcfWW40V2pu/cysy1AX3evCi/f6O6ZZlYG5Lj7/nqfMRCY7e5Dw49/ACS6+62R/81EPktH/CIt8ybuN/Waxuyvd78Gza9JgBT8Ii27uN7Pd8P33+HTr+S7FHg7fP814OvwyXf6dm+vIkVaS0cdIiEp9TqXQuj7Z+tO6Uw2s7mEDpSmh5d9E3jIzL5P6Nur6rpZfgu438yuInRk/3VC3+Qk0mFojF+kGeEx/gJ33xJ0LSJtRUM9IiIxRkf8IiIxRkf8IiIxRsEvIhJjFPwiIjFGwS8iEmMU/CIiMeb/A8+MA8RsGdEVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def plot_perplexity(perplexities):\n",
    "    \"\"\"plot perplexities\"\"\"\n",
    "    plt.title(\"Perplexity per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.plot(perplexities)\n",
    "\n",
    "plot_perplexity(dev_perplexities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "555"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [\" \".join(example.trg) for example in valid_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = []\n",
    "alphas = []  \n",
    "for batch in valid_iter:\n",
    "  batch = rebatch(PAD_INDEX, batch)\n",
    "  pred, attention = greedy_decode(\n",
    "    model, batch.src, batch.src_mask, batch.src_lengths, max_len=25,\n",
    "    sos_index=TRG.vocab.stoi[SOS_TOKEN],\n",
    "    eos_index=TRG.vocab.stoi[EOS_TOKEN])\n",
    "  hypotheses.append(pred)\n",
    "  alphas.append(attention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = [lookup_words(x, TRG.vocab) for x in hypotheses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = [\" \".join(x) for x in hypotheses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555\n",
      "бір бүт ##ін жүз ##ден екі бұл <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "print(len(hypotheses))\n",
    "print(hypotheses[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = sacrebleu.raw_corpus_bleu(hypotheses, [references], .01).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.864976863024914\n"
     ]
    }
   ],
   "source": [
    "print(bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(src, trg, scores):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.pcolor(scores, cmap='viridis')\n",
    "\n",
    "    ax.set_xticklabels(trg, minor=False, rotation='vertical')\n",
    "    ax.set_yticklabels(src, minor=False)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    # and the x-ticks on top\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.set_xticks(np.arange(scores.shape[1]) + 0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(scores.shape[0]) + 0.5, minor=False)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    plt.colorbar(heatmap)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> , <unk> ##с жеті бүт ##ін жүз секс ##ен жеті ж'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src ['20', ',', 'содан', 'бері', 'бірде', 'бір', 'рет', 'күрделі', 'жөндеу', '##ден', 'ө', '##т', '##пеген', '.', '</s>']\n",
      "ref ['20', ',', 'содан', 'бері', 'бірде', 'бір', 'рет', 'күрделі', 'жөндеу', '##ден', 'ө', '##т', '##пеген', '</s>']\n",
      "pred ['<unk>', ',', '<unk>', '<unk>', '<unk>', 'бір', '<unk>', '<unk>', 'ө', '<unk>', 'ө', '<unk>', '<unk>', '<unk>', '</s>']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 14\n",
    "src = valid_data[idx].src + [\"</s>\"]\n",
    "trg = valid_data[idx].trg + [\"</s>\"]\n",
    "pred = hypotheses[idx].split() + [\"</s>\"]\n",
    "pred_att = alphas[idx][0].T[:, :len(pred)]\n",
    "print(\"src\", src)\n",
    "print(\"ref\", trg)\n",
    "print(\"pred\", pred)\n",
    "plot_heatmap(src, pred, pred_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36pyt14",
   "language": "python",
   "name": "py36pyt14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
